[
('word2vec has been patented. What does it change for NLP practitioners?', "Login Sign Search Expert Search Quick Search Patents Apps Non-Patent Literature Blogs Groups MPEP Case Law SEARCH BLOGS MPEP 2 0 TOOLS RESOURCES PRODUCT SERVICES HELP Title Computing numeric representations words high-dimensional space Document Type Number United States Patent 9037464 Abstract Methods systems apparatus including computer programs encoded computer storage media computing numeric representations words One methods includes obtaining set training data wherein set training data comprises sequences words training classifier embedding function set training data wherein training embedding function comprises obtained trained values embedding function parameters processing word vocabulary using embedding function accordance trained values embedding function parameters generate respective numerical representation word vocabulary high-dimensional space associating word vocabulary respective numeric representation word high-dimensional space Inventors Mikolov Tomas Jersey City NJ US Chen Kai San Bruno CA US Corrado Gregory San Francisco CA US Dean Jeffrey Palo Alto CA US Application Number 13 841640 Publication Date 05 19 2015 Filing Date 03 15 2013 View Patent Images Download PDF 9037464 PDF help Export Citation Click automatic bibliography generation Assignee Google Inc Mountain View CA US Primary Class 704 255 Classes 704 256 International Classes G10L15 00 G06F17 28 US Patent References 20130262467METHOD APPARATUS PROVIDING TOKEN-BASED CLASSIFICATION DEVICE INFORMATIONOctober 2013Zhang et al 707 7378566102System method automating spoken dialogue serviceOctober 2013Bangalore et al 704 270 16092043Apparatuses method training operating speech recognition systemsJuly 2000Squires et al 704 2515960394Method speech command recognition dynamic assignment probabilities according state controlled applicationsSeptember 1999Gould et al 704 270 15920836Word recognition system using language context current cursor position affect recognition probabilitiesJuly 1999Gould et al 704 2515909666Speech recognition system creates acoustic models concatenating acoustic models individual wordsJune 1999Gould et al 704 251 References Bengio LeCun Scaling learning algorithms towards AI Large-Scale Kernel Machines MIT Press 41 pages 2007 Bengio et al neural probabilistic language model Journal Machine Learning Research 3 1137-1155 2003 Brants et al Large language models machine translation Proceedings Joint Conference Empirical Methods Natural Language Processing Computational Language Learning 10 pages 2007 Collobert Weston Unified Architecture Natural Language Processing Deep Neural Networks Multitask Learning International Conference Machine Learning ICML 8 pages 2008 Collobert et al Natural Language Processing Almost Scratch Journal Machine Learning Research 12 2493-2537 2011 Dean et al Large Scale Distributed Deep Networks Neural Information Processing Systems Conference 9 pages 2012 Elman Finding Structure Time Cognitive Science 14 179-211 1990 Huang et al Improving Word Representations via Global Context Multiple Word Prototypes Proc Association Computational Linguistics 10 pages 2012 Mikolov Zweig Linguistic Regularities Continuous Space Word Representations submitted NAACL HLT 6 pages 2012 Mikolov et al Empirical Evaluation Combination Advanced Language Modeling Techniques Proceedings Interspeech 4 pages 2011 Mikolov et al Extensions recurrent neural network language model IEEE International Conference Acoustics Speech Signal Processing ICASSP pp 5528-5531 May 22-27 2011 Mikolov et al Neural network based language models highly inflective languages Proc ICASSP 4 pages 2009 Mikolov et al Recurrent neural network based language model Proceedings Interspeech 4 pages 2010 Mikolov et al Strategies Training Large Scale Neural Network Language Models Proc Automatic Speech Recognition Understanding 6 pages 2011 Mikolov RNNLM Toolkit Faculty Information Technology FIT Brno University Technology online 2010-2012 retrieved Jun 16 2014 Retrieved Internet 3 pages Mikolov Statistical Language Models based Neural Networks PhD thesis Brno Univer-sity Technology 133 pages 2012 Mnih Hinton Scalable Hierarchical Distributed Language Model Advances Neural Information Processing Systems 21 MIT Press 8 pages 2009 Morin Bengio Hierarchical Probabilistic Neural Network Language Model AISTATS 7 pages 2005 Rumelhart et al Learning representations back-propagating errors Nature 323 533-536 1986 Turian et al MetaOptimize projects wordreprs Metaoptimize com online captured Mar 7 2012 Retrieved Internet using Wayback Machine 2 pages Turlan et al Word Representations Simple General Method Semi-Supervised Learning Proc Association Computational Linguistics 384-394 2010 Turney Measuring Semantic Similarity Latent Relational Analysis Proc International Joint Conference Artificial Intelligence 6 pages 2005 Zweig Burges Microsoft Research Sentence Completion Challenge Microsoft Research Technical Report MSR-TR-2011-129 7 pages Feb 20 2011 Primary Examiner Abebe Daniel D Attorney Agent Firm Fish Richardson P C Parent Case Data CROSS-REFERENCE RELATED APPLICATIONThis application claims benefit 35 U C 119 e U Patent Application 61 752 911 filed Jan 15 2013 entitled Computing Vector Representations incorporated reference herein entirety Claims claimed 1 system comprising classifier implemented one computers comprising embedding function layer configured receive input comprising plurality words surround unknown word sequence words map plurality words numeric representation high-dimensional space classifier layer configured process numeric representation input generate respective word score word pre-determined set words wherein respective word scores represents predicted likelihood corresponding word unknown word instructions process word vocabulary words using embedding function layer obtain respective numeric representation word vocabulary high-dimensional space associate word vocabulary respective numeric representation word high-dimensional space 2 system claim 1 wherein numeric representations continuous representations represented using floating-point numbers 3 system claim 1 wherein positions representations high-dimensional space reflect semantic similarities syntactic similarities words represented representations 4 system claim 1 wherein embedding layer maps plurality words respective floating point vector outputs single merged vector combination respective floating point vectors 5 system comprising classifier implemented one computers comprising embedding function layer configured receive input comprising input word map input word numeric representation high-dimensional space plurality classifiers wherein classifiers corresponds respective position surrounding input word sequence words wherein plurality classifiers configured process numeric representation input word generate respective word score word pre-determined set words wherein respective word scores represents predicted likelihood corresponding word found corresponding position relative input word instructions process word vocabulary words using embedding function layer obtain respective numeric representation word vocabulary high-dimensional space associate word vocabulary respective numeric representation word high-dimensional space 6 system claim 5 wherein numeric representations continuous representations represented using floating-point numbers 7 system claim 5 wherein positions representations high-dimensional space reflect semantic similarities syntactic similarities words represented representations 8 system claim 5 wherein embedding layer maps input word floating point vector 9 method assigning respective point high-dimensional space word vocabulary words method comprising obtaining set training data wherein set training data comprises sequences words training plurality classifiers embedding function set training data wherein embedding function receives input word maps input word numeric representation high-dimensional space accordance set embedding function parameters wherein classifiers corresponds respective position surrounding input word sequence words wherein classifiers processes numeric representation input word generate respective word score word pre-determined set words wherein respective word scores represents predicted likelihood corresponding word found corresponding position relative input word wherein training embedding function comprises obtaining trained values embedding function parameters processing word vocabulary using embedding function accordance trained values embedding function parameters generate respective numerical representation word vocabulary associating word vocabulary respective numeric representation word high-dimensional space 10 method claim 9 wherein numeric representations continuous representations represented using floating-point numbers 11 method claim 9 wherein positions representations high-dimensional space reflect semantic similarities syntactic similarities words represented representations 12 method claim 9 wherein embedding function maps input word floating point vector 13 method claim 9 wherein training plurality classifiers embedding function comprises preforming backpropagation training technique obtain trained values embedding function parameters 14 method assigning points high dimensional space word vocabulary words method comprising obtaining set training data wherein set training data comprises sequences words training classifier embedding function set training data wherein embedding function receives plurality words surrounding unknown word sequence words maps plurality words numeric representation accordance set embedding function parameters wherein classifier processes numeric representation sequence words generate respective word score word pre-determined set words wherein respective word scores measure predicted likelihood corresponding word unknown word wherein training embedding function comprises obtained trained values embedding function parameters processing word vocabulary using embedding function accordance trained values embedding function parameters generate respective numerical representation word vocabulary high-dimensional space associating word vocabulary respective numeric representation word high-dimensional space 15 method claim 14 wherein numeric representations continuous representations represented using floating-point numbers 16 method claim 14 wherein positions representations high-dimensional space reflect semantic similarities syntactic similarities words represented representations 17 method claim 14 wherein embedding function maps plurality words respective floating point vector outputs single merged vector combination respective floating point vectors 18 method claim 14 wherein training classifier embedding function comprises preforming backpropagation training technique obtain trained values embedding function parameters Description BACKGROUNDThis specification relates computing numeric representations words order predict one unknown words sequence words e g sentence phrase systems convert one words sequence numeric representations SUMMARYIn general one innovative aspect subject matter described specification embodied system includes classifier implemented one computers comprising embedding function layer configured receive input comprising plurality words surround unknown word sequence words map plurality words numeric representation high-dimensional space classifier layer configured process numeric representation input generate respective word score word pre-determined set words wherein respective word scores represents predicted likelihood corresponding word unknown word instructions process word vocabulary words using embedding function layer obtain respective numeric representation word vocabulary high-dimensional space associate word vocabulary respective numeric representation word high-dimensional space embodiments optionally include one following features numeric representations continuous representations represented using floating-point numbers Positions representations high-dimensional space reflect semantic similarities syntactic similarities words represented representations embedding layer map plurality words respective floating point vector outputs single merged vector combination respective floating point vectors general one innovative aspect subject matter described specification embodied system includes classifier implemented one computers comprising embedding function layer configured receive input comprising input word map input word numeric representation high-dimensional space plurality classifiers wherein classifiers corresponds respective position surrounding input word sequence words wherein plurality classifiers configured process numeric representation input word generate respective word score word pre-determined set words wherein respective word scores represents predicted likelihood corresponding word found corresponding position relative input word instructions process word vocabulary words using embedding function layer obtain respective numeric representation word vocabulary high-dimensional space associate word vocabulary respective numeric representation word high-dimensional space embodiments optionally include one following features numeric representations continuous representations represented using floating-point numbers Positions representations high-dimensional space reflect semantic similarities syntactic similarities words represented representations embedding layer map input word floating point vector general another innovative aspect subject matter described specification embodied methods include actions obtaining set training data wherein set training data comprises sequences words training plurality classifiers embedding function set training data wherein embedding function receives input word maps input word numeric representation high-dimensional space accordance set embedding function parameters wherein classifiers corresponds respective position surrounding input word sequence words wherein classifiers processes numeric representation input word generate respective word score word pre-determined set words wherein respective word scores represents predicted likelihood corresponding word found corresponding position relative input word wherein training embedding function comprises obtaining trained values embedding function parameters processing word vocabulary using embedding function accordance trained values embedding function parameters generate respective numerical representation word vocabulary associating word vocabulary respective numeric representation word high-dimensional space embodiments aspect include corresponding systems apparatus computer programs configured perform actions methods encoded computer storage devices embodiments optionally include one following features numeric representations continuous representations represented using floating-point numbers Positions representations high-dimensional space reflect semantic similarities syntactic similarities words represented representations embedding layer map input word floating point vector general another innovative aspect subject matter described specification embodied methods include actions obtaining set training data wherein set training data comprises sequences words training classifier embedding function set training data wherein embedding function receives plurality words surrounding unknown word sequence words maps plurality words numeric representation accordance set embedding function parameters wherein classifier processes numeric representation sequence words generate respective word score word pre-determined set words wherein respective word scores measure predicted likelihood corresponding word unknown word wherein training embedding function comprises obtained trained values embedding function parameters processing word vocabulary using embedding function accordance trained values embedding function parameters generate respective numerical representation word vocabulary high-dimensional space associating word vocabulary respective numeric representation word high-dimensional space embodiments aspect include corresponding systems apparatus computer programs configured perform actions methods encoded computer storage devices embodiments optionally include one following features numeric representations continuous representations represented using floating-point numbers Positions representations high-dimensional space reflect semantic similarities syntactic similarities words represented representations embedding layer map plurality words respective floating point vector outputs single merged vector combination respective floating point vectors system one computers configured perform particular operations actions virtue software firmware hardware combination installed system operation causes cause system perform actions One computer programs configured perform particular operations actions virtue including instructions executed data processing apparatus cause apparatus perform actions Particular embodiments subject matter described specification implemented realize one following advantages Unknown words sequences words effectively predicted surrounding words known Words surrounding known word sequence words effectively predicted Numerical representations words vocabulary words easily effectively generated numerical representations reveal semantic syntactic similarities relationships words represent using word prediction system two-layer architecture parallelizing training process word prediction system effectively trained large word corpuses e g corpuses contain order 200 billion words resulting higher quality numeric representations obtained training systems relatively smaller word corpuses words represented high-dimensional spaces e g spaces order 1000 dimensions resulting higher quality representations words represented relatively lower-dimensional spaces Additionally time required train word prediction system greatly reduced details one embodiments subject matter specification set forth accompanying drawings description features aspects advantages subject matter become apparent description drawings claims BRIEF DESCRIPTION DRAWINGSFIG 1 block diagram example word prediction system FIG 2 flow diagram example process predicting word based surrounding words FIG 3 block diagram another example word prediction system FIG 4 flow diagram example process predicting words surround word sequence words FIG 5 flow diagram example process generating numeric representations high-dimensional space vocabulary words Like reference numbers designations various drawings indicate like elements DETAILED DESCRIPTIONThis specification generally describes systems used generate numeric representations words high-dimensional space numeric representations continuous high-dimensional representations e words represented floating point numbers high-dimensional space e g high-dimensional vectors floating point numbers systems trained positions representations high-dimensional space generated systems reflect semantic syntactic similarities words represent FIG 1 block diagram example word prediction system 100 word prediction system 100 example system implemented computer programs one computers one locations systems components techniques described implemented word prediction system 100 receives inputs generates predicted output based received inputs particular inputs words surround unknown word sequence words e g phrase sentence predicted output respective score predetermined set words implementations input words tokenized received system e g known compounds e g New York City entity names treated single word system score generated words prediction likely corresponding word unknown word example sequence words X1X2X3 words X1 X3 known word X2 word prediction system 100 receive input words X1 X3 generate score predetermined set words prediction likely corresponding word unknown word X2 example word prediction system 100 receive sequence words 104 respective word data stores 102 word data store 102 stores words respective position sequence words includes unknown word example word position sequence words unknown word data stores 102 may store words position N 1 1 N respectively N predetermined integer value Thus sequence words 104 received word prediction system 100 words positions N 1 1 N particular sequence words unknown word position word prediction system 100 uses sequence words 104 predict output e vector scores includes respective score word set words represents predicted likelihood corresponding word found position sequence word prediction system 100 includes embedding function 106 classifier 110 embedding function 106 receives sequence words accordance set embedding function parameters applies transformation words maps words continuous high-dimensional numeric representation example embedding function 106 apply transformation words 104 map words floating point representation 108 Embedding functions described detail reference FIG 2 classifier 110 receives numeric representation generated embedding function 106 predicts value field word score vector accordance values classifier parameters fields word score vector corresponds respective word set words example classifier 110 generate word score vector 112 floating point representation 108 classifier 110 multiclass multilabel classifier e g multiclass logistic regression classifier multiclass support vector machine classifier Bayesian classifier implementations instead classifier 110 concept term scoring system 100 include ranking function orders words based numeric representation generated embedding function 106 e order predicted likelihood word position ranking function may e g hinge-loss ranking function pairwise ranking function generated word score vectors stored predicted word store 114 used immediate purpose FIG 2 flow diagram example process 200 predicting word based surrounding words convenience process 200 described performed system one computers located one locations example word prediction system e g word prediction system 200 FIG 2 appropriately programmed perform process 200 system obtains set input words step 202 set input words words sequence words includes unknown word whose value predicted sequence includes unknown word position set input words may words position N 1 1 N sequence implementations input words tokenized received system e g known compounds treated single word system system processes words using embedding function step 204 generate numeric representation words example embedding function may combining embedding function combining embedding function maps word sequence words respective continuous high-dimensional representation e g respective high-dimensional vector floating point numbers based current parameter values embedding function e g stored lookup table merges respective floating point vectors single merged vector combining embedding function merge respective floating point vectors using linear function e g sum average weighted linear combination respective floating point vectors using nonlinear function e g component-wise maximum norm-constrained linear combination example order identify respective floating point vectors parallel embedding function may use single lookup table multiple different lookup tables simplified example ordered list Atlanta Hotel parallel embedding function may map Atlanta vector 0 1 0 2 0 3 Hotel 0 4 0 5 0 6 output sum two vectors e 0 5 0 7 0 9 system processes numeric representations using classifier step 206 predict output set words classifier predicts output based values set parameters numeric representation output given word prediction value variable corresponds word e g score word represents predicted likelihood word unknown word position implementations system process numeric representation input words using ranking function instead classifier predict ranking words according predicted likelihood words unknown word sequence process 200 performed predict scores input desired output known e sequence words one word known process 200 also performed inputs set training data e set inputs output predicted system known order train system e determine optimal values parameters classifier deep network example process 200 performed repeatedly inputs selected set training data part backpropagation training technique determines optimal values parameters Generally inputs set training data sequences words every word sequence known part training process label predicted classifier particular sequence words set training data different known desired label particular sequence words e word scores generated classifier align actual value unknown word classifier adjust parameters reduce expected error particular input using conventional gradient based methods Furthermore part backpropagation method classifier sends error signal embedding function allows embedding function adjust parameters successive stages backpropagation circumstances e g large sets training data training process parallelized variety ways example training process parallelized using one techniques parallelizing training machine learning model described Large Scale Distributed Deep Networks Jeffrey Dean et al Neural Information Processing Systems Conference 2012 FIG 3 block diagram another example word prediction system 300 word prediction system 300 example system implemented computer programs one computers one locations systems components techniques described implemented word prediction system 300 receives input generates predicted output based received input particular input word output respective score vector position surrounding position input word sequence words implementations input words tokenized received system e g input word known compound treated single word system score vector given position includes respective score predetermined set words score prediction likely word word position sequence example sequence words X1X2X3 word X2 known words X1 X3 word prediction system 300 receive input word X2 words X1 X3 generate score predetermined set words prediction likely corresponding word found corresponding position sequence relative input word e corresponding word unknown word X1 unknown word X3 example word prediction system 300 receive input word 304 word data store 302 input word 304 word position sequence words words positions sequence e words positions N 1 1 N known e predicted word prediction system 300 word prediction system 300 uses input word 304 predict output e positions N 1 1 N word prediction 300 generates vector scores includes respective score word set words word prediction system 300 includes embedding function 306 set classifiers 310 embedding function 306 receives input word accordance set embedding function parameters applies transformation word maps word continuous high-dimensional numeric representation example embedding function 306 apply transformation input word 304 map word floating point representation 308 Embedding functions described detail reference FIG 4 classifiers 310 receives numeric representation generated embedding function 306 predicts value field respective word score vector accordance values respective set classifier parameters Generally classifiers 310 different values classifier parameters word score vector corresponds respective position sequence words example word score vector 312 includes values predetermined set words prediction likely corresponding word word position N sequence classifiers 310 multiclass multilabel classifier e g multiclass logistic regression classifier multiclass support vector machine classifier Bayesian classifier implementations instead classifiers 310 concept term scoring system 300 include ranking functions order words based numeric representation generated embedding function 106 e order predicted likelihood word corresponding position ranking function may e g hinge-loss ranking function pairwise ranking function generated word score vectors stored respective predicted word store 314 used immediate purpose FIG 4 flow diagram example process 400 predicting words surround known word sequence words convenience process 400 described performed system one computers located one locations example word prediction system e g word prediction system 300 FIG 3 appropriately programmed perform process 400 system obtains input word step 402 input word word position sequence words implementations input words tokenized received system e g input word known compound treated single word system system processes word using embedding function step 404 generate numeric representation word high-dimensional space embedding function maps word continuous high-dimensional representation e g high-dimensional vector floating point numbers example embedding function may map word cat vector 0 1 0 5 0 2 word tablet vector 0 3 0 9 0 0 based current parameter values embedding function e g stored lookup table system processes numeric representation using set classifiers step 406 predict output set positions surrounding position e words positions N 1 1 N sequence classifier predicts output respective position sequence based values set parameters numeric representation output given position sequence set predicted values variables correspond respective word pre-determined set words e g score word represents predicted likelihood word word position implementations system process numeric representation input words using ranking functions instead classifiers predict ranking words according predicted likelihood words corresponding position sequence Like process 200 process 400 performed inputs desired output known e input word surrounding words known inputs set training data e input word surrounding words known part training process part training process label predicted one classifiers particular input word set training data different known desired label particular word e word scores generated classifier align value word corresponding position classifier adjust parameters reduce expected error particular input using conventional gradient based methods Furthermore part backpropagation method classifier sends error signal embedding function allows embedding function adjust parameters successive stages backpropagation described reference FIG 2 training process parallelized variety ways word prediction system 100 word prediction system 300 trained parameters embedding function 106 embedding function 306 adjusted numeric representations produced embedding functions used variety purposes input classifier example training word prediction system 100 word prediction system 300 generate trained values embedding function parameters embedding function 106 embedding function 306 e g described reference FIG 5 numeric representations produced embedding functions encode many useful regularities positions representations high-dimensional space reflect syntactic similarities e g showing virtue positions numerical representations word space words similar word small include words smaller tiny smallest semantic similarities e g showing word queen similar words king prince Furthermore encoded regularities numeric representations may show word king similar word queen sense word prince similar word princess alternatively word king similar word prince word queen similar word princess Advantageously operations performed numeric representations identify words desired relationship words particular vector subtraction vector addition operations performed floating point vectors generated embedding function accordance trained values parameters embedding function used determine relationships words example order identify word similar relationship word word B word C following operation may performed vectors representing words B C vector vector B vector C example operation vector King vector Man vector Woman may result vector closest vector representation word Queen FIG 5 flow diagram example process 500 generating numeric representations high-dimensional space vocabulary words convenience process 500 described performed system one computers located one locations example word prediction system e g word prediction system 100 FIG 1 word prediction system 300 FIG 3 appropriately programmed perform process 500 system obtains set training data step 502 set training data includes sequences known words e g sentences phrases implementations training data tokenized received system e g known compounds treated single word system system performs training process train one classifiers embedding function step 504 e g embedding function 106 classifier 110 FIG 1 embedding function 306 set classifiers 310 FIG 3 training process performed repeatedly inputs selected set training data part backpropagation training technique determines trained values parameters classifier part training process label predicted classifier particular sequence words set training data different known desired label particular sequence words e word scores generated classifier align actual value unknown word words sequence classifier adjust parameters reduce expected error particular input using conventional gradient based methods Furthermore part backpropagation method classifier sends error signal embedding function allows embedding function adjust parameters successive stages backpropagation system processes word vocabulary words using embedding function step 506 accordance trained embedding function parameters embedding function generate respective continuous numeric representation words high-dimensional space example numeric representations may floating point vectors e high-dimensional vectors floating point values system associates word vocabulary respective numeric representation word step 508 system store associations e g data set words associated representations described numeric representations generated accordance trained parameters embedding function numeric representations may reflect certain semantic similarities words represent positions representations high-dimensional space reflect semantic syntactic similarities represented words Additionally predictions generated system 100 system 300 e predictions likelihoods words found particular positions sequences words used variety ways example system 100 system 300 trained used predict missing words various positions sentences another example system 100 system 300 used question answer system predict answer known question predict question known answer system 100 system 300 trained known question-answer pairs used predict question part answer known answer part question known Embodiments subject matter functional operations described specification implemented digital electronic circuitry tangibly-embodied computer software firmware computer hardware including structures disclosed specification structural equivalents combinations one Embodiments subject matter described specification implemented one computer programs e one modules computer program instructions encoded tangible non transitory program carrier execution control operation data processing apparatus Alternatively addition program instructions encoded artificially generated propagated signal e g machine-generated electrical optical electromagnetic signal generated encode information transmission suitable receiver apparatus execution data processing apparatus computer storage medium machine-readable storage device machine-readable storage substrate random serial access memory device combination one term data processing apparatus encompasses kinds apparatus devices machines processing data including way example programmable processor computer multiple processors computers apparatus include special purpose logic circuitry e g FPGA field programmable gate array ASIC application specific integrated circuit apparatus also include addition hardware code creates execution environment computer program question e g code constitutes processor firmware protocol stack database management system operating system combination one computer program may also referred described program software software application module software module script code written form programming language including compiled interpreted languages declarative procedural languages deployed form including stand-alone program module component subroutine unit suitable use computing environment computer program may need correspond file file system program stored portion file holds programs data e g one scripts stored markup language document single file dedicated program question multiple coordinated files e g files store one modules sub programs portions code computer program deployed executed one computer multiple computers located one site distributed across multiple sites interconnected communication network processes logic flows described specification performed one programmable computers executing one computer programs perform functions operating input data generating output processes logic flows also performed apparatus also implemented special purpose logic circuitry e g FPGA field programmable gate array ASIC application specific integrated circuit Computers suitable execution computer program include way example based general special purpose microprocessors kind central processing unit Generally central processing unit receive instructions data read memory random access memory essential elements computer central processing unit performing executing instructions one memory devices storing instructions data Generally computer also include operatively coupled receive data transfer data one mass storage devices storing data e g magnetic magneto optical disks optical disks However computer need devices Moreover computer embedded another device e g mobile telephone personal digital assistant PDA mobile audio video player game console Global Positioning System GPS receiver portable storage device e g universal serial bus USB flash drive name Computer readable media suitable storing computer program instructions data include forms non-volatile memory media memory devices including way example semiconductor memory devices e g EPROM EEPROM flash memory devices magnetic disks e g internal hard disks removable disks magneto optical disks CD ROM DVD-ROM disks processor memory supplemented incorporated special purpose logic circuitry provide interaction user embodiments subject matter described specification implemented computer display device e g CRT cathode ray tube LCD liquid crystal display monitor displaying information user keyboard pointing device e g mouse trackball user provide input computer kinds devices used provide interaction user well example feedback provided user form sensory feedback e g visual feedback auditory feedback tactile feedback input user received form including acoustic speech tactile input addition computer interact user sending documents receiving documents device used user example sending web pages web browser user's client device response requests received web browser Embodiments subject matter described specification implemented computing system includes back end component e g data server includes middleware component e g application server includes front end component e g client computer graphical user interface Web browser user interact implementation subject matter described specification combination one back end middleware front end components components system interconnected form medium digital data communication e g communication network Examples communication networks include local area network LAN wide area network WAN e g Internet computing system include clients servers client server generally remote typically interact communication network relationship client server arises virtue computer programs running respective computers client-server relationship specification contains many specific implementation details construed limitations scope invention may claimed rather descriptions features may specific particular embodiments particular inventions Certain features described specification context separate embodiments also implemented combination single embodiment Conversely various features described context single embodiment also implemented multiple embodiments separately suitable subcombination Moreover although features may described acting certain combinations even initially claimed one features claimed combination cases excised combination claimed combination may directed subcombination variation subcombination Similarly operations depicted drawings particular order understood requiring operations performed particular order shown sequential order illustrated operations performed achieve desirable results certain circumstances multitasking parallel processing may advantageous Moreover separation various system modules components embodiments described understood requiring separation embodiments understood described program components systems generally integrated together single software product packaged multiple software products Particular embodiments subject matter described embodiments within scope following claims example actions recited claims performed different order still achieve desirable results one example processes depicted accompanying figures necessarily require particular order shown sequential order achieve desirable results certain implementations multitasking parallel processing may advantageous - Previous Patent Efficient exploitati Next Patent Automatic disclosure - Home Search Services Communities Help Contact us Advertise Site 2004-2015 FreePatentsOnline com rights reserved Privacy Policy Terms Use SumoBrain Solutions Company"),
('I Let IBMs Robot Chef Tell Me What to Cook for a Week', "Sign Sign upMatt O'Leary May 2212 minNext storyNext storyThe author chose make story unlisted means people link see sure want share Yes show sharing optionsI Let IBM Robot Chef Tell Cook Week Share Twitter Share FacebookI Let IBM Robot Chef Tell Cook WeekPhoto Jeff Kubina Flickr CC BY-SA 2 0Originally published www howwegettonext com ve following IBM Watson project like food may noticed growing excitement among chefs gourmands molecular gastronomists one aspect development main Watson project artificial intelligence engineers built answer questions native language questions phrased way people normally talk stilted way search engine like Google understands far worked Watson helping nurses doctors diagnose illnesses also managed major Jeopardy win Chef Watson developed alongside Bon Appetit magazine several world finest flavor-profilers launched beta enabling mash recipes according ingredients choosing receive taste-matching advice reportedly fail world foremost tech luminaries conspiracy theorists bit skeptical wiseness going used allowing tell make fridge full unloved leftovers seems like inoffensive enough place start decided put test employed food writer well decade ve also spent good part last nine years working kitchens Figuring use spare ingredients become quite commonplace professional life ve also developed healthy disregard recipes anything sources inspiration annoyance purposes experiment willing follow along try ingredient least mind m going let Watson tell eat week ve spent good amount time playing around app found m going follow instructions letter possible audience willing testers food intend best recreating recipes plate Still m going try test bit want see whether save time kitchen also whether amazing suggestions dazzling taste matches help use things fridge whether going try get buy load stuff really need lot work gone creation app lot expertise useable human beings understand recipes want eat Let find disclaimer start Chef Watson isn great telling stuff actually ready cooked need use common sense Take advice advice inspiration flavors really count Monday Tailgating Corn Salmon SandwichMy first impression app intuitive pretty simple use ve added ingredient suggests number flavor matches types dishes moods including off-the-wall ones like Mother Day Choose options actual recipes begin bunch right screen selected salmon corn opted wildly suggestive Tailgating corn salmon sandwich recipe page links original Bon Appetit dish inspired m lange accompanied couple pictures battery disclaimers stating Chef Watson really wants suggest ideas rather tell eat presumably stop people want try cooking fiberglass example launching win fee cases salmon tailgating recipe seemed pretty straightforward couple nice touches page regard usability swap ingredients might stock others Watson suggest seems fond adding celery root dishes first attempt decided follow Watson advice almost didn garlic chile sauce managed make presumably functional analog garlic chili sauce change made involved adding broad beans like broad beans prep employed nearly unconscious bit initiative namely cooked salmon entirely likely Watson seemed case suggesting use raw salmon Monday night m mood anything mind-bending Team Watson ruined tailgater pig-headed insistence cooked fish m sorry Although m sorry know actually really good dish first unsure basil seemed like bit afterthought wasn sure lime zest necessary cold salmon salad burger bun isn really easy sell damn d make sandwich missing substance overall made enough two small buns teamed nice bit Korean-spiced pickled cucumber side worked well fellow diner deemed fine little uninteresting yes maybe could done bit sharpness depth maybe little computer told make flavor wackiness overall Well done Hint Definitely add broad beans totally worked mull tailgating might mean Tuesday Spanish Blood Sausage PorridgeIt day two Chef Watson guest slot kitchen things get interesting Buoyed yesterday Tailgating Salmon Sandwich success decided give Watson something sink digital teeth supply one ingredient blood sausage also specified main style really knew wasn expecting dessert m honest ve read appetizing recipes blood sausage porridge Even inclusion word Spanish doesn anything fancy bit concerningly recipe Watson extrapolated one Rye Porridge Morels replacing rye rice mushroom sausage original chicken livers single potato one tomato Still maybe would brilliant unlike yesterday ran problems wasn sure many tomatoes potatoes Watson expected ingredients list says one method suggests many also soak tomato boiling water first although makes sense original mushroom-centric method Additionally Wastson offered whimsical instruction cook tomatoes potatoes presumably long feel like lot butter involved recipe rather much liquid recommended eight cups stock one-and-a-half rice actually got bit fed four stopped adding Forty 50 minutes cooking time bit long directly extracted rye recipe mere trifles dish tasted great lovely blend flavors textures thanks blood sausage potato butter works brilliantly tomato top nice touch proves Watson functionality suggest one ingredient find fridge use initiative bit ll left something lovely buttery Lovely buttery Well done Watson Wednesday Diner Cod PizzaWhen read recipe wondered whether going Watson Diner cod pizza three words really belong together ingredients list seemed like supermarket sweep recipe ve actually made meal know think anything might remember classic 1978 George Romero-directed horror film called Dawn Dead 2004 remake following paradigm shift running zombies 28 Days Later suffered critically impression remake always d called something different Zombies Go Shopping instance every single person saw would loved viewers thought seemed unauthentic gathered essentially unfair criticism See also recent RoboCop remake call CyberSwede vs Detroit meal culinary Dawn Dead Watson called something pizza would utterly perfect emphatically isn pizza much common pizza cake something radishes cod ginger olives tomatoes green onions pizza crust work remarkably well clear fully expected throw meal away website curry delivery already open phone ate two pizzas taste like nothing earth addition Comt cheese chives sort genius absurdity makes people millionaires however nervous give one pregnant fianc e ingredients weird sure d suffer really strange psychic reaction baby would grow extremely contrary careful recipe preparation ve found Watson doesn tell assure fish cooked tell long pre-bake crust base kinds things really important need make sure dish cooked properly takes longer might expect m writing Sweden home ridiculous pizza yet feeling show recipe chef ordinarily thinks nothing piling kilo kebab meat B arnaise sauce bread serving cardboard box side salad fermented cabbage would balk tell ve gone far would loss think m going take Dragon Den instead Watson know m going cope normal recipes little holiday together re changing way think food Thursday Fall Celery Sour Cream Parsley Lemon TacoFollowing yesterday culinary epiphany keen keep cool head critical eye Chef Watson decided road-test one theory article found Internet mentioned frequently discarded items American fridges celery sour cream fresh herbs lemons Let dwell much luxury problems aspect imagine people everywhere world lamenting amount sour cream flat-leaf parsley toss focus instead Watson admittedly tricky-sounding shopping list Immediately add shrimp tortillas salsa verde salsa verde recommended un-Watsoned recipe courtesy Bon Appetit fantastic nothing like salsa verde know love capers dill pickles anchovies iteration required bit simmer super-spicy delicious cheat use normal tomatoes instead tomatillos think made huge difference marinade shrimp unusual like lot Watson recommends used ton butter hefty wallop old friend kosher salt ve worked chef several years unfazed appearance salt butter recipes re make things taste nice However getting away fact bought stick butter start week already gone assembled tacos good uncontroversial dining companion deemed salsa bit spicy liked kick gave dish sour cream calmed bit struck bit shame fire barbecue two minutes worth cooking time May sun shining heck recipe absurd yesterday Absolutely memorable Sadly think Would make m sorry Watson probably tacos good ultimately worth prep hassle Friday Mexican Mushroom LasagnaBefore start want get impression love affair reached height passion Wednesday Watson absolutely isn consistently impressed software intelligence ease use audacity suggestions flavor-matching incredible really works probably won save money won make thin won teach actually cook stuff work stage distinctly impressive worthwhile project give go prepared coax something workable every Today took long time find meat-free recipe didn came contain sort meat selected meat option didn want include took recipe sausage lasagne one-and-a-half pounds sausage removed sausage replaced turkey mince Maybe someone needs tell Watson neither sausages turkeys grow trees much tinkering submitting resubmitting recipe ended lasagne topped sort creamy mashed potato sauce easy profoundly smart use ingredients lasagne world aesthetically appealing dish astonishingly flavored week revelations think ll making cheese sauce way point onwards Top marks essence kind sums Watson need tinker bit find something usable may need make want put mashed potato lasagne leap faith re going actually go want app full benefit ll consume lot dairy products might find daydreaming nice simple unadorned salads decide go all-in suggestions tell us make pizza cod ginger radishes know going taste amazing One gladly suggest workable recipe blood sausage porridge walk without much hassle gives crazy option ingredient designed make lives food enthusiasts interesting earth Watson going good friends point forward even speak every day wait introduce others though m going consume smoothies week Seriously even look butter next days m probably going puke Originally published www howwegettonext com RecommendRecommendedBookmarkBookmarkedShareMoreFollowFollowingHow Get NextInspiring stories people places building future Created Steven Johnson edited Ian Steadman Duncan GeereBlockedUnblockFollowFollowingMatt O'LearyPublished May 22 rights reserved author"),
('6 Tricks I Learned From The OTTO Kaggle Challenge', 'Request storySign Sign upChristophe Bourguignat May 244 minNext storyNext storyThe author chose make story unlisted means people link see sure want share Yes show sharing options6 Tricks Learned OTTO Kaggle Challenge Share Twitter Share Facebook6 Tricks Learned OTTO Kaggle ChallengeHere things learned OTTO Group Kaggle competition chance team great Kaggle Master Xavier Conort french community whole active Hacking Otto Group Challenge1 Stacking blending averagingTeaming Xavier opportunity practice ensembling technics heavily used stacking added initial set 93 features new features predictions N different classifiers Random Forest GBM Neural Networks retrained P classifiers 93 N features finally made weighted average P outputs tested two tricks average use harmonic mean instead geometric mean improved bit scorewhen adding N features add logit prediction instead prediction didn improve things case 2 CalibrationThis one great functionalities last scikit-learn version 0 16 allows rescale classifier predictions taking observations predicted within segments e g 0 3 04 comparing actual truth ratio observation e g 0 23 means rescaling needed mini notebook explaining use calibration demonstrating well worked OTTO challenge data Using Scikit-Learn calibration3 GridSearchCV RandomizedSearchCVAt beginning competition appeared quickly Gradient Boosting Trees one best performing algorithm provided find right hyper parameters scikit-learn implementation important hyper parameters learning_rate shrinkage parameter n_estimators number boosting stages max_depth limits number nodes tree best value depends interaction input variables min_samples_split min_samples_leaf also way control depth trees optimal performance also discovered two parameters crucial competition must admit never paid attention challenge namely subsample fraction samples used fitting individual base learners max_features number features consider looking best split problem find way quickly find best hyperparameters combination first discovered GridSearchCV makes exhaustive search specified parameter ranges always scikit-learn convenient programming interface handling example smoothly cross-validation parallel distributing search However number parameters tune range large discover best ones acceptable time frame mind typically sleeping e 7 10 hours fall back option used RandomizedSearchCV appeared 0 14 version method search done randomly subspace parameters gives generally good results described paper able find suitable parameter set within hours Note competitors like french kaggler Amine used Hyperopt hyperparameters optimization Grid Search Calibration scikit-learn4 XGBoostXGBoost Gradient Boosting implementation heavily used kagglers understand never used hot topic discussed forum decided look even main interface R Python API didn use yet XGBoost much faster scikit-learn gave better prediction remain sure part toolblox gentle XGBoost tutorial5 Neural NetworksSomeone posted forum guess really time try neural nets right opportunity play neural networks first time Several implementations used competitors H2O Keras cxxnet personally used Lasagne Main challenges fine tune number layers number neurons dropout learning rate notebook learned Fine Tuning Lasagne Neural Network6 Bagging ClassifierOne secret competition run several times algorithm random selection observations features take average output easily discovered scikit-learn BaggingClassifier meta-estimator hides tedious complexity looping model fits random subsets selection averaging exposes easy fit predict_proba entry points RecommendRecommendedBookmarkBookmarkedShareMoreBlockedUnblockFollowFollowingChristophe BourguignatData enthusiast BigData DataScience MachineLearning FrenchData KagglePublished May 24 rights reserved author'),
('Setting up a GPU accelerated deep learning environment on Elementary OS freya with CUDA', "Skip content wolfchimneyrock Sidebar Search Recent Posts Setting eOS Freya Anaconda hybrid graphics laptop GPU accelerated deep learning Recent Comments Roger Setting eOS Freya Anaco Archives May 2015 Categories deep learning linux machine learning programming Meta Register Log Entries RSS Comments RSS WordPress com Setting eOS Freya Anaconda hybrid graphics laptop GPU accelerated deep learning May 25 2015May 27 2015 wolfchimneyrockanaconda CUDA deep learning elementary OS eOS keras linux machine learning pyCUDA python Theano goal set new Lenovo y50 integrated Intel GPU used interactive UI tasks NVIDIA GPU computation tasks way entire memory NVIDIA GPU available computation using GPU computation remains idle doesn drain battery burn belly purchased laptop learn GPU-accelerated machine learning deep learning Initial research topic led understanding situation nVidia Linux perilous encountered many frustrating dead-end black screen situations freezes eventually help osdf little bit additional trial error able get things working order operations critical many problems encountered due necessary steps wrong order instructions worked may work directions assume fresh install eOS scratch Disable discrete graphics BIOS y50 means choosing UMA Graphic Configuration Graphic Device Install eOS Freya normally wrote ISO usb drive Unetbootin nVidia GPU still disabled add Nouveau blacklist per osdf guide next steps copied main guide Blacklist driver conflict NVIDIA binary driver e g nouveau Create file blacklist-file-drivers conf etc modprobe d contents blacklist nouveau blacklist lbm-nouveau blacklist amd76x_edac blacklist vga16fb blacklist rivatv blacklist rivafb blacklist nvidiafb blacklist nvidia-173 blacklist nvidia-96 blacklist nvidia-current blacklist nvidia-173-updates blacklist nvidia-96-updates alias nvidia nvidia_current_updates alias nouveau alias lbm-nouveau Save file reboot enter BIOS setup re-enable Hybrid graphics reboot back eOS Luckily next step osdf require since nVidia toolkit 7 0 compiled GCC 4 8 skip part setting GCC 4 6 alternative compiler verify X11 running Intel GPU still nVidia GPU available perform following command lspci grep 'NVIDIA VGA' output something like showing Intel NVIDIA present 00 02 0 VGA compatible controller Intel Corporation 4th Gen Core Processor Integrated Graphics Controller rev 06 01 00 0 3D controller NVIDIA Corporation GM107M GeForce GTX 860M rev a2 next command requires glxinfo isn installed default eOS rectified running sudo apt-get install mesa-utils able run glxinfo egrep OpenGL vendor OpenGL renderer output something like confirming Intel GPU used OpenGL vendor string Intel Open Source Technology Center OpenGL renderer string Mesa DRI Intel R Haswell Mobile point went ahead updated eOS recent packages sudo apt-get update sudo apt-get upgrade sudo apt-get dist-upgrade installed recent 3 19 8 Linux kernel Note recent better kernel may available read copy paste following terminal unless want follow pedantically cd wget http kernel ubuntu com kernel-ppa mainline v3 19 8-vivid linux-headers-3 19 8-031908-generic_3 19 8-031908 201505110938_amd64 deb wget http kernel ubuntu com kernel-ppa mainline v3 19 8-vivid linux-headers-3 19 8-031908_3 19 8-031908 201505110938_all deb wget http kernel ubuntu com kernel-ppa mainline v3 19 8-vivid linux-image-3 19 8-031908-generic_3 19 8-031908 201505110938_amd64 deb sudo dpkg -i linux-headers-3 19 8 deb sudo dpkg -i linux-image-3 19 8 deb installed tlp power management sudo add-apt-repository ppa linrunner tlp sudo apt-get update sudo apt-get install tlp tlp-rdw sudo service tlp start fun part Download 346 72 driver NVIDIA website http www nvidia com Download driverResults aspx 84721 en-us file extension run meant run terminal want install NVIDIA driver overwrite Intel openGL driver Nvidia naughtily default parameters driver no-opengl-files chmod x NVIDIA-Linux-x86_64-346 72 run Press CTRL-ALT-F1 switch text terminal login run command location downloaded driver sudo service lightdm stop sudo NVIDIA-Linux-x86_64-346 72 run --dkms --no-opengl-files --accept-license driver install finishes able reboot verify driver installed sudo modprobe nvidia-uvm nvidia-smi output something like Mon May 25 18 45 28 2015 ------------------------------------------------------ NVIDIA-SMI 346 72 Driver Version 346 72 ------------------------------- ---------------------- ---------------------- GPU Name Persistence-M Bus-Id Disp Volatile Uncorr ECC Fan Temp Perf Pwr Usage Cap Memory-Usage GPU-Util Compute M 0 GeForce GTX 860M 0000 01 00 0 N N N 40C P0 N N 9MiB 4095MiB N Default ------------------------------- ---------------------- ---------------------- ----------------------------------------------------------------------------- Processes GPU Memory GPU PID Type Process name Usage 0 C G Supported ----------------------------------------------------------------------------- time install CUDA framework flag no-opengl-libs time overwrite Intel openGL cd wget http developer download nvidia com compute cuda 7_0 Prod local_installers cuda_7 0 28_linux run chmod x cuda_7 0 28_linux run sudo cuda_7 0 28_linux run --no-opengl-libs --toolkit --samples Note didn say install driver step n ask make sure say warn installing driver Good News installed driver previous step probably worth reading NVIDIA CUDA Linux getting started guide Add CUDA library path profile echo export CUDA_HOME usr local cuda-7 0 bashrc echo export LD_LIBRARY_PATH CUDA_HOME lib64 LD_LIBRARY_PATH bashrc echo export PATH CUDA_HOME bin PATH bashrc exit terminal re-open load new path Install G compiler compile run samples sudo apt-get install g cd NVIDIA_CUDA-7 0_Samples make 1_Utilities deviceQuery deviceQuery explore samples make sure run point sign NVIDIA registered developer program next step download install cuDNN requires registered Take survey agree conditions download cuDNN Library Linux User Guide Code Samples install library tar -xvf cudnn-6 5-linux-x64 tgz cd cudnn-6 5 sudo cp h CUDA_HOME include sudo cp CUDA_HOME lib64 sudo cp CUDA_HOME lib64 sudo cp h usr local include sudo cp usr local lib sudo cp usr local lib sudo ldconfig install cuDNN sample program tar -xvf cudnn-sample-v2 tgz cd cudnn-sample-v2 make goes well shouldn get errors Run sample mnistCUDNN output Loading image data one_28x28 pgm Performing forward propagation Resulting weights Softmax 4 05186e-07 0 999404 2 21383e-07 1 20837e-08 0 000587085 5 06682e-08 2 80583e-06 1 47965e-06 3 56051e-06 2 46337e-07 Loading image data three_28x28 pgm Performing forward propagation Resulting weights Softmax 4 67739e-05 5 83973e-07 1 76501e-06 0 75859 1 06138e-11 0 24133 2 62157e-10 1 11104e-05 3 39113e-07 1 88164e-05 Loading image data five_28x28 pgm Performing forward propagation Resulting weights Softmax 3 22452e-10 8 69774e-10 3 73033e-12 3 2219e-07 2 67785e-11 0 999992 4 58862e-06 5 08385e-10 9 35238e-07 1 87656e-06 Result classification 1 3 5 Test passed cuDNN installed working time install Anaconda free Anaconda Accelerate paid free academic affiliation install free Anaconda go http continuum io downloads download 64 bit installation run bash Anaconda-2 2 0-Linux-x86_64 sh conda update conda easiest library get started numbapro free 30 day trial included already Anaconda Accelerate let install conda install numbapro haven installed chrome yet time unable install chrome download http www google com chrome reason use repository method wget -q -O - https dl-ssl google com linux linux_signing_key pub sudo apt-key add - sudo sh -c 'echo deb http dl google com linux chrome deb stable main etc apt sources list d google-chrome list' sudo apt-get update sudo apt-get install google-chrome-stable run Chrome least let become default browser m sure eOS default web browser Midori task running IPython notebook Let test things IPython notebook Go http nbviewer ipython org gist harrism f5707335f40af9463c43 click download notebook icon top right run location downloaded ipython notebook mandelbrot_numbapro ipynb open web browser window mandelbrot set calculation example Go notebook everything installed working correctly time CUDA numbapro run least 10x-20x faster CPU autojit version install theano pyCUDA keras GPU accelerated deep learning libraries require license ll first need install git python-dev cmake check sudo apt-get install git sudo apt-get install python-dev sudo add-apt-repository ppa george-edison55 cmake-3 x sudo apt-get update sudo apt-get install cmake sudo apt-get install check install boost sudo apt-get install libboost-all-dev install mako cython nose pip install mako pip install cython conda install nose h5py http docs h5py org en latest index html conda install h5py Theano http deeplearning net software theano sudo pip install theano --upgrade run tests http deeplearning net software theano tutorial using_gpu html verify Theano using GPU Note Theano doesn play nice ATLAS way dynamically link openBLAS Anaconda pyCUDA http mathema tician de software pycuda cd git clone --recursive http git tiker net trees pycuda git cd pycuda configure py make sudo make install maxAs Maxwell GPU assembler https github com NervanaSystems maxas cd git clone https github com NervanaSystems maxas git cd maxas perl Makefile PL make sudo make install install keras https github com fchollet keras cd git clone https github com fchollet keras git cd keras sudo python setup py install Go ahead run unit tests cd keras test python test_models py python test_constraints py python test_save_weights py example take long time cd keras examples THEANO_FLAGS mode FAST_RUN device gpu floatX float32 python cifar10_cnn py get deep learning Share TwitterFacebookGoogleLike Like Loading Setting eOS Freya Anaconda hybrid graphics laptop GPU accelerated deep learning One thought Setting eOS Freya Anaconda hybrid graphics laptop GPU accelerated deep learning Roger says Thanks posting lots gotcha may interested comparison integrated discrete GPU Toward GPUs mainstream analytic processing initial argument using simple scan-aggregate queries http hgpu org p 14029 LikeLike May 26 2015 7 30 pm Reply Leave Reply Cancel reply Enter comment Fill details click icon log Email required Address never made public Name required Website commenting using WordPress com account Log Change commenting using Twitter account Log Change commenting using Facebook account Log Change commenting using Google account Log Change Cancel Connecting Notify new comments via email Blog WordPress com Minnow Theme Follow Follow wolfchimneyrock Get every new post delivered Inbox Build website WordPress com d bloggers like"),
('Classifying Golf Players with K-Means and Hierarchical Clustering', 'DataBucket Harold Barbara analyze things Pages Home Monday May 25 2015 Classifying Types Players PGA Tour Clustering Methods http www amicnews com wp-content uploads 2015 04 Rory-McIlroy-and-Tiger-Woods-a-dream-Masters-pair-but-on-another-day jpg Classifying athletes relatively intuitive task team sports basketball traditional point shooting guards small power forwards center soccer goalkeepers defenders midfielders forwards individual sports golf PGA Tour last several years seen emerging players becoming athletic nimble hitting golf ball like never Stars Bubba Watson Rickie Fowler Rory Mcilroy revolutionized game hitting 300 yard drives allowing hit short iron approaches increase chances scoring low said veterans going anywhere Jim Furyk Ernie Els Jimmy Walker utilize accurate play shrewd course management keep young guns creates competitor tour consisting players distinctly contrasting styles quantitatively capture different styles Using 49 metrics pgatour com cover driving approaches fairways roughs scrambling putting seek classifying 2014 PGA Tour players using two clustering methods - K-Means Clustering Hierarchical Clustering K-Means Clustering K-means clustering type unsupervised learning algorithm set methods extrapolate information unlabeled data K-means clustering seeks segregate data K parts variances within K regions minimized case seeking segregate PGA Tour players groups based metrics indicate playing styles without actually labeled data kind style player key consideration determining number clusters K Usually done plotting within-group sum-of-squares WSS number clusters number clusters increases WSS decrease decrease segmentation creates closely-knitted region less fails result choose K expanding number clusters K 1 would insignificant change WSS Graphically slope plot generally becomes flatter Looking plot occurs K 4 grouped PGA Tour players four groups heat map provides overview characteristics four groups clustered using K-Means Red represents average values blue represents average values looking values essentially describe 4 groups follows Elite Group - players complete game players consist young players amazing power shown superior Par-5 performance dark red driving distance dark blue also elite short game putting scrambling metrics blue group surprisingly group consists Bubba Watson Rory Mcilroy Adam Scott Dustin Johnson Average Group - players mediocre steady statistical categories extreme colors throughout entire row except blues Driving Accuracy Consecutive Fairways testament consistency players group Examples players include Brandt Snedeker Bo Van Pelt Graeme McDowell Henrik Stenson Make Greens Group - players perform well tour mainly due poor driving approach abilities red among driving approach metrics Fortunately make ability around greens scrambling putting performances blue Prominent players include Ian Poulter Lee Westwood Ernie Els Suck Greens Group - players fine fairways poorly greens slightly red fairways hit comes putting scrambling metrics putts round scrambling putts 5 10 ft players poorly explains group performs worst Louis Oosthuizen Martin Laird Davis Love III belong group 2014 Hierarchical Clustering Instead segregating dataset top hierarchical clustering takes bottom-up approach Single data points merge adjacent data points form clusters continue merge closest clusters data merged one cluster creates tree maps different players group together different ways determining closest cluster Complete method measures furthest distance elements cluster Ward method measures increase sum-of-squares two clusters merge Mcquitty method specifies distance new cluster another cluster average pre-merged clusters cluster found Ward method gave us interesting interpretable results consistent 4 groups found K-means clustering examined 3 highest layers hierarchical clustering trees get 4 subgroups hierarchical clustering method Ward Method Ward method leads balanced partitioning players said green group heat map still residual group consisting players without many standout features three groups described follows black group describes players drive ball short accurately solid game around greens strong scrambling metrics Lee Westwood Luke Donald Ian Poulter belong partition blue group consists poor performers average tee lackluster around green poor scrambling percentages particular putting within 10 feet similar I-suck-on-the-greens group red group contains elite players strong scoring performances particular approach game stellar close proximity hole iron shots Top players like Rory Mcilroy Jason Day Jordan Spieth belong clustering results say Though sizes groups differ among different clustering methods seems three groups players consistently identified elite stars - young guns established top game combining powerful tee shots superior short game consistent lads - established veterans hit ball far still perform well tour fine-tuned short game short game newbies - players hit tee shots well performed poorly 2014 due inability approach handle greens well interesting whether clusters remain end 2015 season Also interesting note Tiger Woods part analysis due injuries 2014 group would belong 2015 given subpar performances far remains question answered end year Posted Data Bucket 7 08 PM Email BlogThis Share Twitter Share Facebook Share Pinterest comments Post Comment Older Post Home Subscribe Post Comments Atom Subscribe Posts Atom Posts Comments Atom Comments Follow Email Blog Archive 2015 5 May 4 Classifying Types Players PGA Tour likely go jail soccer players perform transferring Serving First Matter Tennis April 1 Powered Blogger'),
('New Keyword Extractor: smarter and more flexible extractor', 'DOCS PRICING BLOG LOGIN Previous New Keyword Extractor smarter flexible excited announce made several improvements Keyword Extractor try free need MonkeyLearn account module extracts keywords text English Keywords compounded one words defined important topics content used index data generate tag clouds searching keyword extraction algorithm employs statistical algorithms natural language processing technology analyze content identify relevant keywords customers already love extractor using empower kind applications decided take step make improvements make module powerful keyword extractor market Improvements response extractor include count positions text different keywords extracts Also using parameters API turn following behaviors Number keywords Set maximum amount keywords extract defaults 10 Capitalization Lowercase given keywords defaults 0 false Company Names Expand company names text appears word Google part appears Google Inc word Google expanded Google Inc Defaults 0 false Stemming Take words base form order get better results defaults 1 true Acronyms Expand acronyms full form example US United States tokens appear given text Defaults 0 false Hyphenated Keep char appears inside name example Ferrara Wolf Defaults 0 false Example Input Google Inc American multinational technology company specializing Internet-related services products include online advertising services search cloud computing software profits derived AdWords online advertising service places advertising near list search results Google founded Larry Page Sergey Brin Ph D students Stanford University Together 14 shares control 56 stockholder voting power supervoting stock incorporated Google privately held company September 4 1998 Initial Public Offering announced August 19 2004 first day IPO share price set 85 closed 100 34 price gain 18 Output 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 result relevance 0 980 count 2 positions_in_text 630 708 keyword Initial Public Offering relevance 0 980 count 2 positions_in_text 130 247 keyword online advertising services relevance 0 882 count 3 positions_in_text 0 331 570 keyword Google Inc relevance 0 490 count 1 positions_in_text 27 keyword multinational technology company relevance 0 490 count 1 positions_in_text 500 keyword stockholder voting power relevance 0 294 count 1 positions_in_text 717 keyword share price relevance 0 294 count 1 positions_in_text 368 keyword Sergey Brin relevance 0 294 count 1 positions_in_text 396 keyword Ph D students relevance 0 294 count 1 positions_in_text 533 keyword supervoting stock relevance 0 294 count 1 positions_in_text 76 keyword Internet-related services read API reference keyword extractor Feedback Would love feedback new smarter keyword extractor suggestions please let us know Share EmailFacebookTwitterLinkedInTumblrReddit Federico Pascual 2015-05-26T14 05 06 00 00May 20th 2015 News 0 Comments Author Federico Pascual Maker Disruptor Entrepreneur Win-Win Mindset Techie Business Marketing TEDxDurazno Speaker Related Posts Permalink Gallery New modules Product Sentiment Analysis Extractors New modules Product Sentiment Analysis Extractors Permalink Gallery MonkeyLearn PyCon Startup Row event MonkeyLearn PyCon Startup Row event Permalink Gallery MonkeyLearn featured Twitter Dev Blog MonkeyLearn featured Twitter Dev Blog Permalink Gallery Black Friday MonkeyLearn 80 every new subscription plan Black Friday MonkeyLearn 80 every new subscription plan Permalink Gallery GUEST POST AngularJS provider MonkeyLearn GUEST POST AngularJS provider MonkeyLearn Leave Reply Cancel reply Recent Posts New Keyword Extractor smarter flexible Hacker News categorizer MonkeyLearn getting whole story Analyzing news headlines across globe Kimono MonkeyLearn New modules Product Sentiment Analysis Extractors MonkeyLearn PyCon Startup Row event Categories Applications News 2015 MonkeyLearn LLC rights reserved Send Email Address Name Email Address Cancel Post sent - check email addresses Email check failed please try Sorry blog cannot share posts email'),
("Stanford's Deep Learning Javascript library is learning to draw Reddit Mascot (Snoo), just go to link and wait for Image Regresssion procedure", "ConvnetJS demo Image Painting demo treats pixels image learning problem takes x y position grid learns predict color point using regression r g b It's bit like compression since image information encoded weights network almost certainly practical kind Note entire ConvNetJS definition shown textbox gets eval 'd create network feel free fiddle parameters hit reload found empirically interestingly deeper networks tend work much better task given fixed parameter budget Report questions bugs suggestions karpathy Choose image Original Image Neural Network output Learning rate learning rate probably decreased time slide left let network better overfit training data It's nice worry overfitting upload image click Choose File click images load Go back ConvNetJS"),
('Data mining algorithms in plain English', "Skip content rayli net Learn Teach Grow Menu Home Articles Projects Resume Contact May 2 2015May 18 2015 Ray Li Top 10 data mining algorithms plain English Today m going explain plain English top 10 influential data mining algorithms voted 3 separate panels survey paper know work find hope ll blog post springboard learn even data mining waiting Let get started Contents1 C4 52 k-means3 Support vector machines4 Apriori5 EM6 PageRank7 AdaBoost8 kNN9 Naive Bayes10 CARTInteresting ResourcesNow turn Update 16-May-2015 Thanks Yuval Merhav Oliver Keyes suggestions ve incorporated post 1 C4 5 C4 5 constructs classifier form decision tree order C4 5 given set data representing things already classified Wait classifier classifier tool data mining takes bunch data representing things want classify attempts predict class new data belongs example Sure suppose dataset contains bunch patients know various things patient like age pulse blood pressure VO2max family history etc called attributes Given attributes want predict whether patient get cancer patient fall 1 2 classes get cancer won get cancer C4 5 told class patient deal Using set patient attributes patient corresponding class C4 5 constructs decision tree predict class new patients based attributes Cool decision tree Decision tree learning creates something similar flowchart classify new data Using patient example one particular path flowchart could Patient history cancer Patient expressing gene highly correlated cancer patients Patient tumors Patient tumor size greater 5cm bottomline point flowchart question value attribute depending values gets classified find lots examples decision trees supervised unsupervised supervised learning since training dataset labeled classes Using patient example C4 5 doesn learn patient get cancer won get cancer told first generated decision tree uses decision tree classify might wondering C4 5 different decision tree systems First C4 5 uses information gain generating decision tree Second although systems also incorporate pruning C4 5 uses single-pass pruning process mitigate over-fitting Pruning results many improvements Third C4 5 work continuous discrete data understanding specifying ranges thresholds continuous data thus turning continuous data discrete data Finally incomplete data dealt ways use C4 5 Arguably best selling point decision trees ease interpretation explanation also quite fast quite popular output human readable used popular open-source Java implementation found OpenTox Orange open-source data visualization analysis tool data mining implements C4 5 decision tree classifier Classifiers great make sure checkout next algorithm clustering 2 k-means k-means creates groups set objects members group similar popular cluster analysis technique exploring dataset Hang cluster analysis Cluster analysis family algorithms designed form groups group members similar versus non-group members Clusters groups synonymous world cluster analysis example Definitely suppose dataset patients cluster analysis would called observations know various things patient like age pulse blood pressure VO2max cholesterol etc vector representing patient Look basically think vector list numbers know patient list also interpreted coordinates multi-dimensional space Pulse one dimension blood pressure another dimension forth might wondering Given set vectors cluster together patients similar age pulse blood pressure etc Want know best part tell k-means many clusters want K-means takes care rest k-means take care rest k-means lots variations optimize certain types data high level something like k-means picks points multi-dimensional space represent k clusters called centroids Every patient closest 1 k centroids hopefully won closest one ll form cluster around nearest centroid k clusters patient member cluster k-means finds center k clusters based cluster members yep using patient vectors center becomes new centroid cluster Since centroid different place patients might closer centroids words may change cluster membership Steps 2-6 repeated centroids longer change cluster memberships stabilize called convergence supervised unsupervised depends would classify k-means unsupervised specifying number clusters k-means learns clusters without information cluster observation belongs k-means semi-supervised use k-means think many issue key selling point k-means simplicity simplicity means generally faster efficient algorithms especially large datasets gets better k-means used pre-cluster massive dataset followed expensive cluster analysis sub-clusters k-means also used rapidly play k explore whether overlooked patterns relationships dataset smooth sailing Two key weaknesses k-means sensitivity outliers sensitivity initial choice centroids One final thing keep mind k-means designed operate continuous data ll need tricks get work discrete data used ton implementations k-means clustering available online Apache Mahout Julia R SciPy Weka MATLAB SAS decision trees clustering didn impress re going love next algorithm 3 Support vector machines Support vector machine SVM learns hyperplane classify data 2 classes high-level SVM performs similar task like C4 5 except SVM doesn use decision trees Whoa hyper-what hyperplane function like equation line fact simple classification task 2 features hyperplane line turns SVM perform trick project data higher dimensions projected higher dimensions SVM figures best hyperplane separates data 2 classes example Absolutely simplest example found starts bunch red blue balls table balls aren mixed together could take stick without moving balls separate stick see new ball added table knowing side stick ball predict color balls table stick represent balls represent data points red blue color represent 2 classes stick represents simplest hyperplane line coolest part SVM figures function hyperplane things get complicated Right frequently balls mixed together straight stick won work work-around Quickly lift table throwing balls air balls air thrown right way use large sheet paper divide balls air might wondering cheating Nope lifting table equivalent mapping data higher dimensions case go 2 dimensional table surface 3 dimensional balls air SVM using kernel nice way operate higher dimensions large sheet paper still called hyperplane function plane rather line Note Yuval re 3 dimensions hyperplane must plane rather line found visualization super helpful Reddit also 2 great threads ELI5 ML subreddits balls table air map real-life data ball table location specify using coordinates example ball could 20cm left edge 50cm bottom edge Another way describe ball x y coordinates 20 50 x y 2 dimensions ball deal patient dataset patient could described various measurements like pulse cholesterol level blood pressure etc measurements dimension bottomline SVM thing maps higher dimension finds hyperplane separate classes Margins often associated SVM margin distance hyperplane 2 closest data points respective class ball table example distance stick closest red blue ball margin key SVM attempts maximize margin hyperplane far away red ball blue ball way decreases chance misclassification SVM get name Using ball table example hyperplane equidistant red ball blue ball balls data points called support vectors support hyperplane supervised unsupervised supervised learning since dataset used first teach SVM classes SVM capable classifying new data use SVM SVM along C4 5 generally 2 classifiers try first classifier best cases due Free Lunch Theorem addition kernel selection interpretability weaknesses used many implementations SVM popular ones scikit-learn MATLAB course libsvm next algorithm one favorites 4 Apriori Apriori algorithm learns association rules applied database containing large number transactions association rules Association rule learning data mining technique learning correlations relations among variables database example Apriori Let say database full supermarket transactions think database giant spreadsheet row customer transaction every column represents different grocery item best part applying Apriori algorithm learn grocery items purchased together k association rules power find items tend purchased together frequently items ultimate goal get shoppers buy Together items called itemsets example probably quickly see chips dip chips soda seem frequently occur together called 2-itemsets large enough dataset much harder see relationships especially re dealing 3-itemsets precisely Apriori helps might wondering Apriori works getting nitty gritty algorithm ll need define 3 things first size itemset want see patterns 2-itemset 3-itemset etc second support number transactions containing itemset divided total number transactions itemset meets support called frequent itemset third confidence conditional probability item given certain items itemset good example given chips itemset 67 confidence soda also itemset basic Apriori algorithm 3 step approach Join Scan whole database frequent 1-itemsets Prune itemsets satisfy support confidence move onto next round 2-itemsets Repeat repeated itemset level reach previously defined size supervised unsupervised Apriori generally considered unsupervised learning approach since often used discover mine interesting patterns relationships wait Apriori also modified classification based labelled data use Apriori Apriori well understood easy implement many derivatives hand algorithm quite memory space time intensive generating itemsets used Plenty implementations Apriori available popular ones ARtool Weka Orange next algorithm difficult understand look next algorithm 5 EM data mining expectation-maximization EM generally used clustering algorithm like k-means knowledge discovery statistics EM algorithm iterates optimizes likelihood seeing observed data estimating parameters statistical model unobserved variables OK hang explain m statistician hopefully simplification correct helps understanding concepts make way easier statistical model see model something describes observed data generated example grades exam could fit bell curve assumption grades generated via bell curve k normal distribution model Wait distribution distribution represents probabilities measurable outcomes example grades exam could fit normal distribution normal distribution represents probabilities grade words given grade use distribution determine many exam takers expected get grade Cool parameters model parameter describes distribution part model example bell curve described mean variance Using exam scenario distribution grades exam measurable outcomes followed bell curve distribution mean 85 variance 100 need describe normal distribution 2 parameters mean variance likelihood Going back previous bell curve example suppose bunch grades told grades follow bell curve However re given grades sample deal know mean variance grades estimate using sample likelihood probability bell curve estimated mean variance results bunch grades words given set measurable outcomes let estimate parameters Using estimated parameters hypothetical probability outcomes called likelihood Remember hypothetical probability existing grades probability future grade re probably wondering probability Using bell curve example suppose know mean variance re told grades follow bell curve chance observe certain grades often observed probability general terms given parameters let estimate outcomes observed probability us Great difference observed unobserved data Observed data data saw recorded Unobserved data data missing number reasons data could missing recorded ignored etc kicker data mining clustering important us looking class data point missing data know class interpreting missing data way crucial applying EM task clustering EM algorithm iterates optimizes likelihood seeing observed data estimating parameters statistical model unobserved variables Hopefully way understandable best part optimizing likelihood EM generates awesome model assigns class labels data points sounds like clustering EM help clustering EM begins making guess model parameters follows iterative 3-step process E-step Based model parameters calculates probabilities assignments data point cluster M-step Update model parameters based cluster assignments E-step Repeat model parameters cluster assignments stabilize k convergence supervised unsupervised Since provide labeled class information unsupervised learning use EM key selling point EM simple straight-forward implement addition optimize model parameters also iteratively make guesses missing data makes great clustering generating model parameters Knowing clusters model parameters possible reason clusters common cluster new data belongs EM without weaknesses though First EM fast early iterations slow later iterations Second EM doesn always find optimal parameters gets stuck local optima rather global optima used EM algorithm available Weka R implementation mclust package scikit-learn also implementation gmm module data mining Google Take look 6 PageRank PageRank link analysis algorithm designed determine relative importance object linked within network objects Yikes link analysis type network analysis looking explore associations k links among objects example prevalent example PageRank Google search engine Although search engine doesn solely rely PageRank one measures Google uses determine web page importance Let explain Web pages World Wide Web link rayli net links web page CNN vote added CNN page indicating rayli net finds CNN web page relevant doesn stop rayli net votes turn weighted rayli net importance relevance words web page voted rayli net increases rayli net relevance bottom line concept voting relevance PageRank rayli net vote CNN increases CNN PageRank strength rayli net PageRank influences much vote affects CNN PageRank PageRank 0 1 2 3 etc mean Although precise meaning PageRank number isn disclosed Google get sense relative meaning see bit like popularity contest sense websites relevant popular minds PageRank uber elegant way define applications PageRank PageRank specifically designed World Wide Web Think core PageRank really super effective way link analysis objects linked web pages 3 innovative applications PageRank Dr Stefano Allesina University Chicago applied PageRank ecology determine species critical sustaining ecosystems Twitter developed WTF Who-to-Follow personalized PageRank recommendation engine follow Bin Jiang Hong Kong Polytechnic University used variant PageRank predict human movement rates based topographical metrics London supervised unsupervised PageRank generally considered unsupervised learning approach since often used discover importance relevance web page use PageRank Arguably main selling point PageRank robustness due difficulty getting relevant incoming link Simply stated graph network want understand relative importance priority ranking relevance give PageRank try used PageRank trademark owned Google However PageRank algorithm actually patented Stanford University might wondering use PageRank m lawyer best check actual lawyer probably use algorithm long doesn commercially compete Google Stanford 3 implementations PageRank C OpenSource PageRank Implementation Python PageRank Implementation igraph network analysis package R powers combined 7 AdaBoost AdaBoost boosting algorithm constructs classifier probably remember classifier takes bunch data attempts predict classify class new data element belongs boosting Boosting ensemble learning algorithm takes multiple learning algorithms e g decision trees combines goal take ensemble group weak learners combine create single strong learner difference strong weak learner weak learner classifies accuracy barely chance popular example weak learner decision stump one-level decision tree Alternatively strong learner much higher accuracy often used example strong learner SVM example AdaBoost Let start 3 weak learners re going train 10 rounds training dataset containing patient data dataset contains details patient medical records question predict whether patient get cancer AdaBoost answers question round 1 AdaBoost takes sample training dataset tests see accurate learner end result find best learner addition samples misclassified given heavier weight higher chance picked next round One thing best learner also given weight depending accuracy incorporated ensemble learners right 1 learner round 2 AdaBoost attempts look best learner kicker sample patient training data influenced heavily misclassified weights words previously misclassified patients higher chance showing sample like getting second level video game start character killed Instead start level 2 focus efforts getting level 3 Likewise first learner likely classified patients correctly Instead trying classify let focus efforts getting misclassified patients best learner weighted incorporated ensemble misclassified patients weighted higher chance picked rinse repeat end 10 rounds re left ensemble weighted learners trained repeatedly retrained misclassified data previous rounds supervised unsupervised supervised learning since iteration trains weaker learners labelled dataset use AdaBoost AdaBoost simple algorithm relatively straight-forward program addition fast Weak learners generally simpler strong learners simpler means ll likely execute faster Another thing super elegant way auto-tune classifier since successive AdaBoost round refines weights best learners need specify number rounds Finally flexible versatile AdaBoost incorporate learning algorithm work large variety data used AdaBoost ton implementations variants scikit-learn ICSIBoost gbm Generalized Boosted Regression Models like Mr Rogers ll like next algorithm 8 kNN kNN k-Nearest Neighbors classification algorithm However differs classifiers previously described lazy learner lazy learner lazy learner doesn much training process store training data new unlabeled data input type learner look classify hand eager learner builds classification model training new unlabeled data input type learner feeds data classification model C4 5 SVM AdaBoost fit Unlike kNN eager learners C4 5 builds decision tree classification model training SVM builds hyperplane classification model training AdaBoost builds ensemble classification model training kNN kNN builds classification model Instead stores labeled training data new unlabeled data comes kNN operates 2 basic steps First looks closest labeled training data points words k-nearest neighbors Second using neighbors classes kNN gets better idea new data classified might wondering kNN figure closer continuous data kNN uses distance metric like Euclidean distance choice distance metric largely depends data even suggest learning distance metric based training data tons details papers kNN distance metrics discrete data idea transform discrete data continuous data 2 examples Using Hamming distance metric closeness 2 text strings Transforming discrete data binary features 2 Stack Overflow threads suggestions dealing discrete data KNN classification categorical data Using k-NN R categorical values kNN classify new data neighbors disagree kNN easy time neighbors class intuition neighbors agree new data point likely falls class ll bet guess things get hairy kNN decide class neighbors class 2 common techniques dealing Take simple majority vote neighbors Whichever class greatest number votes becomes class new data point Take similar vote except give heavier weight neighbors closer simple way use reciprocal distance e g neighbor 5 units away weight vote 1 5 neighbor gets away reciprocal distance gets smaller smaller exactly want supervised unsupervised supervised learning since kNN provided labeled training dataset use kNN Ease understanding implementing 2 key reasons use kNN Depending distance metric kNN quite accurate part story 5 things watch kNN get computationally expensive trying determine nearest neighbors large dataset Noisy data throw kNN classifications Features larger range values dominate distance metric relative features smaller range feature scaling important Since data processing deferred kNN generally requires greater storage requirements eager classifiers Selecting good distance metric crucial kNN accuracy used number kNN implementations exist MATLAB k-nearest neighbor classification scikit-learn KNeighborsClassifier k-Nearest Neighbour Classification R Spam Fuhgeddaboudit Read ahead learn next algorithm 9 Naive Bayes Naive Bayes single algorithm family classification algorithms share one common assumption Every feature data classified independent features given class independent mean 2 features independent value one feature effect value another feature example Let say patient dataset containing features like pulse cholesterol level weight height zip code features would independent value features effect dataset reasonable assume patient height zip code independent since patient height little zip code let stop features independent Sadly answer 3 feature relationships independent height increases weight likely increases cholesterol level increases weight likely increases cholesterol level increases pulse likely increases well experience features dataset generally independent ties next question called naive assumption features dataset independent precisely called naive generally case features independent Bayes Thomas Bayes English statistician Bayes Theorem named click link find Bayes Theorem nutshell theorem allows us predict class given set features using probability simplified equation classification looks something like Let dig deeper equation mean equation finds probability Class given Features 1 2 words see Features 1 2 probability data Class equation reads probability Class given Features 1 2 fraction fraction numerator probability Feature 1 given Class multiplied probability Feature 2 given Class multiplied probability Class fraction denominator probability Feature 1 multiplied probability Feature 2 example Naive Bayes great example taken Stack Overflow thread deal training dataset 1 000 fruits fruit Banana Orange classes fruit Long Sweet Yellow features see training dataset 500 bananas 400 long 350 sweet 450 yellow 300 oranges none long 150 sweet 300 yellow remaining 200 fruit 100 long 150 sweet 50 yellow given length sweetness color fruit without knowing class calculate probability banana orange fruit Suppose told unknown fruit long sweet yellow calculate probabilities 4 steps Step 1 calculate probability fruit banana let first recognize looks familiar probability class Banana given features Long Sweet Yellow succinctly exactly like equation discussed earlier Step 2 Starting numerator let plug everything Multiplying everything together equation get Step 3 Ignore denominator since ll calculations Step 4 similar calculation classes Since greater Naive Bayes would classify long sweet yellow fruit banana supervised unsupervised supervised learning since Naive Bayes provided labeled training dataset order construct tables use Naive Bayes could see example Naive Bayes involves simple arithmetic tallying counts multiplying dividing frequency tables calculated classifying unknown fruit involves calculating probabilities classes choosing highest probability Despite simplicity Naive Bayes surprisingly accurate example found effective spam filtering used Implementations Naive Bayes found Orange scikit-learn Weka R Finally check 10th algorithm 10 CART CART stands classification regression trees decision tree learning technique outputs either classification regression trees Like C4 5 CART classifier classification tree like decision tree classification tree type decision tree output classification tree class example given patient dataset might attempt predict whether patient get cancer class would either get cancer won get cancer regression tree Unlike classification tree predicts class regression trees predict numeric continuous value e g patient length stay price smartphone easy way remember Classification trees output classes regression trees output numbers Since ve already covered decision trees used classify data let jump right things compare C4 5 C4 5 CART Uses information gain segment data decision tree generation Uses Gini impurity confused Gini coefficient good discussion differences impurity coefficient available Stack Overflow Uses single-pass pruning process mitigate over-fitting Uses cost-complexity method pruning Starting bottom tree CART evaluates misclassification cost node vs without node cost doesn meet threshold pruned away decision nodes 2 branches decision nodes exactly 2 branches Probabilitically distributes missing values children Uses surrogates distribute missing values children supervised unsupervised CART supervised learning technique since provided labeled training dataset order construct classification regression tree model use CART Many reasons d use C4 5 also apply CART since decision tree learning techniques Things like ease interpretation explanation also apply CART well Like C4 5 also quite fast quite popular output human readable used scikit-learn implements CART decision tree classifier R tree package implementation CART Weka MATLAB also implementations Interesting Resources Apriori algorithm Data Mining made simple Google PageRank Earned Transferred 2 main differences classification regression trees AdaBoost Tutorial Ton References turn ve shared thoughts research around data mining algorithms want turn going give data mining try data mining algorithms heard weren list maybe question algorithm Let know think leaving comment right Like ve read please follow RSS feed subscribe mailing list get updates stuff cannot find blog Email Address First Name related posts Posted DataTagged data mining84 Comments Post navigation Previous PostHistory data mining 84 thoughts Top 10 data mining algorithms plain English Pingback 1 Top data mining algorithms plain English blog offeryour com Pingback Bookmarks May 17th Chris's Digital Detritus Joe Guy says May 17 2015 6 19 pm explanation SVM best ever seen Thanks Reply Raymond Li says May 17 2015 7 14 pm Thanks Joe Definitely appreciate owe lot threads Reddit Yuval linked post Reply Roger Huang says May 17 2015 7 10 pm Really snappy informative view data mining algorithms clicked whole ton links always mark resource done right Kudos Reply Raymond Li says May 17 2015 8 27 pm Thanks Roger m happy found snappy click-worthy Sometimes data mining resources bit dry side Reply Lakshminarayanan says May 17 2015 11 41 pm Thanks excellent compile looking starter Reply Ray Li says May 18 2015 12 19 pm Thanks Lakshminarayanan Reply Pingback Els 10 primers algoritmes del Big data explicats en paraules Blog d'estad stica oficial Pingback LessThunk com Top 10 data mining algorithms plain English recommended big data users Meghana says May 18 2015 2 57 numerous websites data mining algorithms gone one far best Explaining everything casual terms really helps beginners like examples definitely apt helpful Thank much made work lot easier Reply Ray Li says May 18 2015 12 26 pm m excited hear helped work Meghana really appreciate kind words Reply Vrashabh Irde says May 18 2015 3 36 awesome list Thanks Trying dabble ML simple know everything useful Reply Ray Li says May 18 2015 12 29 pm glad hear find useful Vrashabh Thank Reply Pingback Data Mining Algorithms Explained Stephen E Arnold Beyond Search Pingback Distilled News Data Analytics R Kyle says May 18 2015 9 23 Excellent man well explained Thanks Reply Ray Li says May 18 2015 12 32 pm pleasure Kyle Reply suanfazu says May 18 2015 9 47 Thanks excellent share Reply Ray Li says May 18 2015 12 35 pm pleasure Suanfazu Thanks exploring blog leaving kind words Reply Anonymous says May 18 2015 9 53 Hey great introduction would love see posts like community great way grasp concept algorithms diving hard math one thing though Step 2 Naive Bayes repeated P Long Banana twice third one P Yellow Banana Thanks Reply Ray Li says May 18 2015 12 45 pm Hi Anonymous Nice catch fixed one attribute fix totally agree understanding concepts algorithm hard math ve always felt using concepts examples platform understanding makes math part way easier Thanks Ray Reply Robert Klein says May 18 2015 9 59 great resource ve bookmarked Thanks work love using height-zip code illustrate independence go-to thing offer return heads-up API released ML preprocessing correlating themes unstructured information streams Hope useful Let us know think Thanks Reply Ray Li says May 18 2015 12 57 pm Thanks bookmarking heads-up Robert Reply Raghav says May 18 2015 12 46 pm Hello Ray Thanks great article looks like typo step 2 Naive Bayes One probabilities P Yellow Banana Thanks Reply Ray Li says May 18 2015 1 00 pm pleasure Raghav Thanks also letting know typo corrected Reply Jens says May 18 2015 1 00 pm Hello Raymond first kudos sum data mining algos ve exploring weeks mainly using scikit learn nltk python past days came idea create classifier able group products title corresponding product taxonomy crawled German product marketplace category landingpages created corpus consisting taxonomy tree node column set snowball stemmed relevant uni bigram keywords appx 50 per node extracted products category page comma separated column b would like build classifier idea mind could throw stemmed product titles classifier let return probable taxonomy node Could advise would appropriate one given task email corpus Hope get direction omit detours much trial error Looking forward reply Thanks great article Cheers Cologne Germany Jens Reply Ray Li says May 18 2015 9 05 pm Hi Jens Thanks kudos taking time leave comment Short answer question know sounds like bunch could learn example taught stemming Snowball framework Honestly m amazed tools like Snowball create stemming algorithms cool Longer answer found StackOverflow com stats stackexchange com reddit com forums invaluable learning researching simplifying algorithms make easier describe Sorry couldn help m working catch Ray Reply Jens says May 20 2015 6 48 Hi Ray thanks feedback found good solution meantime using naive bayes approach way regular contact form work htaccess authentication popping upon form submit Cheers Jens Reply Ray Li says May 20 2015 7 30 Awesome Also thanks heads contact form fixed small issue confirmation message fields displayed auth pop-up message successfully sends Reply Malhar says May 18 2015 1 21 pm goes bookmarks Excellent simple explanation Loved taken SVM would great put Neural network various kernels Reply Ray Li says May 18 2015 9 21 pm Definitely appreciate bookmark Malhar Thanks suggestion neural nets ll definitely diving one soon Reply Meghana says May 18 2015 11 37 pm Exactly concern Malhar looking information Neural Networks well Reply Serge says May 18 2015 6 04 pm Man really wish guide years ago trying hand unsupervised categorization email messages didn know terms google thing used LSM latent semantic mapping problem thousands words tens thousands emails N 2 matrix gets little hard handle computationally ended giving never considered using different algorithm pre-create groups would helped lot useful read Reply Ray Li says May 18 2015 9 31 pm Thanks reading kind words Serge Reply Pingback Data Scientist - Professional Data Science Singapore 10 Data Science Algorithms Explained English David says May 19 2015 5 27 pm Great article public service decision tree categorization matrix selecting right algorithm Reply Ray Li says May 20 2015 12 28 Thanks David good call selecting right algorithm readings far feel picking right one hardest part one main reasons attracted original survey paper despite bit outdated Might well dive ones panelists thought important figure use certainly lot learn m already ideas future posts Ray Reply D Lego says May 19 2015 5 48 pm Good post curious m write one version spanish theme Reply Ray Li says May 20 2015 12 34 Thank D Lego m curious email link Reply Pingback Data mining algorithms many code_blocks michael davies says May 20 2015 8 24 Great work Raymond Reply Ray Li says May 20 2015 12 57 pm Appreciate Michael Reply Sthitaprajna Sahoo says May 20 2015 9 04 Couldn ask simpler explanation good collection hoping posts Reply Ray Li says May 20 2015 1 00 pm pleasure Sthitaprajna Reply Pingback Data Mining Algorithms Explained Plain Language Artificial Intelligence Matters Stephen Oman says May 20 2015 10 36 really excellent article nice explanations Looking forward piece Artificial Neural Networks Reply Ray Li says May 20 2015 1 31 pm Thanks Stephen Reply Richard Grigonis says May 21 2015 9 13 Including Decision Forests would nice Reply Ray Li says May 21 2015 10 13 pm Although haven used one good one Richard Reply Pingback Top 10 data mining algorithms plain English Another Word Daniel Zilber says May 21 2015 12 03 pm Thanks write Reply Ray Li says May 21 2015 10 13 pm Appreciate Daniel Reply Pingback Els 10 primers algoritmes del Big data explicats en paraules Econometria aplicada Sylvio Allore says May 21 2015 10 26 pm Hello good review things undergraduates learn starting single example application predicting stock returns example example applying example naive Bayes predicting stock returns would useful listing set methods one find ML books Reply Ray Li says May 21 2015 11 00 pm Thanks Sylvio appreciate constructive comments Depth real-life applications certainly something improve article series Yep think deserves series Stay tuned Reply Ray Li says May 21 2015 10 31 pm Super excited Due comments sharing article reposted KDnuggets leading resource data mining http bit ly 1AoicbW way could ve happened without reading commenting sharing sincerest thank Reply Matt Cairnduff says May 22 2015 5 07 Echoing sentiments Ray tremendously useful resource gone straight bookmarks Really appreciate informal writing style well makes nice accessible easy share colleagues Reply Ray Li says May 22 2015 5 41 pm Thank Matt m glad found writing style accessible shareable Please share Reply Adriana Wilde says May 22 2015 5 36 Excellent blogpost accessible rather complete apart multilayer perceptrons hope ll touch follow post found useful refer NFL theorem list characteristics algorithm make suited one type problem another e g lazy learners faster training slower classifiers also liked explained algorithms supervised unsupervised learning things take account choosing classifier Wish read 5 years ago Thanks Reply Ray Li says May 22 2015 5 52 pm Hi Adriana Thank kind words think came across standard perceptron researching SVM Definitely thinking tackling MLPs recently buzz deep learning point Thanks insightful comment Ray Reply brian piercy says May 22 2015 8 00 awesome article learned 20 hours plowing SciKit Well done Reply Ray Li says May 22 2015 5 53 pm Appreciate Brian Reply david berneda says May 25 2015 3 04 Thanks lot Ray article clustering library sometime ago article encourages try expanding algorithms regards david Reply Ray Li says May 25 2015 1 11 pm pleasure David Reply Pingback Les liens de la semaine dition 133 French Coding Pingback 1 Time Management Key Kenechi Learns Code Martin Campbell says May 25 2015 11 25 pm fantastic article needed start attempting learn stuff ll shooting Kaggle rankings time well 100 000 90 000 perhaps Reply Ray Li says May 26 2015 12 45 pm Appreciate Martin m really happy hear helps get ball rolling increased Kaggle ranking would nice icing cake Reply Yolande Tra says May 26 2015 6 27 Excellent overview gift teaching complex topics down-to earth terms comment using data mining algorithm list classifiers concerned accuracy try use one end interested validation training Accuracy addressed SVM Adaboost Reply Ray Li says May 26 2015 12 50 pm Thank kind words Yolande good point accuracy ll definitely keep mind explore accuracy upcoming post Reply Maksim Gayduk says May 26 2015 8 43 didn quite understand part C4 5 pruning link provided says order decide whether prune tree calculates error rate pruned unpruned tree decides one leads lower limit confidence interval work okey already pruned trees start Usually decision tree algorhythms build tree reaches entrophy 0 means zero error rate zero upper limit confidence interval case tree never pruned using logic Reply Ray Li says May 26 2015 5 30 pm great question Maksim got thinking bunch unfortunately answer m satisfied investigation far indicates error rate training data distinct estimated error rate unseen data pointed confidence interval meant bound Based formula link given f 0 m also loss pruned tree could beat unpruned tree re CrossValidated StackOverflow might awesome place get question answered could even post link reference Reply Pingback solutions simple predictive analytics challenge Decision Management Community Ilan Sharfer says May 26 2015 12 42 pm Ray thanks lot really useful review algorithms already familiar others new surely helps one place practical application m interested data mining algorithm used investment portfolio selection based historical data decide stocks invest make timely buy sell orders recommend suitable algorithm Reply Ray Li says May 26 2015 6 33 pm pleasure Ilan ve come across algorithms writing article teach unfamiliar ones m planning go practical applications upcoming post Stay tuned one side note might already aware random walk hypothesis efficient-market hypothesis might interest doesn answer question alternate perspective predicting future returns based historical data Reply Zeeshan says May 26 2015 7 59 pm Awesome explanation Reply Ray Li says May 26 2015 8 29 pm Much appreciated Zeeshan Reply Lalit Patel says May 26 2015 11 09 pm excellent blog helping digest studied elsewhere Thanks lot Reply Ray Li says May 28 2015 8 07 Thank Lalit m happy hear blog helping studies Reply Phaneendra says May 28 2015 1 40 Fantastic post ray Nicely explained Helped enhancing understanding Please keep sharing knowledge helps Regards Phaneendra Reply Ray Li says May 28 2015 8 10 Thanks Phaneendra definitely way Reply Adrian Cuyugan says May 28 2015 7 01 good simple explanation Thank sharing Reply Ray Li says May 28 2015 8 11 Appreciate Adrian Reply Leave Reply Cancel reply email address published Required fields marked Name Email Website Please enter answer digits 1 14 Comment may use HTML tags attributes href title abbr title acronym title b blockquote cite cite code del datetime em q cite strike strong Notify followup comments via e-mail also subscribe without commenting Get free email updates Email Address First Name TopicsData Programming Life Blogging Search Recent CommentsRay Li Top 10 data mining algorithms plain EnglishRay Li Top 10 data mining algorithms plain EnglishRay Li Top 10 data mining algorithms plain EnglishAdrian Cuyugan Top 10 data mining algorithms plain EnglishPhaneendra Top 10 data mining algorithms plain EnglishLalit Patel Top 10 data mining algorithms plain EnglishRay Li Top 10 data mining algorithms plain EnglishZeeshan Top 10 data mining algorithms plain EnglishRay Li Top 10 data mining algorithms plain EnglishRay Li Top 10 data mining algorithms plain English Get free email updates Email Address First Name TopicsData Programming Life Blogging Follow Home Articles Projects Resume Contact Proudly powered WordPress Theme Sequential WordPress com"),
('Comparison of official test scores of current Image Captioning systems', 'Toggle navigation Worksheets Competitions Help Sign Competition Microsoft COCO Image Captioning Challenge Organized tl483 - Current server time May 28 2015 5 51 p m UTC Current Challenge March 15 2015 midnight UTC End Competition Ends Never Learn Details Phases Participate Results Forums Overview Evaluation Terms Conditions Microsoft COCO Image Captioning automatic generation captions images long-standing challenging problem artificial intelligence promote measure progress area carefully created Microsoft Common objects COntext MS COCO dataset provide resources training validation testing automatic image caption generation Currently MS COCO 2014 dataset contains one million captions 160 000 images CodaLab evaluation server provides platform measure performance validation held-out test set MS COCO Caption Evaluation API provided compute several performance metrics evaluate caption generation results details data collection evaluation metrics found paper Microsoft COCO Captions Data Collection Evaluation Server participate find instructions MS COCO website particular please see overview download format evaluate captions upload pages details MS COCO Caption Evaluation API used evaluate results software uses candidate reference captions applies sentence tokenization output several performance metrics including BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L METEOR CIDEr-D details found paper Microsoft COCO Captions Data Collection Evaluation Server Please refer MS COCO Terms Use submissions made public Challenge Start March 15 2015 midnight must logged participate competitions Sign submissions made public Challenge submissions made public Submission details Download submission View predict output log View predict error log View scoring output log View scoring error log Download evaluation output prediction step Download evaluation output scoring step Close Forum Survey Privacy Terms'),
('Trend (regime) change detection with t-test', 'Ensemble Blogging Beyond data signal statistics Thursday May 21 2015 Simple Regime Change Detection t-test always fun find trend time series data scenarios trend time series changes Detecting point trend change quite beneficial example immediately detect change revenue regime company valuable company detect point temperature device starts increasing possibly prevent catastrophe post aim touch surface know regime change detection trend change detecting literature use simple t-test rolling window approach detect regime change simple approach falls general category sequential statistical tests regime change detection also online algorithm meaning readily apply new measurement decide weather trend exist new measurement example semi-practical dataset use hourly measurement average duration second visitor spends blog data belongs three month period purpose post dataset cleaned slightly modified importantly seasonality removed summary focusing trend session duration goal detect trend changed Let start visualizing data Clearly somewhere around 1500 hour average time spent blog saturated believe root cause follows first two months put effort spreading website either asking people go visit sharing different social medias guess point blog found audience naive primitive advertising lost effect point growth trend stopped hope starts growing simple words Let first describe algorithm simple words start taking initial window data assuming trend change initial window tricky part one hand size window needs large enough establish good model hand small enough cover trend change fit line window data pick points window call test points Using t-test see test points trend statistically different trend points window hyposis rejected add test points current window continue new set test points continue till either find point hypothesis accepted reach end dataset Detection Let start 20 points currentWindowSize - 20 len - length df visit described simple words following part iterative removed loop put comments part find full code Github Fit line current window x0 - df time 1 currentWindowSize y0 - df visit 1 currentWindowSize c0 - lm y0 x0 Although modeling linear x0 y0 simply extended nonlinear functions x0 Pick data points test data fit line test set x1 - df time currentWindowSize 1 min currentWindowSize incrementalWindowSize len y1 - df visit currentWindowSize 1 min currentWindowSize incrementalWindowSize len c - lm y1 x1 Check similarity trend two lines statistically significant words want test null hypothesis slope c0 equal test slop c n length x1 SSR - sum c residuals 2 SSRx - sum x1-mean x1 2 beta0 - c0 coefficients 2 beta - c coefficients 2 tscore - beta-beta0 sqrt n-2 sqrt SSR SSRx tscore student-t distribution n-2 degree freedom Hence reject null hypothesis tau usually set 05 0 01 simple words inequality satisfied two windows trend case merge test points current window currentWindowSize - currentWindowSize incrementalWindowSize inequality true trend change test window following plot shows P tscore Red line shows log10 p-value 0 001 p-value 0 001 conservative algorithm detect around 1460 hour change regime seems good enough Testing created 9 artificial datasets test approach error function calculated follows residualChangeHour actualChangeHour - calculatedChangeHour table shows error dataset download dataset give try File name Actual Change Hour Calculated Change Hour Residual Change Hour webvisit_0 csv -- 1460 -- webvisit_1 csv 673 680 -7 webvisit_2 csv 917 920 -3 webvisit_3 csv 1067 1080 -13 webvisit_4 csv 1023 1040 -17 webvisit_5 csv 808 820 -12 webvisit_6 csv 812 820 -8 webvisit_7 csv 1031 1040 -9 webvisit_8 csv 875 880 -5 webvisit_9 csv 523 540 -17 Email ThisBlogThis Share TwitterShare FacebookShare Pinterest comments Post Comment Older Post Home Subscribe Post Comments Atom Data Scientist Base CRM Follow twitter Follow mamhamedFollow Linkedin email mhfirooz gmail Favorite Quote never thought writing reputation honor heart must reason compose --Beethoven models wrong useful --George Box Good Books Win Friends Influence People Lean Startup Second Machine Age Time Populars Bayesian network R Introduction Bayesian networks BNs type graphical model encode conditional probability different learning variables dir use Gaussian Mixture Model EM clustering universally used generative unsupervised clustering Gaussains Mixture Model GMM also known EM Clustering Simple Regime Change Detection t-test always fun find trend time series data scenarios trend time series changes Detecting p Using RANSAC Robust Regression Outlier defined point set points dataset follow dominant pattern data Due wide variety Goodness fit test R data scientist occasionally receive dataset would like know generative distribution dataset Blog Archive 2015 15 May 1 Simple Regime Change Detection t-test April 1 March 1 February 3 January 9 2014 7 December 3 June 1 May 2 January 1 Pageviews Simple template Powered Blogger'),
('The unreasonable effectiveness of Character-level Language Models (and why RNNs are still cool)', "Toggle navigation nbviewer FAQ IPython Jupyter Python 2 Kernel View Gist Download Notebook unreasonable effectiveness Character-level Language Models RNNs still cool Yoav Goldberg RNNs LSTMs Deep Learning rage recent blog post Andrej Karpathy great job explaining models train also provides impressive results capable great post interested natural language machine learning neural networks definitely read Go read come back You're back good Impressive stuff huh could network learn immitate input like Indeed quite impressed well However feels readers post impressed wrong reasons familiar unsmoothed maximum-liklihood character level language models unreasonable effectiveness generating rather convincing natural language outputs follows briefly describe character-level maximum-likelihood langauge models much less magical RNNs LSTMs show produce rather convincing Shakespearean prose also show 30 lines python code take care training model generating output Compared baseline RNNs may seem somehwat less impressive impressed explain Unsmoothed Maximum Likelihood Character Level Language Model name quite long idea simple want model whose job guess next character based previous n letters example seen ello next characer likely either commma space assume end word hello letter w believe middle word mellow Humans quite good course seeing larger history makes things easier see 5 letters instead 4 choice space w would much easier call n number letters need guess based order language model RNNs LSTMs potentially learn infinite-order language model guess next character based state supposedly encode previous history restrict fixed-order language model seeing n letters need guess n 1 th one also given large-ish amount text say Shakespear works use would go solving task Mathematiacally would like learn function P c h c character h n -letters history P c h stands likely see c we've seen h Perhaps simplest approach would count divide k maximum likelihood estimates count number times letter c' appeared h divide total numbers letters appearing h unsmoothed part means see given letter following h give probability zero that's Training Code code training model fname file read characters order history size consult Note pad data leading also learn start 41 collections import def train_char_lm fname order 4 data file fname read lm defaultdict Counter pad order data pad data xrange len data -order history char data order data order lm history char 1 def normalize counter float sum counter values return c cnt c cnt counter iteritems outlm hist normalize chars hist chars lm iteritems return outlm Let's train Andrej's Shakespears's text 42 wget http cs stanford edu people karpathy char-rnn shakespeare_input txt --2015-05-23 02 05 18-- http cs stanford edu people karpathy char-rnn shakespeare_input txt Resolving cs stanford edu cs stanford edu 171 64 64 64 Connecting cs stanford edu cs stanford edu 171 64 64 64 80 connected HTTP request sent awaiting response 200 OK Length 4573338 4 4M text plain Saving shakespeare_input txt shakespeare_input 100 4 36M 935KB 8 8s 2015-05-23 02 05 49 507 KB - shakespeare_input txt saved 4573338 4573338 43 lm train_char_lm shakespeare_input txt order 4 Ok let's queries 44 lm 'ello' 44 ' ' 0 0068143100511073255 ' ' 0 013628620102214651 ' 0 017035775127768313 ' ' 0 027257240204429302 ' ' 0 0068143100511073255 'r' 0 059625212947189095 'u' 0 03747870528109029 'w' 0 817717206132879 'n' 0 0017035775127768314 ' ' 0 005110732538330494 ' ' 0 0068143100511073255 45 lm 'Firs' 45 't' 1 0 46 lm 'rst ' 46 ' 0 0008025682182985554 'A' 0 0056179775280898875 'C' 0 09550561797752809 'B' 0 009630818619582664 'E' 0 0016051364365971107 'D' 0 0032102728731942215 'G' 0 0898876404494382 'F' 0 012038523274478331 'I' 0 009630818619582664 'H' 0 0040128410914927765 'K' 0 008025682182985553 'M' 0 0593900481540931 'L' 0 10674157303370786 'O' 0 018459069020866775 'N' 0 0008025682182985554 'P' 0 014446227929373997 'S' 0 16292134831460675 'R' 0 0008025682182985554 'T' 0 0032102728731942215 'W' 0 033707865168539325 'a' 0 02247191011235955 'c' 0 012841091492776886 'b' 0 024879614767255216 'e' 0 0032102728731942215 'd' 0 015248796147672551 'g' 0 011235955056179775 'f' 0 011235955056179775 'i' 0 016853932584269662 'h' 0 019261637239165328 'k' 0 0040128410914927765 'm' 0 02247191011235955 'l' 0 01043338683788122 'o' 0 030497592295345103 'n' 0 020064205457463884 'q' 0 0016051364365971107 'p' 0 00882825040128411 's' 0 03290529695024077 'r' 0 0072231139646869984 'u' 0 0016051364365971107 't' 0 05377207062600321 'w' 0 024077046548956663 'v' 0 002407704654895666 'y' 0 002407704654895666 ello followed either space punctuation w r u n Firs pretty much deterministic word following ist start pretty much every letter Generating model Generating also simple generate letter take history look last order characteters sample random letter based corresponding distribution 47 random import random def generate_letter lm history order history history -order dist lm history x random c v dist x x - v x 0 return c generate passage k characters seed initial history run letter generation loop updating history turn 48 def generate_text lm order nletters 1000 history order xrange nletters c generate_letter lm history order history history -order c append c return join Generated Shakespeare different order models Let's try generate text based different language-model orders Let's start something silly order 2 35 lm train_char_lm shakespeare_input txt order 2 print generate_text lm 2 Fif thad yourty Fare sid Che al sheace ing Thy thy ove dievest sord wit whand sold iset Commet laund hant KINCESARGANT aboy tur Pome musicell losts blover difte quainge sh usbas ey Chor bacterea mens grou Princeser 'Tis hends ing noth much Lo withiell thicest se nourink gray the's ge fat requand pink menis lat sall favere whathews frevisars FLAVIIIII Whout les MACUS O -- Hie thout nown mis yought Phimne shappy bley sirs --Ha Hart frow mas gen SEY Herfe inese vereat voter'd theave shashall er ist hem thdre mare Lovenat bree shatteed Besat's giverve se FLANY Whis I'll-volover man hitinut thadarthopeatund wing pourisforniners dinguent liked withe brave heiry fore ist Fain Thess kno st witund nothousto yesty art stry son ford bas sood cal love thys th tund great increase order 4 order 4 36 lm train_char_lm shakespeare_input txt order 4 print generate_text lm 4 First devishin son MONTANO 'Tis true full Squellen rest passacre nothink fairs ' done vision actious thy love brings gods THUR comfited flight offend make thy love Brothere oats thes '--why cross shouldestruck one hearina go lives Costag tyrant fill hath trouble KING JOHN Great though gain talk mine Christ right kiss kindness loves Gower stray ever flight wild ebbs fair knowned worship asider thyself-skin ever eat behold speak imposed thy hand Give cours sweet sorrow gone prince see likewis thee hearts kiss come eanly fire prince 'twixt young piece honourish fort 37 lm train_char_lm shakespeare_input txt order 4 print generate_text lm 4 First Office masters part may direct brance would dead Pleaseth profit last awaked Far night I'll courteous Herneath circle SPEED PEDRO preferment DUCHESS QUICKLY Rome other's chamber tears head VIRGILIA O show bowls thouse two hones loved proned speaking shrought upon shall affect onest man Milford's worth boundeserts woman great that's noble upon burth one well surfew-begot thy daughed trib trumpet Sever heave First truth marry Troilus' mouth'd rever hang cond Malvolio EXETER Blists speak morn back would soverdities fatherefore pate rever mirth let thoughts Orsino's heard make methink Oxford name GONZALO reason known Yet care Moor-worm DUCHESS O partles father already quite reasonable reads like English 4 letters history increase 7 order 7 38 lm train_char_lm shakespeare_input txt order 7 print generate_text lm 7 First Citizen One graves Within rich Pisa walls noses snapper-up uncurrent roar'd HOTSPUR Hath call bear admiration young BIRON One word FALSTAFF Ay good Lord sir OCTAVIUS Philarmonus Soothsayer Worthy's thumb-ring green-a box MISTRESS QUICKLY Ay sir CADE would unstate Vexed one royal cheer yon strangers boast God speed CHIRON virtue years prodigious farthing whether deigned already Widow master's pleasure COUNTESS Timon else FALSTAFF Prithee gone CONSTANCE compiled iniquity walked Gentleman Ay Philip Madam Juliet go thou shall forth Silvia Silvia--witness come know heart-string picked must nothing valour put opening Widow Thus met wife' there's Bohemia much bosom stay brav 10 39 lm train_char_lm shakespeare_input txt order 10 print generate_text lm 10 First Citizen Nay speaks service since youth circumstance spoken uncle one Baptista's daughter SEBASTIAN stand till break BIRON Hide thy head VENTIDIUS purposeth Athens whither vow made handle FALSTAFF good knave MALVOLIO Sad lady could forgiven you're welcome Give ear sir doublet hose leave present death Second Gentleman may confess lord enraged forestalled ere come man Drown thyself APEMANTUS Ho ho laugh see beard BOYET Madam great extremes passion discovers PAROLLES white head wit Values sepulchre ' lord business let's away First Keeper Forbear murder wilt thou say lies lies let devil would said sir speed Hath balm heal woes B works pretty well order 4 already get quite reasonable results Increasing order 7 word half history 10 two short words history already gets us quite passable Shakepearan text I'd say par examples Andrej's post simple un-mystical model impressed RNNs Generating English character time -- impressive view RNN needs learn previous n letters rather small n that's However code-generation example impressive context awareness Note posted examples code well indented braces brackets correctly nested even comments start end correctly something achieved simply looking previous n letters examples cherry-picked output generally nice LSTM learn something trivial fun let's see simple language model linux-kernel code 49 wget http cs stanford edu people karpathy char-rnn linux_input txt --2015-05-23 02 07 59-- http cs stanford edu people karpathy char-rnn linux_input txt Resolving cs stanford edu cs stanford edu 171 64 64 64 Connecting cs stanford edu cs stanford edu 171 64 64 64 80 connected HTTP request sent awaiting response 200 OK Length 6206996 5 9M text plain Saving linux_input txt linux_input txt 100 5 92M 1 10MB 9 3s 2015-05-23 02 08 09 654 KB - linux_input txt saved 6206996 6206996 50 lm train_char_lm linux_input txt order 10 print generate_text lm 10 linux kernel time c Please report hardware void irq_mark_irq unsigned long old_entries eval Divide 1000 ns 2 - us 2 conversion values don't overflow seq_puts m ttramp pS void class- contending_point likely t- flags WQ_UNBOUND Update inode information slowpath sleep time abs rel rmtp remaining either due consume state ring buffer size header_size - size bytes chain BUG_ON error cgrp old kdb_continue_catastrophic endif deadlock n return 0 endif info- hdr return diag sharing problem roundup collection better readable 0 rp- maxactive max_t u64 delay 10000LL __hrtimer_get_res - get timer timer hrtimer sched_clock_data my_rdp bool oneshot tick_oneshot_mask GFP_KERNEL free_cpumask_v 51 lm train_char_lm linux_input txt order 15 print generate_text lm 15 linux kernel power snapshot c file licensed terms GNU General Public License detailed information memory ordering guarantees cgroups bigger numbers newer smaller numbers Also csses always appended parent put ref cgroup freed let's make sure every task struct event- ctx- task could possibly point remains valid condition satisfied called perf_event_init_context child ctxn ret pr_err Module len lu truncated n info- len return -ENOMEM env- prog prog grab mutex protect coming going jump_label table static const struct user_regset find_regset const struct cpumask cpu_map int diag 0 kdb_printf go must execute entry cpu please use cpu d execute go n kdb_initial_cpu Used single threaded 52 lm train_char_lm linux_input txt order 20 print generate_text lm 20 linux kernel irq spurious c Copyright C 2004 Nadia Yvette Chambers include linux irq h include linux mutex h include linux capability h include linux suspend h include linux shm h include asm uaccess h include linux interrupt h include kdb_private h Table kdb_breakpoints kdb_bp_t kdb_breakpoints KDB_MAXBPT static void kdb_setsinglestep struct pt_regs regs struct swevent_htable swhash per_cpu swevent_htable cpu mutex_lock swhash- hlist_mutex swhash- online true swhash- hlist_refcount swevent_hlist_release swhash mutex_unlock show_mutex return 0 Unshare file descriptor table shared static int unshare_fs unsigned long unshare_flags struct cred new_cred struct cred cred current_cred retval -EPERM rgid gid_t -1 gid_eq old- gid kegid gid_eq old- sgid kegid gid_eq old- sgid kegid gid_eq old- egid 53 print generate_text lm 20 linux kernel irq chip c Copyright 2003-2004 Red Hat Inc Durham North Carolina Rights Reserved Copyright c 2009 Wind River Systems Inc Copyright C 2008 Thomas Gleixner tglx timesys com code based David Mills's reference nanokernel implementation mostly rewritten keeps idea void __hardpps const struct timespec tp ktime_get_real_ts tp return 0 Walks iomem resources calls func matching resource ranges walks whole tree first level children memory ranges overlap start end also match flags name valid candidates name name resource flags resource flags start start addr end end addr int walk_iomem_res char name unsigned long val static int alloc_snapshot struct trace_array tr struct dentry d_tracer d_tracer tracing_init_dentry void struct trace_array tr wakeup_t 55 print generate_text lm 20 nletters 5000 linux kernel irq resend c Copyright C 2008 Steven Rostedt srostedt redhat com Copyright C 2002 Khalid Aziz khalid_aziz hp com Copyright C 2002 Richard Henderson Copyright C 2001 Rusty Russell 2002 2010 Rusty Russell IBM program distributed hope useful WITHOUT WARRANTY without even implied warranty MERCHANTABILITY FITNESS PARTICULAR PURPOSE See GNU General Public License published Free Software Foundation Inc 51 Franklin St - Fifth Floor Boston MA 02110-1301 USA include linux cpuset h include linux sched deadline h include linux ioport h include linux fs h include linux export h include linux mm h include linux ptrace h include linux profile h include linux smp h include linux proc_fs h include linux interrupt h include kdb_private h Table kdb_breakpoints kdb_bp_t kdb_breakpoints KDB_MAXBPT static void kdb_setsinglestep struct pt_regs regs static int uretprobe_dispatcher struct uprobe_consumer con int ret -ENOENT spin_lock hash_lock tree- goner spin_unlock hash_lock fsnotify_put_mark parent- mark static void cpu_cgroup_css_offline fork cpu_cgroup_fork can_attach cpu_cgroup_can_attach struct cgroup_subsys_state last last pos - prev isn't RCU safe walk - next till end pos NULL css_for_each_child pos css struct freezer parent parent_freezer freezer mutex_lock freezer_mutex rcu_read_lock list_for_each_entry_safe owatch nextw parent- watches wlist audit_compare_dname_path const char dname const char path int parentlen int dlen pathlen const char p dlen strlen dname pathlen strlen path pathlen dlen return 1 parentlen parentlen AUDIT_NAME_FULL parent_len path parentlen pathlen - parentlen dlen return 1 p path parentlen return strncmp p dname dlen static int audit_log_pid_context context context- target_pid context- target_sessionid context- target_auid context- target_uid context- target_sessionid context- target_sid context- target_comm t- comm TASK_COMM_LEN return 0 spin_lock_mutex lock- wait_lock flags schedule raw_spin_lock_init rq- lock rq- nr_running 0 rq- calc_load_active nr_active return delta a1 a0 e 1 - e e 1 - e a0 e 2 1 - e 1 - e n 1 - e a0 e 2 1 - e 1 e e n-1 1 a0 e n 1 - e 1 e e 2 a0 e n 1 - e 1 e e n-1 1 a0 e n 1 - e 1 e a3 a2 e 1 - e a2 a1 e 1 - e a0 e 2 1 - e 1 - e n 1 - e a0 e 2 1 - e 1 e e 2 a0 e n 1 - e n 1 application geometric series '0' '1' n static void l_start struct seq_file file void v loff_t offset unsigned long flags spin_lock_irqsave timekeeper_lock flags global_trace stop_count tracing_is_enabled - Show global_trace disabled Shows global trace enabled uses mirror flag buffer_disabled used fast paths irqsoff tracer may inaccurate due races need know accurate state use tracing_is_on little slower accurate int tracing_is_enabled tracer_enabled 0 unregister_wakeup_function tr graph 0 ret tracing_is_enabled return local_irq_save flags gdbstub_msg_write count local_irq_restore flags -ENOENT try_to_grab_pending work is_dwork flags someone else already canceling wait finish flush_work doesn't work PREEMPT_NONE may get scheduled work's completion canceling task resuming clearing CANCELING - flush_work return false immediately work longer busy try_to_grab_pending struct work_struct work unsigned long data atomic_long_read rsp- expedited_done ULONG_CMP_GE jiffies rdp- rsp- gp_start 2 jiffies return 0 Grace period old enough barrier local_read cpu_buffer_a- committing goto out_dec local_read cpu_buffer- overrun local_sub BUF_PAGE_SIZE cpu_buffer- entries_bytes entries zeroed move tail page still break case RB_PAGE_UPDATE really fixup work struct statically initialized make sure tracked object tracker debug Order 10 pretty much junk order 15 things sort-of make sense jump abruptly order 20 quite nicely -- far keeping good indentation brackets could memory things modeled could quite easily enrich model support also keeping track brackets indentation adding information seen conditioning history requires extra work non-trivial human reasoning make model significantly complex LSTM hand seemed learn that's impressive Back top web site host notebooks renders notebooks available websites Delivered Fastly Rendered Rackspace nbviewer GitHub repository nbviewer version f399a06 IPython version 3 2 0-dev 5e57377 Rendered Thu 28 May 2015 17 17 14 UTC"),
('Why are Eight Bits Enough for Deep Neural Networks?', "Search Pete Warden's blog Ever tried Ever failed matter Try Fail Fail better Main menu Skip content HomeAbout Eight Bits Enough Deep Neural Networks May 23 2015 Pete Warden Uncategorized 2 Comments Picture Retronator Deep learning weird technology evolved decades different track mainstream AI kept alive efforts handful believers started using years ago reminded first time played iPhone felt like d handed something sent back us future alien technology One consequences engineering intuitions often wrong came across im2col memory redundancy seemed crazy based experience image processing turns efficient way tackle problem complex approaches yield better results re ones graphics background would predicted Another key area seems throw lot people much precision need calculations inside neural networks career precision loss fairly easy thing estimate almost never needed 32-bit floats d screwed numerical design fragile algorithm would go wrong pretty soon even 64 bits 16-bit floats good lot graphics operations long weren chained together deeply could use 8-bit values final output display end algorithm weren useful much else turns neural networks different run eight-bit parameters intermediate buffers suffer noticeable loss final results astonishing something re-discovered colleague Vincent Vanhoucke paper ve found covering result deep networks ve seen eyes holds true across every application ve tried ve also convince almost every engineer tell m crazy watch prove running lot tests post attempt short-circuit work see example low-precision approach Jetpac mobile framework though keep things simple keep intermediate calculations float use eight bits compress weights Nervana NEON library also supports fp16 though eight-bit yet long accumulate 32 bits re long dot products heart fully-connected convolution operations take vast majority time need float though keep inputs output eight bit ve even seen evidence drop bit two eight without much loss pooling layers fine eight bits ve generally seen bias addition activation functions trivial relu done higher precision 16 bits seems fine even ve generally taken networks trained full float down-converted afterwards since m focused inference training also done low precision Knowing re aiming lower-precision deployment make life easier even train float since things like place limits ranges activation layers work see fundamental mathematical reason results hold well low precision ve come believe emerges side-effect successful training process trying teach network aim understand patterns useful evidence discard meaningless variations irrelevant details means expect network able produce good results despite lot noise Dropout good example synthetic grit thrown machinery final network function even adverse data networks emerge process robust numerically lot redundancy calculations small differences input samples affect results Compared differences pose position orientation noise images actually comparatively small problem deal layers affected small input changes extent develop tolerance minor variations means differences introduced low-precision calculations well within tolerances network learned deal Intuitively feel like weebles won fall matter much push thanks inherently stable structure heart m engineer ve happy see works practice without worrying much want look gift horse mouth ve laid best guess cause property would love see principled explanation researchers want investigate thoroughly Update related paper Matthieu Courbariaux thanks Scott mean good news anyone trying optimize deep neural networks general CPU side modern SIMD instruction sets often geared towards float eight bit calculations offer massive computational advantage recent x86 ARM chips DRAM access takes lot electrical power though slow reducing bandwidth 75 big help able squeeze values fast low-power SRAM cache registers win GPUs originally designed take eight bit texture values perform calculations higher precisions write back eight bits re perfect fit needs generally wide pipes DRAM gains aren quite straightforward achieve exploited bit work ve learned appreciate DSPs great low-power solutions instruction sets geared towards sort fixed-point operations need Custom vision chips like Movidius Myriad good fits Deep networks robustness means implemented efficiently across wide range hardware Combine flexibility almost-magical effectiveness lot AI tasks eluded us decades see m excited alter world next years Share TwitterFacebookGoogleLike Like Loading Related Post navigation Jetpac deep learning framework Beaglebone Black 2 responses Mike says May 24 2015 12 24 reminds Geoff Hinton talk neurons brain prefer communicate single bits rather real values touches 6 minutes gets back 45 minutes https www youtube com watch v DleXA5ADG78 Reply Nikolai says May 24 2015 3 35 pm sort 8 bit proof could taken notebook Open video recording tool look hope recognize pretty fast explanation proof starts fact notebook camera likely uses 8 bits per BGR channel appears enough task Another consideration 1 value generally significant 1 bit value 8 bit range counts 0 4 fits quite well course math guys put lot critics proofs think coarse assessment work memory requirements re right evaluation test discovered machine needs 3 terabytes RAM mimic neocortex HS 8 bit precision 64 bit computing cuts range 8 Sounds bad Reply Leave Reply Cancel reply Enter comment Fill details click icon log Email required Address never made public Name required Website commenting using WordPress com account Log Change commenting using Twitter account Log Change commenting using Facebook account Log Change commenting using Google account Log Change Cancel Connecting Notify new comments via email Notify new posts via email Follow petewarden TwitterMy TweetsRSS - Posts Recent Posts Eight Bits Enough Deep Neural Networks Jetpac deep learning framework Beaglebone Black Image Recognition Raspberry Pi 2 visit Living Computer Museum Seattle GEMM heart deep learning Recent Comments Nikolai Eight Bits Enough Mike Eight Bits Enough Eight Bits E GEMM heart de Jetpac deep Image Recognition Raspb w3576hyu8i9o nerd culture must die Archives May 2015 April 2015 March 2015 January 2015 December 2014 November 2014 October 2014 September 2014 August 2014 July 2014 June 2014 May 2014 April 2014 March 2014 February 2014 January 2014 December 2013 November 2013 October 2013 September 2013 August 2013 July 2013 June 2013 May 2013 April 2013 March 2013 February 2013 January 2013 November 2012 October 2012 August 2012 July 2012 June 2012 May 2012 April 2012 March 2012 February 2012 January 2012 December 2011 November 2011 October 2011 September 2011 August 2011 July 2011 June 2011 May 2011 April 2011 March 2011 February 2011 January 2011 December 2010 November 2010 October 2010 September 2010 August 2010 July 2010 June 2010 May 2010 April 2010 March 2010 February 2010 January 2010 December 2009 November 2009 October 2009 September 2009 August 2009 July 2009 June 2009 May 2009 April 2009 March 2009 February 2009 January 2009 December 2008 November 2008 October 2008 September 2008 August 2008 July 2008 June 2008 May 2008 April 2008 March 2008 February 2008 January 2008 December 2007 November 2007 October 2007 September 2007 August 2007 July 2007 June 2007 May 2007 April 2007 March 2007 December 2006 November 2006 October 2006 September 2006 August 2006 Pete Warden's blog Footer menu HomeAbout Blog WordPress com Elemin Theme Design Themify Follow Follow Pete Warden's blog Get every new post delivered Inbox Join 1 154 followers Build website WordPress com d bloggers like"),
("I used Andrej Karpathy's char-rnn to compose Irish Folk songs. Here are the results!", "SoundCloudJavaScript disabledYou need enable JavaScript use SoundCloudShow enable char-rnn composes Irish Folk music seaandsailor published 2015 05 22 21 19 18 0000 samples generated char-rnn trained 14k Irish Folk tunes downloaded thesession org Contains tracks Jimmy O'Corlan seaandsailor published 2015 05 22 21 19 18 0000 Paddy Pilcor's seaandsailor published 2015 05 22 21 19 18 0000 Boyse seaandsailor published 2015 05 23 02 11 36 0000 Paddy Mountry Airding Baie seaandsailor published 2015 05 23 02 38 25 0000 Users like char-rnn composes Irish Folk music Users reposted char-rnn composes Irish Folk music License all-rights-reserved current browser isn't compatible SoundCloud Please download one supported browsers Need help Chrome Firefox Safari Internet ExplorerSorry Something went wrongIs network connection unstable browser outdated need help Try mobile site"),
('20 articles on Deep learning shared by Andrew Y NG in 4 weeks. He literally brought back Neural Network from dead!', 'Articles Magazine Linear Influencers Conversations ARTICLES Magazine Layout Linear Layout INFLUENCERS CONVERSATIONS Search topical articles influencers conversations Search topical articles influencers conversations Tweets rightrelevance Right Relevance Inc 2014 FAQ Blog Team Terms Privacy Contact Create RightRelevance account Discover fresh relevant content interests save interesting articles follow influential experts first share soon-to-be viral content much Cancel account Login Sign RightRelevance Continue Twitter Continue Facebook Continue LinkedIn Discover fresh relevant content interests save interesting articles follow influential experts first share soon-to-be viral content much Sign'),
('Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1505 05612 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context cs CV prev next new recent 1505Change browse cs cs CL cs LG References CitationsNASA ADS Bookmark Computer Science Computer Vision Pattern Recognition Title Talking Machine Dataset Methods Multilingual Image Question Answering Authors Haoyuan Gao Junhua Mao Jie Zhou Zhiheng Huang Lei Wang Wei Xu Submitted 21 May 2015 Abstract paper present mQA model able answer questions content image answer sentence phrase single word model contains four components Long-Short Term Memory LSTM extract question representation Convolutional Neural Network CNN extract visual representation LSTM storing linguistic context answer fusing component combine information first three components generate answer construct Freestyle Multilingual Image Question Answering FM-IQA dataset train evaluate mQA model contains 120 000 images 250 000 freestyle Chinese question-answer pairs English translations quality generated answers mQA model dataset evaluated human judges Turing Test Specifically mix answers provided humans model human judges need distinguish model human also provide score e 0 1 2 larger better indicating quality answer propose strategies monitor quality evaluation process experiments show 64 7 cases human judges cannot distinguish model humans average score 1 454 1 918 human Subjects Computer Vision Pattern Recognition cs CV Computation Language cs CL Learning cs LG ACM classes 2 6 2 7 2 10 Cite arXiv 1505 05612 cs CV arXiv 1505 05612v1 cs CV version Submission history Junhua Mao view email v1 Thu 21 May 2015 06 09 36 GMT 1971kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('PCANet: A Simple Deep Learning Baseline for Image Classification? (x-post r/CompressiveSensing )', "Nuit Blanche name Igor Carron homepage Page Views Nuit Blanche since July 2010 Follow IgorCarron Cite Nuit Blanche related pages recent Compressive Sensing article Scientific Reports Attendant Project Page Please join comment Google Community 1502 CompressiveSensing subreddit 811 Facebook page LinkedIn Compressive Sensing group 3293 Advanced Matrix Factorization Group 1017 Reference pages include Big Picture Compressive Sensing Learning Compressive Sensing Advanced Matrix Factorization Jungle Page Highly Technical Reference Pages - Aggregators Technologies Exist CAI Cable Igor's Adventures Matrix Factorization search Reproducible Research page Paris Machine Learning Meetup Archives Meetup com register 2222 members LinkedIn post jobs 721 Google 233 Facebook follow-on discussions Twitter Friday May 22 2015 PCANet Simple Deep Learning Baseline Image Classification - implementation - Email ThisBlogThis Share TwitterShare FacebookShare Pinterest Iteration matrix factorizations way build deep architectures Interesting PCANet Simple Deep Learning Baseline Image Classification Tsung-Han Chan Kui Jia Shenghua Gao Jiwen Lu Zinan Zeng Yi Ma work propose simple deep learning network image classification comprises basic data processing components cascaded principal component analysis PCA binary hashing block-wise histograms proposed architecture PCA employed learn multistage filter banks followed simple binary hashing block histograms indexing pooling architecture thus named PCA network PCANet designed learned extremely easily efficiently comparison better understanding also introduce study two simple variations PCANet namely RandNet LDANet share topology PCANet cascaded filters either selected randomly learned LDA tested basic networks extensively many benchmark visual datasets different tasks LFW face verification MultiPIE Extended Yale B AR FERET datasets face recognition well MNIST hand-written digits recognition Surprisingly tasks seemingly naive PCANet model par state art features either prefixed highly hand-crafted carefully learned DNNs Even surprisingly sets new records many classification tasks Extended Yale B AR FERET datasets MNIST variations Additional experiments public datasets also demonstrate potential PCANet serving simple highly competitive baseline texture classification object recognition implementation PCAnet Tsung-Han's source code page Join CompressiveSensing subreddit Google Community post Liked entry subscribe Nuit Blanche's feed there's came also subscribe Nuit Blanche Email explore Big Picture Compressive Sensing Matrix Factorization Jungle join conversations compressive sensing advanced matrix factorization calibration issues Linkedin labels implementation MF Igor 5 22 2015 10 30 00 comments Post Comment Newer Post Older Post Home Subscribe Post Comments Atom Printfriendly Nuit Blanche Stemming Data Tsunami One Algorithm Time Tweet Igor Search Nuit Blanche Loading Subscribe E-MAIL Nuit Blanche Get new entries directly mailbox enter email address Nuit Blanche Nuit Blanche blog focuses Compressive Sensing Advanced Matrix Factorization Techniques Machine Learning well many engaging ideas techniques needed handle make sense high dimensional data also known Big Data Nuit Blanche french expression translates nighter restless night Contact Cassini MSL Oppy HiRISE SOHO SDO Rosetta-Philae Rosetta Contact igorcarron gmail com Webon LinkedIn Twitter Pages Home Reproducible Research implementations Randomized Numerical Linear Algebra RandNLA Advanced Matrix Factorization Learning Compressed Sensing It's CAI Cable Igor's Adventures Matrix Factorization Machine Learning Meetups Around World Compressed Sensing Pages Focused Interest Pages Datasets Challenges Nuit Blanche Conversations Linking Nuit Blanche blogs CS Meetings Real Time Experiments Highly Technical Reference Pages - Aggregators Recent Nuit Blanche entries Paris Machine Learning Meetup Archives Pinterest Boards Imaging Nature Technologies Exist Wondering Star Computational Photography Subscribe LinkedIn Matrix Factorization Group 1001 members right one Link stats Subscribe Nuit Blanche RSS Feed Posts Atom Posts Comments Atom Comments Subscribe LinkedIn Compressive Sensing group 3145 members right one Link stats Google Badge updated profile LinkedIn reflect activities Nuit Blanche means provide recommendations based experience reading blog Nuit Blanche QR code Search Nuit Blanche LoadingLatest news Compressive Compressed Sensing Arxiv Full Text Search Arxiv Google Compressive Sensing Compressed Sensing 24 hours week month Rice University Compressive Sensing repositoryLatest news Matrix Factorization Arxiv old Arxiv new Google 24 hours week month Readership Statistics another set watching blog feedreaders 740 readers receive every entries mailboxes 600 people come site directly everyday detailed information following blog entries far site seen 3 500 000 pageviews since counter installed 2007 Nuit Blanche Referenced Dead Tree World Big Picture Compressive Sensing mentioned article La Recherche french speaking equivalent competitor Science October 2010 issue page 20-21 Wired Magazine piece Compressed Sensing featuring links blog Big Picture March 1 2010 Emmanuel Candes Terry Tao wrote Nuit Blanche Dec '08 issue IEEE Information Theory Society Newsletter Xiaochuan Pan Emil Sidky Michael Vannier wrote Nuit Blanche commercial CT scanners still employ traditional filtered back-projection image reconstruction Check also acknowledgments Ghost Imaging paper one Like Link Xi'an's Og Toscana 3 - Filed Mountains pictures Running Travel Wines Tagged Chianti farmhouse Italia ruins sunset Tuscany 30 minutes ago Hack Day Eye-Controlled Wheelchair Advances Talented Teenage Hackers - Myrijam Stoetzer friend Paul Foltin 14 15 years old kids Duisburg Germany working eye movement controller wheel chair Th 48 minutes ago Terahertz Technology Abstract-Terahertz response patterned epitaxial graphene - Christian Sorger Sascha Preu1 2 Johannes Schmidt3 Stephan Winnerl3 Yuliy V Bludov4 Nuno M R Peres4 Mikhail Vasilevskiy4 Heiko B Weber1 http 1 hour ago Endeavour Data code regulation - Data code code data distinction software code input data blurry best arbitrary worst distinction 5 hours ago Another Word Cybersecurity Authoritative Reports Resources Topic Need Librarians - Cybersecurity Authoritative Reports Resources Topic Rita Tehan Information Specialist Congressional Research Service summary 18 hours ago Image Sensors World Sony 2015 IR Day - Sony held 2015 Investor Relations Day today Device Segment presentation Tomoyuki Suzuki Executive Deputy President Corporate Executive Offic 21 hours ago free hunch Interactive R Tutorial Machine Learning Titanic Competition - Always wanted compete Kaggle competition sure right skill set DataCamp created free interactive tutorial help 21 hours ago High Noon GMT Oh torn 'twixt love an' tenure Behold molten children Recurrent Neural Networks Generating Even Word God - Please note post taken seriously show artificial system learned enough lot text generate new text 2 days ago What's new Cancellation multilinear Hilbert transform - ve uploaded arXiv paper Cancellation multilinear Hilbert transform submitted Collectanea Mathematica paper uses method 2 days ago G del's Lost Letter P NP John Alicia Nash 1928 1933 2015 - condolences Awesome Stories source John Nash wife Alicia killed taxi accident New Jersey Turnpike Saturday afternoon wer 2 days ago Timothy Lottes OS Project 7 - PS 2 Misc - started back long break Finished PS 2 driver Simplified keyboard interface one 64-bit bit array memory simple p 3 days ago Ergodic Walk AISTATS 2015 talks one day - attended AISTATS day change year unfortunately due teaching missed poster Shuang Song presented work 3 days ago Herve La cuisine cr erait-elle son objet - La chimie cr e son objet la phrase est paradoxale dit qu'elle est du chimiste Marcellin Berthelot mais est-elle vraiment de lui Voir Marcellin 3 days ago ChapterZero quick thought Supernatural tv shows - finished season 9 Supernatural ve got give show credit one demands deus ex machina ending Anything less 3 days ago Mr Vacuum Tube Phased Array Radar Looking Walls - Phased Array Radar Looking Walls http blog array2016 org p 24 5 days ago Walking Randomly MATLAB Vectorisation double-edged sword - Imagine new MATLAB programmer create N x N matrix called j j first attempt solution might 6 days ago slice pizza Morning Madness Ode Mercedes - morning madness drive along skyline Cutting fog thick stew waves dew 70mph hugging memorized curves without 1 week ago Decision Science News Gelman sense dubious Science article - Statistician Andrew Gelman sense something dubious Science article soon published post Gelman sense 1 week ago 0xDE Graham Erd Egyptian fractions - recent paper Ron Graham surveys work Paul Erd Egyptian fractions know Erd s' second paper subject didn't p 1 week ago Geomblog ITA conference really enjoy - Continuing thoughts STOC 2017 reboot went back Boaz's original question would make likely go STOC thought I'd 1 week ago Pillow Lab Blog Fast Kronecker trick Gaussian Process regression expressive kernels - May 11th presented following paper lab meeting Fast Kernel Learning Multidimensional Pattern Extrapolation Andrew Gordon Wilson Elad Gil 2 weeks ago Machine Learning etc ICLR 2015 - ICLR posters caught eye larger image simple implement idea gives impressive results force two groups units un 2 weeks ago Libres pens es d'un math maticien ordinaire adventure Google search - interesting experience Google recently related Electronic Journal Probability EJP Electronic Communications Probability ECP 2 weeks ago tombone's blog Dyson 360 Eye Baidu Deep Learning Embedded Vision Summit Santa Clara - Bringing Computer Vision Consumer Mike Aldred Electronics Lead Dyson Ltd vision research priority decades results 3 weeks ago Secrets Consulting Requirements Hints Variations - volumes Exploring Requirements follow chapter section hints variations topic chapter Many readers tel 3 weeks ago Harvest Imaging Blog Third HARVEST IMAGING FORUM December 2015 - successful forums 2013 2014 third one organized December 2015 Voorburg Hague Netherlands basic inte 3 weeks ago m bandit COLT 2015 accepted papers cool videos - Like last year compiled list COLT 2015 accepted papers together links arxiv version whenever could find one papers 3 weeks ago Information Structuralist Counting bits Vapnik Chervonenkis - Machine learning enabling computers improve performance given task get data express intuition quantitative 4 weeks ago Le Petit Chercheur Illustr Quasi-isometric embeddings vector sets quantized sub-Gaussian projections - Last January honored invited RWTH Aachen University Holger Rauhut Sjoerd Dirksen give talk general topic quantized co 4 weeks ago Machine Learning Theory Randomized experimentation - One good thing machine learning present people actually use back-ends many systems interact daily basis 5 weeks ago La vertu d'un LA virtue - fortunate hive D dom nologie la science du traitement de donn es signal images etc - O l'on propose le n ologisme d dom nologie pour d signer la technique la pratique la science du traitement de signal et de l'analyse d'images au c 1 month ago robots net Robots Podcast Farewell robots net join us Robohub - Since May 2007 colleagues Robots Podcast Robohub working robots net bring latest news views robo 1 month ago Machine Learning Deep Learning Works II Renormalization Group - Deep Learning amazing Deep Learning successful Deep Learning old-school Neural Networks modern hardware w 1 month ago Follow Data Genomics Today Tomorrow presentation - Slideshare link widget presentation gave Genomics Today Tomorrow event Uppsala couple weeks ago March 19 2015 sp 1 month ago Adventures Signal Processing Open Science Open Access Journals Missing - end could value proposition future journals 1 month ago Thoughts Mysterious Universe State Probabilistic Programming - two weeks last July cocooned hotel Portland living breathing probabilistic programming student probabilistic p 1 month ago Epistasis Blog Biomedical Informatics Faculty Positions University Pennsylvania - recently moved research lab Perelman School Medicine University Pennsylvania serve Director Institute Biomed 2 months ago Petros Boufounos Internship Opening Sensor Fusion - new internship opening MERL area sensor fusion posting follows MM880 Sensor fusion MERL looking well qualified 2 months ago G-media Le blog Nouveaut d veloppeurs domotique et openpicus Prise Gigogne et Dolphin View - Dans cet article nous allons voir comment utiliser la prise gigogne avec mesure de comptage depuis le logiciel Dolphin View Mat riel n cessaire une c 2 months ago Neurevolution Neurevolution relaunch - hard believe started blog eight years ago way back grad students long way ve come Patryk Dir 2 months ago Inductio Ex Machina Upcoming Conference Sydney Predictive APIs Apps - big kick-off Barcelona last year 200 attendees second International Conference Predictive APIs Apps held Sydn 2 months ago Ars Mathematica Nine Chapters Semigroup Art - Googling something came across Nine Chapters Semigroup Art leisurely introduction theory semigroups 2 months ago yellow noise takis champs magnetiques palais de tokyo february 2015 - takis champs magnetiques palais de tokyo february 2015 3 months ago polylogblog CPM 2015 - Ely asked remind everyone deadline 26th Annual Symposium Combinatorial Pattern Matching fast approaching 2nd F 4 months ago Mirror Image Kinect depth sensor works stereo triangulation simple ways speed convnet little - quite complex methods making convolutional networks converge faster Natural gradient dual coordinate ascent second order hessian fre 5 months ago natural language processing blog myth strong baseline - probably count fingers number papers I've submitted reviewer hasn't complained baseline way don't mean 6 months ago Building Intelligent Probabilistic Systems Harvard Center Research Computation Society Call Fellows Visiting Scholars - Harvard Center Research Computation Society CRCS solicits applications Postdoctoral Fellows Visiting Scholars Programs 7 months ago Pixel shaker Pics Manipulated Photos Notable Historic Figures Digital Era Images - Manipulating photos happened way Photoshop around series shows afters famous notable figures digital era 7 months ago Victoria Stodden input OSTP RFI reproducibility - Sept 23 2014 US Office Science Technology Policy Whitehouse accepting comments Strategy American Innovation 8 months ago Ga l Varoquaux Hiring engineer mine large brain connectivity databases - Work us leverage leading-edge machine learning neuroimaging Parietal research team work improving way brain images analyz 8 months ago Computers don't see Compiling OpenCV 3 0 alpha CUDA support MacOS X - quick tip people troubles compiling OpenCV 3 0 alpha MacOS X variable called CUDA_TOOLKIT_DIR cmake configura 8 months ago Herr Strathmann - home Shogun NYC - late August invited NYC present Shogun open-source Machine Learning software workshop link organised John Langford Seeing Sh 8 months ago Lousodrome Vie au Japon Le certificat de r sidence j minhy - Le certificat de r sidence j minhy en japonais est un simple document d une page qui certifie votre adresse de r sidence et qui est demand pour 10 months ago trekkinglemon's fresh squeeze Data processing flashy yellow Peyresq 2014 Last day - 2014 edition Peyresq summer school finished coda let us summarize last talk Continue reading 10 months ago Camdp com updates DataOrigami Launch - I'm proud announce latest project dataorigami net still go check 11 months ago Neighborhood Infinity Cofree meets Free - - LANGUAGE RankNTypes MultiParamTypeClasses TypeOperators - Introduction spoke BayHac 2014 free monads asked co 1 year ago Statistical Trader Follow twitter StatTrader - Since working full time managing Data Sciences team Bloomberg Global Data haven't done sort long-form blogging used 1 year ago Ivan Oseledets homepage Introduction cross approximation - written short incomplete introductory page skeleton decomposition using maxvol algorithm update references 1 year ago MAKE Magazine New Project TV-B-Gone Kit - Tired LCD TVs everywhere Want break advertisements re trying eat Want zap screens across street TV-B-Go 1 year ago i2pi com focus - focus 1 year ago brain map statistics connection Linus Pauling fMRI - think Linus Pauling two things come mind work nature chemical bond awarded 1954 Nobel Prize Chem 1 year ago Computational Information Geometry Wonderland New blog address Moving Wordpress - use anymore blog system rather use Wordpress supports many goodies like latex Please update yo 1 year ago Martin Tall Gaze Interaction Introducing Eye Tribe Tracker - It's great pride today introduce Eye Tribe Tracker It's worlds smallest remote tracker first use USB3 0 one 100 1 year ago De Rerum Natura Functional programming - least interesting concept programming languages purity Java prime example Everything object much prefer Python wa 1 year ago Doyung Pig Hive Pig Hive - Hadoop ecosystem Data processing Pig Hive Pig Hive pig hive data 2 years ago BlackbordRMT Dyson Brownian motion thirty stationnary Brownian motions Ornstein Uhlenbeck - image Image Hosted ImageShack us Dyson Brownian motion thirty stationnary Brownian motions Ornstein Uhlenbeck processes constr 2 years ago Freakonometrics Mod lisation et pr vision cas d' cole - Quelques lignes de code que l'on reprendra au prochain cours avec une transformation en log et une tendance lin aire Consid rons la recherche du mot c 2 years ago Brain Windows WordPress accepts Bitcoin - WordPress blog platform accepts bitcoin hosting costs via BitPay WordPress hosts Brain Windows many world biggest best blogs 2 years ago Science Sands Science Sands moved - Dear readers migrated blog along things new site http davidketcheson info New posts longer appear blo 2 years ago Journey Randomness SMC sampler - monte carlo sequential monte carlo SMC sampler Jasra Doucet Del Moral 2 years ago Hao's TechBlog Sampling Rate Pixels compare sampling rate camera single-pixel camera - beginning I'd like make clear two terms Nyquist frequency Nyquist rate may take thing even text 2 years ago bpchesney org Nested Linear Programs Find Agreement among Sensors - Suppose 3 sensors B C set readings goes column matrix b matrix constructed bA bB bC repr 2 years ago Stupid Matlab Hacks busy recently hacks mean time go play - busy recently hacks mean time go play http www mathworks com matlabcentral fileexchange 37104-kappatau mak 2 years ago Becoming Astronaut Orbiters going - month NASA commenced delivery four Space Shuttle orbiters final destinations extensive decommissioning process 3 years ago FUTUREPICTURE Note comments - six months ago hit serious rash spam 20 000 comments posted span two weeks Unfortunately ti 3 years ago CyberGi Os ciborgues da Campus Party - Essa semana fui na quinta edi o da Campus Party e ontem dia 9 tive o prazer de conhecer dois ciborgues o Rob Spencer que fez o document rio Eyeborg q 3 years ago Collective Research Interaction Sound Signal Processing Sonification Handbook - yet heard Sonification Handbook edited Thomas Hermann Andy Hunt John G Neuhoff published even better freely avai 3 years ago Espace Vide PCA Compressive Measurements Video - I've little bit fun visualizations today outcomes pretty nice potentially artistic thought I'd share 3 years ago inspiration etc Session 4 5th Graders - Doodling 2 - Session 4 - Doodling Approximately one half hours Starting one element Adding proximity expanding drawing Tool 3 years ago Marcio Marim Welcome new website - time work second version didn publish happy release new website share creations interests whi 3 years ago Cognitive Radio Blog G Vazquez-Vilar PhD Thesis Interference Management Cognitive Radio - image Thesis Cognitive radio dissertation examination took place couple weeks ago Happily passed say one unexpect 3 years ago OISblog Liquid Crystal Eyeglasses - Pixel Optics introduced Empower eyeglasses use liquid crystal lenses actively adjust power 4 years ago Big Numbers Going commission - m going focus completely school blog going hiatus months ll start ve passed hurd 4 years ago Another Dimension three musketeers - Given vectors three quantities interesting indeed fact concept inner product vector space revolves largely around thos 4 years ago Electrons holes Indefinite hiatus - blog put indefinite hiatus 4 years ago Arthur Charpentier Blog transfert - mentioned past weeks blog transfered please update links bookmarks redirected shortly http freako 4 years ago Chaotic Pearls Indonesian Contoh Program Phase Unwrapping - Ide dari phase-unwraping PU progresif ini muncul di suatu sore hari ketika saya sedang berjalan-jalan di sekitar kampus sekitar tahun 2002-an Saya ber 4 years ago YALL1 ALgorithms L1 Announcements - June 4 2010 Toeplitz circulant sampling demos released June 4 2010 YALL1 version 1 0 released open-source Download link YALL1 n 4 years ago Hashimoto Laboratory's Blog Personal Mobility Next Level - Improving concept self-balancing unicycle Honda introduced brand new U3-X new personal mobility platform regular large whee 5 years ago Lianlin Li's Compressive Sensing blog Chinese Igor's Blog today - Today Post updated Igor's blog following CS Matrix Completion via Thresholding Dick Gordon's Op-Ed Lianlin L 5 years ago Guan Gui's blog expoit channel structure - 6 years ago Three-Toed Sloth - ML Counterexamples Pt 2 - Regression Post-PCA camdp com blogs - Willow Garage Blog Willow Garage - Latest News - KinectHacks net - Little Knowledge - Blog Robotics - Show 25 Show Another Blog List Haldane's Sieve SWEEPFINDER2 Increased sensitivity robustness flexibility - SWEEPFINDER2 Increased sensitivity robustness flexibility Michael DeGiorgio Christian D Huber Melissa J Hubisz Ines Hellmann Rasmus Nielsen Su 4 hours ago Information Processing John Nash dead 86 - original title post won Nobel Memorial Prize see sad news bottom Beautiful Mind Nash went see von Neuman 4 days ago Scientific Clearing House Selection week - Violinist Hilary Hahn hails Baltimore pianist Valentina Lisitsa play first movement American composer Charles Ives Fourth Sonata 6 days ago leon bottou org news news graph_transducer_networks_explained - Graph Transducer Networks explained scavenging old emails couple weeks ago found copy early technical report describes 2 weeks ago tombone's blog Dyson 360 Eye Baidu Deep Learning Embedded Vision Summit Santa Clara - Bringing Computer Vision Consumer Mike Aldred Electronics Lead Dyson Ltd vision research priority decades results 3 weeks ago Property Testing Review News April 2015 - April busy time property testing 9 papers posted online month let jump right Testing properties graphs Approximately Co 3 weeks ago Relax Conquer Courant Institute Mathematical Sciences - Finally longer job market excited announce join Courant Institute Mathematical Sciences Assistant Profess 1 month ago Moody Rd Competing data science contest without reading data - Machine learning competitions become extremely popular format solving prediction classification problems sorts famous ex 2 months ago AK Tech Blog Neustar SIAM SODA 2015 - Author Note Hello readers m Sonya Berg first post Neustar research blog data scientist Neustar Research foc 4 months ago Mostly linguistically computational Adventure collaborative filtering information retrieval matrix factorization stuff Count-Min-Log Strange effect - followed previous steps mentionning previous post odd behaviour Count-Min-Log MAX sampling w 5 months ago Deep Learning Recent Reddit AMA Deep Learning - Recently Geoffrey Hinton Yann Lecun Yoshua Bengio reddit AMA subscribers r MachineLearning asked questions AMA contains 6 months ago jim learning choose mentor - http www cell com neuron fulltext S0896-6273 13 00907-0 https www cs princeton edu courses archive spring15 cos598D http www cs princeton edu cours 1 year ago Blog Interphase Transport Phenomena Laboratory Texas M University Fun Boiling - 1 year ago Math Good another WordPress site Linear Algebra good - Linear algebra branch mathematics concerned solving linear systems natural think linear algebra solving unkno 1 year ago BayesRules STAT 330 November 29 2012 - finished discussion model selection averaging went missing data models cases fact data observed h 2 years ago Comments Lecture Schedule Embryo Physics Course - Blog List FlowingData Compare curve reality income versus college attendance - image Draw line grow poorer families less likely go college grow richer families likely 1 hour ago SynBioFromLeukipposInstitute Scoop Three developments help synthetic biology live promise Genetic Literacy Project - Three developments help synthetic biology live promise Dominic Basulto May 28 2015 Washington Post See Scoop via 2 hours ago Retraction Watch chocolate-diet sting study retracted coverage doesn surprise news watchdog - Yesterday John Bohannon described i09 com successfully created health news conducted flawed trial health benefits chocolate 4 hours ago Statistical Modeling Causal Inference Social Science Cracked com Huffington Post Wall Street Journal New York Times - David Christopher Bell goes trouble link Palko explain Every Map Popular _________ State Bullshit long 4 hours ago olimex MOD-LCD3310 OSHW monochrome LCD 84 48 pixels board UEXT connector - MOD-LCD3310 Open Source Hardware board released Apache 2 0 Licensee low cost 84 48 pixels LCD connect development bo 6 hours ago Science-Based Medicine Florida strikes Brian Clement - Brian Clement charlatan Unfortunately doesn seem problem State Florida made two turned three attempts g 12 hours ago Sage Open Source Mathematics Software Guiding principles SageMath Inc - February year 2015 founded Delaware C Corporation called SageMath Inc first stab guiding principles compan 21 hours ago Quomodocumque evil impulse good - learned teaching Rabbi Rebecca Ben-Gideon last week turning mind Rabbi Nahman said Rabbi Samuel name Behold 1 day ago Machine Vision 4 Users re backlighting cylindrical parts - see backlighting used time machine vision training classes trade shows typically gauging locating shapes Look closely though 1 day ago Skulls Stars comics moment inspires Suicide Squad - Update Forgot say thank Dad mailing complete Suicide Squad collection made whole post possible suspect peop 1 day ago InnoCentive Challenges Bioabsorbable Elastomeric Film - Seeker looking bio-absorbable elastomeric film specific properties could existing product one adapted 1 day ago What's new Cancellation multilinear Hilbert transform - ve uploaded arXiv paper Cancellation multilinear Hilbert transform submitted Collectanea Mathematica paper uses method 2 days ago Ben Krasnow physics floating screwdrivers - explain jet air float common screwdriver Plans make fluid turbulence disc http makezine com projects rheoscopic-coffee-tab 2 days ago Short Fat Matrices Three Paper Announcements - ve pretty busy lately writing researching visitors announcements serve quick summary ve 1 Tables 1 week ago 2Physics Current Fluctuations - left right Pierre Pfe er Fabian Hartmann Sven H ing Martin Kamp Lukas Worschech Authors Pierre Pfe er1 Fabian Hartmann1 Sven H ing1 2 1 week ago Large Scale Machine Learning Animals Open Data Science Conference - May 30 Boston - 1 week ago Zhilin's Scientific Journey Yann LeCun's Comments Extreme Learning Machine ELM - Yann LeCun https www facebook com yann lecun posts 10152872571572143 Facebook commented ELM quoted What's great Ext 2 weeks ago Gowers's Weblog Nick Clegg Liberal Democrat - life found Liberal Democrat policies Liberal-SDP Alliance policies Liberal policies n 4 weeks ago JeremyBlum com Shapeoko2 CNC Mill Build Log Review - new Inventables Shapeoko2 CNC mill carving away Watch timelapse build-log thorough review do-it-yourself CNC milling machine Continue 4 weeks ago F Pedregosa IPython Jupyter notebook gallery - Draft - TL DR created gallery IPython Jupyter notebooks Check - image Notebook gallery couple months ago put online website d 5 weeks ago Richard Baraniuk Probabilistic Theory Deep Learning - Patel Nguyen R G Baraniuk Probabilistic Theory Deep Learning arXiv preprint arxiv org abs 1504 00641 2 April 2014 grand challeng 1 month ago Welcome Sparse Land Discreteness Sparsity - Discrete signals may sparse whereas sparse signals may discrete Let us consider following signal x 1 1 -1 1 -1 -1 1 1 2 months ago Various Consequences Reliability Growth Enhancing Defense System Reliability - report pdf National academies reliability growth interesting There's lot good stuff design reliability physics fai 2 months ago Inductio Ex Machina Upcoming Conference Sydney Predictive APIs Apps - big kick-off Barcelona last year 200 attendees second International Conference Predictive APIs Apps held Sydn 2 months ago Highly Scalable Blog Data Mining Problems Retail - Retail one important business domains data science data mining applications prolific data numerous optimization p 2 months ago regularize Farewell Ernie Esser - Today learned Ernie Esser passed away March 8th 2015 age 34 Ernie PostDoc UBC Vancouver PhD UCLA worked applied math 2 months ago Thingiverse Blog MakerBot PrintShop 1 4 3D Print iPad 3G 4G - Last week introduced exciting new feature MakerBot 3D Ecosystem ability start 3D print remotely using smartphone MakerBot P 3 months ago much little time Post-doc Molecular Informatics Opening NCATS - post-doc opening Informatics group NCATS work computational aspects high throughput combination screening topics includ 3 months ago next big thing syndrome Books read 2014 - disclaimer book cover images post Amazon Affiliate links click buy book receive cents form 5 months ago Pursuits Null Space Blog done moved - first hesitant handling latex Rolf Mathcination pointed excellent tool latex-to-wordpress wil 5 months ago Proof Pudding M571 Fall 2014 Lecture 5 - 1 Agenda QR Factorization Gram-Schmidt classical modified Householder QR reflectors ended discussion projectors several impo 6 months ago Tianyi Zhou's Research Blog Learn Low-rank Sparse Structures via Randomized Alternating Projections List Submodular Optimization Streaming Data Update - Coresets k-Segmentation Streaming Data NIPS 2014 Streaming Submodular Optimization Massive Data Summarization Fly KDD 2014 8 months ago Wondering Star Lunar Detection Ultra-High-Energy Cosmic Rays Neutrinos - Spotted ArXiv Physics blog using SKA array Moon collector would certainly qualify sensors size pl 8 months ago Dan's Blog make paper spherical panorama - image image Photos usually show rectangular fragment scene image taken Typical panoramic images display landscap 8 months ago Seth's blog Videos Seth Roberts Memorial Talks - Video recordings Ancestral Health Society public talks August 10 2014 honoring Seth life work posted http bit ly 1v33kbM Many 9 months ago Thoughts Artificial Intelligence - blog moved new post metaphor mathematics new blog 9 months ago Lupi Software thoughts Monitorama 2014 PDX - Monitorama fantastic conference came mixed feelings Great work done open source software however based con 1 year ago Andrej Karpathy Blog Interview Data Science Weekly Neural Nets ConvNetJS - Quick post thought mention ve given interview two months ago ConvNetJS background perspectives neural 1 year ago Ivan Oseledets homepage Introduction cross approximation - written short incomplete introductory page skeleton decomposition using maxvol algorithm update references 1 year ago Artificial Intelligence Blog John Dobson 1915 2014 - September 2008 met John Harrisburg slept overnight friend Zoungy place enjoyed beautiful drive Cherry Springs th 1 year ago Hey What's BIG idea 3D Printing Tangible Idea - heywhatsthebigidea net B J Rao write another article 3D printing internet already offers abundance information subject Moun 1 year ago Normal Deviate END - addition best comedy TV show ever Seinfeld great source wisdom one episode Jerry counsels George hit high 1 year ago Math Good another WordPress site Linear Algebra good - Linear algebra branch mathematics concerned solving linear systems natural think linear algebra solving unkno 1 year ago tcs math - mathematics theoretical computer science Kadison-Singer Quantum Mechanics - accounts Kadison-Singer problem mention arose considerations foundations quantum mechanics Specifically question abo 1 year ago programming machine learning blog BibTeX-powered publications list Pelican pelican-bibtex - Hook Wouldn like manage academic publications list easily within context static website Without resorting external services 2 years ago Mathblogging org -- Blog Mathematical Instruments Gianluigi Filippelli - post part series Mathematical Instruments introduce math bloggers listed site Today Gianluigi Filipp 2 years ago openPicus blog Better BBQ Flyport Cosm - knew Austria famous BBQ remember summer 2006 great barbeque austrian friends Klagenfurt fantastic 8 2 years ago Dirac Sea Survey Non Parametric Bayesian marginal applications - Survey Non Parametric Bayesian marginal applications mystery Zoubin one favorite researchers papers usually 2 years ago Biophotonics Review Biology Games Biomedical Research - Games become important part cultures bringing lots entertainment creativity societies Beyond traditional understanding o 2 years ago Clouds Fukushima 9 Months later - regards airborne radiation ground measurement reading NaI detector KEK Tsukuba showing steady decline past 3 years ago Gigapixel News Journal ipConfigure Presents First Gigapixel Wide Area Surveillance Platform - ipConfigure privately owned software research development company announces world first multi-Gigapixel surveillance platform designed 3 years ago Gustavo Tinkers Source Code GUI - Hosted https github com goretkin soundcard-radar 3 years ago Passive Vision Passive Vision - Welcome plan use blog ideas research teaching moment m looking forward teaching Computer Vision class 60 5 years ago Dick Gordon's blog - n 45 years ago Show 25 Show Search Pages Link Loading Previous Entries 2015 207 May 2015 37 Compressive Sensing work Three-dimensional coo Robust Rotation Synchronization via Low-rank Low-Rank Matrix Recovery Row-and-Column Affin Self-Dictionary Sparse Regression Hyperspectra Randomized Robust Subspace Recovery High Dimen Book Dictionary Learning Visual Computing Compressed Nonnegative Matrix Factorization Fas Saturday Morning Videos Slides Videos IC PCANet Simple Deep Learning Baseline Image Four million page views million million Great Convergence FlowNet Learning Optical F CSjob Post-Doc Structured Low-Rank Approximati Low-rank Modeling Applications Image Solving Random Quadratic Systems Equations N Identifiability Blind Deconvolution Subspa Tensor time Adaptive Higher-order Spectral Estima Blue Skies Foundational principles large scal Tensor sparsification via bound spectral Saturday Morning Videos Reruns Hamming's time Important Things Commodi Newton Sketch Linear-time Optimization Algorith Self-Expressive Decompositions Matrix Approxim Tonight Paris Machine Learning Meetup 9 Season Factorization Machines implementation Theano Frequent Directions Simple Deterministic Mat fastFM Library Factorization Machines - imp Structured Block Basis Factorization Scalable Tensorial Kernel kernel-based framework tens Kernel Spectral Clustering applications - impl Sparse LSSVM Reductions Large-Scale Data Random Bits Regression Strong General Predictor Phase retrieval via Kaczmarz methods - implementat Great Convergence Deep Learning Compressive Phase Transition Joint-Sparse Recovery Mul Saturday Morning Videos IMA Workshop Convexity Saturday Morning Videos IMA AP Workshop Inform Nuit Blanche Review April 2015 Apr 2015 45 Mar 2015 44 Feb 2015 31 Jan 2015 50 2014 536 Dec 2014 52 Nov 2014 43 Oct 2014 38 Sep 2014 41 Aug 2014 48 Jul 2014 52 Jun 2014 43 May 2014 56 Apr 2014 47 Mar 2014 44 Feb 2014 35 Jan 2014 37 2013 454 Dec 2013 43 Nov 2013 38 Oct 2013 38 Sep 2013 33 Aug 2013 36 Jul 2013 43 Jun 2013 29 May 2013 38 Apr 2013 40 Mar 2013 29 Feb 2013 47 Jan 2013 40 2012 488 Dec 2012 44 Nov 2012 39 Oct 2012 46 Sep 2012 28 Aug 2012 52 Jul 2012 20 Jun 2012 38 May 2012 60 Apr 2012 41 Mar 2012 50 Feb 2012 29 Jan 2012 41 2011 465 Dec 2011 47 Nov 2011 49 Oct 2011 47 Sep 2011 36 Aug 2011 24 Jul 2011 25 Jun 2011 47 May 2011 50 Apr 2011 56 Mar 2011 39 Feb 2011 17 Jan 2011 28 2010 358 Dec 2010 47 Nov 2010 35 Oct 2010 32 Sep 2010 28 Aug 2010 30 Jul 2010 33 Jun 2010 26 May 2010 27 Apr 2010 28 Mar 2010 28 Feb 2010 19 Jan 2010 25 2009 274 Dec 2009 22 Nov 2009 23 Oct 2009 24 Sep 2009 25 Aug 2009 25 Jul 2009 23 Jun 2009 20 May 2009 16 Apr 2009 25 Mar 2009 27 Feb 2009 21 Jan 2009 23 2008 302 Dec 2008 20 Nov 2008 23 Oct 2008 28 Sep 2008 28 Aug 2008 22 Jul 2008 17 Jun 2008 28 May 2008 22 Apr 2008 31 Mar 2008 32 Feb 2008 25 Jan 2008 26 2007 179 Dec 2007 23 Nov 2007 21 Oct 2007 14 Sep 2007 18 Aug 2007 13 Jul 2007 13 Jun 2007 9 May 2007 11 Apr 2007 9 Mar 2007 22 Feb 2007 19 Jan 2007 7 2006 30 Dec 2006 4 Nov 2006 4 Oct 2006 2 Sep 2006 2 Aug 2006 2 Jul 2006 2 Jun 2006 1 Mar 2006 2 Feb 2006 2 Jan 2006 9 2005 88 Dec 2005 1 Nov 2005 6 Oct 2005 3 Sep 2005 12 Aug 2005 1 Jul 2005 7 Jun 2005 4 May 2005 12 Apr 2005 7 Mar 2005 12 Feb 2005 8 Jan 2005 15 2004 214 Dec 2004 18 Nov 2004 8 Oct 2004 20 Sep 2004 44 Aug 2004 29 Jul 2004 13 Jun 2004 13 May 2004 18 Apr 2004 10 Mar 2004 22 Feb 2004 8 Jan 2004 11 2003 12 Dec 2003 10 Nov 2003 2 Books Wish List Start-ups like InView Technology Corporation See Inside World First Compressive Sensing Camera - Read InView white paper InView210 scientific SWIR camera InView210-CSCameraWhitePaper-Feb2015 See inside world first high 2 months ago Centice Centice Drug Analysis Systems Sold Major Federal Agency Aid Prescription Pill Abuse Operations - Nationwide program allows agents identify 3 800 prescription pills illicit drugs RESEARCH TRIANGLE PARK N C October 28 2014 Centice Co 7 months ago Press GraphLab twitterscroll - post twitterscroll appeared first GraphLab Inc 1 year ago Zoomin' - Aqueti TV Interview Gigapixel Images - Scott McCain interviewed July 10 2013 discuss gigapixel imaging WLOS ABC's Asheville affiliate 1 year ago Metamarkets Blog - wise io Machine Learning Service Big Data Analytics - Translate blog Focused Interest Compressed Sensing Compressive Sampling Compressive Sensing Mapping blog entries Compressed Sensing Cognition - Machine Learning Space Search Rescue Compressive Sensing Technology Watch Compressive Sensing Big Picture Compressive Sensing Hardware Compressed Sensing Videos Compressive Sensing Calendar Compressive Sensing Jobs Local Compressed Sensing Codes CS LinkedIn Group Recent links Blog CS Compressive Sensing 2 0 Community Compressive Sensing 2 0 blogs webpages Saturday Morning Cartoons Sherpa Romeo Publisher copyright policies self-archiving Categories Subjects Interest CS 2163 compressive sensing 1627 compressed sensing 1615 compressive sampling 1589 MF 521 implementation 359 Applied Math 209 ML 209 MatrixFactorization 194 space 152 AMP 113 calibration 105 CSHardware 103 CSjobs 90 CS Community 72 SaturdayMorningVideos 71 CSCommunity 66 BlindDeconvolution 64 QuantCS 62 phaseretrieval 62 RandNLA 60 hyperspectral 60 nonlinearCS 59 nuclear 59 technology 58 SundayMorningInsight 57 CSVideo 50 python 50 tensor 49 cognition 47 Meetups 46 Algorithm 45 grouptesting 45 meetup 43 1bit 42 publishing 41 synbio 41 graphlab 38 RandomFeatures 37 darpa 37 CSmeeting 36 AI 33 NuitBlancheReview 33 search rescue 33 weather modeling 33 remote sensing 32 Csstats 31 wow 30 business 29 bayes 28 data fusion 28 machine learning 28 MLParis 27 jim gray 24 neuroscience 23 autonomous 22 dimensionality reduction 22 mapmaker 20 thesis 20 AlexSmola 19 Kaczmarz 19 ParisMachineLearning 19 geocam 19 space debris 19 space situational awareness 19 medical 18 phaserecovery 18 ChristophStuder 17 ImagingWithNature 17 SAHD 17 maps 17 mishap 17 sleep 17 CSCalendar 16 monday morning algorithm 16 transport 16 CAI 15 energy 15 nanopore 15 phasediagrams 15 hasp 13 superresolution 13 ADMM 12 PatrickGill 12 Technologies Exist 12 causality 12 darpa urban challenge 12 sudoku 12 CSDiscussion 11 ICLR2015 11 TRL 11 qa 11 thermal engineering 11 GPU 10 fft 10 sie 10 videos 10 Computational Neuroscience 9 MultiplicativeNoise 9 RandomForest 9 StarTracker 9 aroundtheblogs 9 france 9 ELM 8 GenomeTV 8 HammingsTime 8 PredictingTheFuture 8 Good 8 collaborative task manager 8 exploration 8 random projections 8 situational awareness 8 sparsity 8 wavelet 8 GreatThoughtsFriday 7 TheGreatConvergence 7 collaborative work 7 innovation 7 mems 7 random lens imaging 7 CSCartoons 6 CitingNuitBlanche 6 CompressibleWGN 6 accidentalcamera 6 complexity vizualisation 6 maxent 6 randomization 6 startups 6 streaming 6 thedip 6 RMM 5 UQ 5 book 5 coded aperture 5 muscle 5 tex-mems 5 BP 4 British Petroleum 4 CfP 4 CompressiveSensingWhatIsItGoodFor 4 DataDrivenSensorDesign 4 HusHambug 4 Comment 4 ReproducibleResearch 4 google maps 4 hypergeocam 4 internet traffic 4 jionc 4 microsystems 4 scaling 4 technologie 4 Deepwater Horizon 3 disruptive technology 3 financement de la recherche 3 google 3 julia 3 radiation detection 3 recherche 3 sketching 3 Columbia 2 DC law 2 LowRank 2 MLZurich 2 MMDS 2 ManifoldSignalProcessing 2 NO-C-WE 2 TheNuitBlancheChronicles 2 UAV 2 aggregators 2 anecdote 2 challenge 2 diet 2 genomics 2 kinect hacks 2 microcontroller 2 notebynotecooking 2 sensor network 2 AWGN 1 BaltiAndBioinformatics 1 Blogger 1 CS MF 1 CT 1 CompanyX 1 JOTRSOI 1 Leonardo 1 QIS 1 RMT 1 SKA 1 SaturdayMorningCartoons 1 SensorsTheSizeOfAPlanet 1 YouAreNotPayingAttention 1 advice 1 aha 1 biographies 1 control 1 crowdfunding 1 csoped 1 dataset 1 donoho-tao 1 extremesampling 1 herschel 1 hushamburg 1 iLab 1 inverse problems 1 iot 1 jacques devooght 1 lfe 1 lua 1 memory 1 mindmaps 1 nanopre 1 octopus 1 oped 1 privacy 1 reference 1 request 1 rr 1 seinfeld 1 solver 1 theano 1 wonderingstar 1 youkeepusingthatword 1 sites interest Blogroll Natural Language Blog Hal Daume III Polylog Blog Andrew McGregor Eric Tramel's Espace Vide Blog Compressed Sensing Lianlin Li's Compressive Sensing blog Chinese Space Engineering Research Center Space Engineering Blog Frank Nielsen's Information Geometry blog David Brady's Blog Le Petit Chercheur Illustre Chaotic Pearls Indonesian De Rerum Natura Michele Guieu's blog Ergodic Walk Laurent Duval's site Laurent Duval's blog Thesilog Diffusion des savoirs - Ecole Normale Superieure What's New Terry Tao Statistical Modeling Causal Inference Social Science Andrew Gelman Aleks Jakulin Masanao Yajima Machine Learning etc Yaroslav Bulatov slice Pizza Muthu Mutukrishnan Geomblog Piotr Indyk Suresh Machine Learning Theory John Langford Lemonodor John Wiseman Yet another Machine Learning blog Pierre Dangauthier Make Magazine blog Theses en ligne Neurevolution blog Pedro Davalos website Damaris' blog Olivier's blog Julie's blog Michele Guieu's site Location visitors Nuit Blanche Dilbert counters Powered Blogger"),
('2 papers in Deep Sensorimotor Learning tobepresented 2015-05-27 and 28 @ICRA', 'Home Team Papers Videos Media Technical Talk Papers Sergey Levine Chelsea Finn Trevor Darrell Pieter Abbeel End-to-End Training Deep Visuomotor Policies presented IEEE International Conference Robotics Automation ICRA 2015 Late Breaking Results Session Thursday May 28 2015 Seattle WA Preprint PDF arXiv Sergey Levine Nolan Wagener Pieter Abbeel Learning Contact-Rich Manipulation Skills Guided Policy Search presented IEEE International Conference Robotics Automation ICRA 2015 Wednesday May 27 2015 Seattle WA Nominated Best Robotic Manipulation Paper Best Conference Paper Preprint PDF website'),
('Weight Uncertainty in Neural Networks', ''),
('GPU accelerated Deep Belief Network in Python (based on cudamat), Jupyter notebook example of training & generating with MNIST data.', 'Skip content Sign Sign repository Explore Features Enterprise Blog Watch 3 Star 14 Fork 2 rodrigosetti dbn-cuda Code Issues Pull requests Pulse Graphs HTTPS clone URL Subversion checkout URL clone HTTPS Subversion Download ZIP GPU accelerated Deep Belief Network 4 commits 1 branch 0 releases Fetching contributors Python 100 0 Python branch master Switch branches tags Branches Tags master Nothing show Nothing show dbn-cuda add requirements section latest commit a0a2be4824 rodrigosetti authored May 22 2015 Permalink Failed load latest commit information input Initial commit May 22 2015 README md add requirements section May 22 2015 example ipynb Rename README ipynb example ipynb May 22 2015 rbm_cuda py Initial commit May 22 2015 README md dbn-cuda GPU accelerated Deep Belief Network Python Wikipedia machine learning deep belief network DBN generative graphical model alternatively type deep neural network composed multiple layers latent variables hidden units connections layers units within layer trained set examples unsupervised way DBN learn probabilistically reconstruct inputs layers act feature detectors inputs learning step DBN trained supervised way perform classification See example Requirements numpy cudamat PyPrind Status API Training Shop Blog 2015 GitHub Inc Terms Privacy Security Contact Something went wrong request Please try'),
("David Silver's UCL Reinforcement Learning Lecture Videos", 'Skip navigation UploadSign inSearch Karolina Kruszynska Videos Playlists Channels Discussion Watch Best YouTube Popular YouTube Music Sports Gaming Movies TV Shows News Live 360 Video Spotlight Browse channels Sign see channels recommendations Sign Watch QueueTV QueueWatch QueueTV Queue Remove allDisconnect Loading Watch Queue TV Queue __count__ __total__ SubscribeSubscribedUnsubscribe35 Subscription preferences Loading Loading Working Karolina Kruszynska Home Videos Playlists Channels Discussion Play RL Course Karolina Kruszynska 9 videos 626 views Last updated May 19 2015 Play Share Loading Save Sign YouTube Sign Play nextPlay RL Course - Lecture 1 Introduction Reinforcement Learning Karolina Kruszynska 1 28 13 Play nextPlay RL Course - Lecture 2 Markov Decision Process Karolina Kruszynska 1 42 05 Play nextPlay RL Course - Lecture 3 Planning Dynamic Programming Karolina Kruszynska 1 39 09 Play nextPlay RL Course - Lecture 4 Model-Free Prediction Karolina Kruszynska 1 37 02 Play nextPlay RL Course - Lecture 5 Model Free Control Karolina Kruszynska 1 36 31 Play nextPlay RL Course - Lecture 6 Value Function Approximation Karolina Kruszynska 1 36 45 Play nextPlay RL Course - Lecture 7 Policy Gradient Methods Karolina Kruszynska 1 31 40 Play nextPlay RL Course - Lecture 8 Integrating Learning Planning Karolina Kruszynska 1 40 13 Play nextPlay RL Course - Lecture 9 Exploration Exploitation Karolina Kruszynska 1 39 18 Language English Country India Safety History Help Loading Loading Loading Press Copyright Creators Advertise Developers YouTube Terms Privacy Policy Safety Send feedback Try something new Loading Working Sign add Watch Later Add Loading playlists'),
('Open Robotics/AI Workshop/Lab/Meetup happening tomorrow in San Francisco', 'Facebook - Messenger Facebook Ad Choices Facebook 2015'),
('On Sensible Baselines, Diminishing Returns, and the Unreasonable Effectiveness of Ensembles', 'Request storySign Sign upAhmed El Deeb May 223 minNext storyNext storyThe author chose make story unlisted means people link see sure want share Yes show sharing optionsOn Sensible Baselines Diminishing Returns Unreasonable Effectiveness Ensembles Share Twitter Share FacebookOn Sensible Baselines Diminishing Returns Unreasonable Effectiveness EnsemblesIn previous post outlined basic components sensible data science process expand important guidelines modeling part process Sensible Baseline deciding metrics important one sensible baseline compare sophisticated algorithms baseline naive still sensible solution problem e g taking movie average rating giving predicted rating users might also useful ideal baseline based expert labels crowdsourcing Without baselines hard measure progress could spend loads time effort implementing sophisticated algorithms without actually knowing whether good whether gains achieved worth effort Simplest First rule thumb simplest algorithms implemented first take considerably less time implement usually easier debug interpret less likely weird hidden bugs also usually get 80 90 performance sophisticated algorithms maybe problems need Also simple algorithms serve additional baselines sophisticated algorithms typically easier adapt different environments e g Map-Reduce implementation easier parallelize Finally simple algorithms less susceptible problem ofoverfitting start training awesome Deep Neural Network building linear model first making sure need Diminishing Returns vs Order-Statistics guideline countermands one special cases Specifically state competition dealing model performance defines competitive strength product Typically one two components product e g search could ranking spell-correction email spam detection classification etc cases rule diminishing returns still true irrelevant since usually last 2 5 performance differentiates competitors goal cases best get ahead competition economic system multiple machine learning components start simple algorithms differentiating features Ensembling Different Algorithms One lessons learned 1 000 000 Netflix Prize competition ML algorithms perfect combining multiple algorithms usually better individual algorithms Ensembling big topic lots literature key idea combine multiple strong sufficiently different algorithms sensible way generate one final prediction combined prediction likely correct closer truth predictions individual algorithms combination method could simple averaging predictions case numeric target voting scheme elaborate methods combining different models Concluding RemarksThese means components successful data science process found widely applicable different scenarios could save considerable amount effort confusion followed RecommendRecommendedBookmarkBookmarkedShareMoreBlockedUnblockFollowFollowingAhmed El DeebPublished May 22 rights reserved author'),
('[1404.3606] PCANet: A Simple Deep Learning Baseline for Image Classification?', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1404 3606 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context cs CV prev next new recent 1404Change browse cs cs LG cs NE References CitationsNASA ADS DBLP - CS Bibliography listing bibtex Tsung-Han Chan Kui Jia Shenghua Gao Jiwen Lu Zinan Zeng Bookmark Computer Science Computer Vision Pattern Recognition Title PCANet Simple Deep Learning Baseline Image Classification Authors Tsung-Han Chan Kui Jia Shenghua Gao Jiwen Lu Zinan Zeng Yi Ma Submitted 14 Apr 2014 v1 last revised 28 Aug 2014 version v2 Abstract work propose simple deep learning network image classification comprises basic data processing components cascaded principal component analysis PCA binary hashing block-wise histograms proposed architecture PCA employed learn multistage filter banks followed simple binary hashing block histograms indexing pooling architecture thus named PCA network PCANet designed learned extremely easily efficiently comparison better understanding also introduce study two simple variations PCANet namely RandNet LDANet share topology PCANet cascaded filters either selected randomly learned LDA tested basic networks extensively many benchmark visual datasets different tasks LFW face verification MultiPIE Extended Yale B AR FERET datasets face recognition well MNIST hand-written digits recognition Surprisingly tasks seemingly naive PCANet model par state art features either prefixed highly hand-crafted carefully learned DNNs Even surprisingly sets new records many classification tasks Extended Yale B AR FERET datasets MNIST variations Additional experiments public datasets also demonstrate potential PCANet serving simple highly competitive baseline texture classification object recognition Subjects Computer Vision Pattern Recognition cs CV Learning cs LG Neural Evolutionary Computing cs NE Cite arXiv 1404 3606 cs CV arXiv 1404 3606v2 cs CV version Submission history Tsung-Han Chan view email v1 Mon 14 Apr 2014 15 02 17 GMT 1033kb D v2 Thu 28 Aug 2014 15 20 44 GMT 842kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('Partially Derivative Episode 23: Political Science Rulez', "podcast data around us Episodes Resources Twitter RSS SoundCloud Stitcher iTunes Navigation Episodes Resources Twitter RSS SoundCloud Stitcher iTunes Episode 23 Political Science Rulez May 22 2015 Jonathon Morgan week Chris overcompensates love political science Jonathon continues unimpressive Links stories discussed week's episode Republicans hate generic women they've never heard biggest political science study last year complete fraud Markov 2 Chainz Dear Madison Avenue Set Data Scientists Free Never miss episode Subscribe Partially Derivative iTunes Stitcher Soundcloud podcasting app May 22 2015 Jonathon Morgan data data science Newer Older Powered Squarespace"),
('The Unreasonable Effectiveness of Recurrent Neural Networks', "Andrej Karpathy blog Hacker's guide Neural Networks Unreasonable Effectiveness Recurrent Neural Networks May 21 2015 There's something magical Recurrent Neural Networks RNNs still remember trained first recurrent network Image Captioning Within dozen minutes training first baby model rather arbitrarily-chosen hyperparameters started generate nice looking descriptions images edge making sense Sometimes ratio simple model quality results get blows past expectations one times made result shocking time common wisdom RNNs supposed difficult train experience I've fact reached opposite conclusion Fast forward year I'm training RNNs time I've witnessed power robustness many times yet magical outputs still find ways amusing post sharing magic We'll train RNNs generate text character character ponder question even possible way together post also releasing code Github allows train character-level language models based multi-layer LSTMs give large chunk text learn generate text like one character time also use reproduce experiments we're getting ahead RNNs anyway Recurrent Neural Networks Sequences Depending background might wondering makes Recurrent Networks special glaring limitation Vanilla Neural Networks also Convolutional Networks API constrained accept fixed-sized vector input e g image produce fixed-sized vector output e g probabilities different classes models perform mapping using fixed amount computational steps e g number layers model core reason recurrent nets exciting allow us operate sequences vectors Sequences input output general case examples may make concrete rectangle vector arrows represent functions e g matrix multiply Input vectors red output vectors blue green vectors hold RNN's state soon left right 1 Vanilla mode processing without RNN fixed-sized input fixed-sized output e g image classification 2 Sequence output e g image captioning takes image outputs sentence words 3 Sequence input e g sentiment analysis given sentence classified expressing positive negative sentiment 4 Sequence input sequence output e g Machine Translation RNN reads sentence English outputs sentence French 5 Synced sequence input output e g video classification wish label frame video Notice every case pre-specified constraints lengths sequences recurrent transformation green fixed applied many times like might expect sequence regime operation much powerful compared fixed networks doomed get-go fixed number computational steps hence also much appealing us aspire build intelligent systems Moreover we'll see bit RNNs combine input vector state vector fixed learned function produce new state vector programming terms interpreted running fixed program certain inputs internal variables Viewed way RNNs essentially describe programs fact known RNNs Turing-Complete sense simulate arbitrary programs proper weights similar universal approximation theorems neural nets shouldn't read much fact forget said anything training vanilla neural nets optimization functions training recurrent nets optimization programs Sequential processing absence sequences might thinking sequences inputs outputs could relatively rare important point realize even inputs outputs fixed vectors still possible use powerful formalism process sequential manner instance figure shows results two nice papers DeepMind left algorithm learns recurrent network policy steers attention around image particular learns read house numbers left right Ba et al right recurrent network generates images digits learning sequentially add color canvas Gregor et al Left RNN learns read house numbers Right RNN learns paint house numbers takeaway even data form sequences still formulate train powerful models learn process sequentially You're learning stateful programs process fixed-sized data RNN computation things work core RNNs deceptively simple API accept input vector x give output vector y However crucially output vector's contents influenced input fed also entire history inputs you've fed past Written class RNN's API consists single step function rnn RNN y rnn step x x input vector y RNN's output vector RNN class internal state gets update every time step called simplest case state consists single hidden vector h implementation step function Vanilla RNN class RNN def step self x update hidden state self h np tanh np dot self W_hh self h np dot self W_xh x compute output vector y np dot self W_hy self h return y specifies forward pass vanilla RNN RNN's parameters three matrices W_hh W_xh W_hy hidden state self h initialized zero vector np tanh function implements non-linearity squashes activations range -1 1 Notice briefly works two terms inside tanh one based previous hidden state one based current input numpy np dot matrix multiplication two intermediates interact addition get squashed tanh new state vector you're comfortable math notation also write hidden state update h_ 1 tanh W_ hh h_t W_ xh x_t tanh applied elementwise initialize matrices RNN random numbers bulk work training goes finding matrices give rise desirable behavior measured loss function expresses preference kinds outputs y you'd like see response input sequences x Going deep RNNs neural networks everything works monotonically better done right put deep learning hat start stacking models like pancakes instance form 2-layer recurrent network follows y1 rnn1 step x y rnn2 step y1 words two separate RNNs One RNN receiving input vectors second RNN receiving output first RNN input Except neither RNNs know care - it's vectors coming going gradients flowing module backpropagation Getting fancy I'd like briefly mention practice us use slightly different formulation presented called Long Short-Term Memory LSTM network LSTM particular type recurrent network works slightly better practice owing powerful update equation appealing backpropagation dynamics won't go details everything I've said RNNs stays exactly except mathematical form computing update line self h gets little complicated use terms RNN LSTM interchangeably experiments post use LSTM Character-Level Language Models Okay idea RNNs super exciting work We'll ground fun application We'll train RNN character-level language models we'll give RNN huge chunk text ask model probability distribution next character sequence given sequence previous characters allow us generate new text one character time working example suppose vocabulary four possible letters helo wanted train RNN training sequence hello training sequence fact source 4 separate training examples 1 probability e likely given context h 2 l likely context 3 l also likely given context hel finally 4 o likely given context hell Concretely encode character vector using 1-of-k encoding e zero except single one index character vocabulary feed RNN one time step function observe sequence 4-dimensional output vectors one dimension per character interpret confidence RNN currently assigns character coming next sequence Here's diagram example RNN 4-dimensional input output layers hidden layer 3 units neurons diagram shows activations forward pass RNN fed characters hell input output layer contains confidences RNN assigns next character vocabulary h e l o want green numbers high red numbers low example see first time step RNN saw character h assigned confidence 1 0 next letter h 2 2 letter e -3 0 l 4 1 o Since training data next correct character e would want increase confidence green decrease confidence letters red common approach use cross-entropy loss function corresponds placing Softmax classifier every output vector correct class index next character sequence loss backpropagated RNN's weights updated correct next letters higher scores faced similar inputs Technical add RNN trained mini-batch Stochastic Gradient Descent like use RMSProp per-parameter adaptive learning rate stablilize updates Notice also first time character l input target l second time target o RNN therefore cannot rely input alone must use recurrent connection keep track context achieve task test time feed character RNN get distribution characters likely come next sample distribution feed right back get next letter Repeat process you're sampling text Lets train RNN different datasets see happens Fun RNNs 5 example character models trained code I'm releasing Github input case single file text we're training RNN predict next character sequence Paul Graham generator Lets first try small dataset English sanity check favorite fun dataset concatenation Paul Graham's essays basic idea there's lot wisdom essays unfortunately Paul Graham relatively slow generator Wouldn't great could sample startup wisdom demand That's RNN comes Concatenating pg essays last 5 years get approximately 1MB text file 1 million characters considered small dataset way Technical Lets train 2-layer LSTM 512 hidden nodes approx 3 5 million parameters dropout 0 5 layer We'll train batches 100 examples truncated backpropagation time length 100 characters settings one batch TITAN Z GPU takes 0 46 seconds cut half 50 character BPTT negligible cost performance Without ado lets see sample RNN surprised investors weren't going raise money I'm company time interesting quickly don't get programmers There's super-angel round fundraising different physical investment become people reduced startup way argument acquirer could see you're also founders part users' affords alternation idea 2 Don't work first member see way kids seem advance bad successful startup act big company Okay clearly unfortunately going replace Paul Graham anytime soon remember RNN learn English completely scratch small dataset including put commas apostrophes spaces also like learns support arguments e g 2 Sometimes says something offers glimmer insight company meeting think investors Here's link 50K character sample you'd like see Temperature also play temperature Softmax sampling Decreasing temperature 1 lower number e g 0 5 makes RNN confident also conservative samples Conversely higher temperatures give diversity cost mistakes e g spelling mistakes etc particular setting temperature near zero give likely thing Paul Graham might say thing startup thing startup thing startup looks like we've reached infinite loop startups Shakespeare looks like learn spell English words structure style data examine downloaded works Shakespeare concatenated single 4 4MB file afford train larger network case lets try 3-layer RNN 512 hidden nodes layer train network hours obtain samples PANDARUS Alas think shall come approached day little srain would attain'd never fed chain subjects death sleep Second Senator away miseries produced upon soul Breaking strongly buried perish earth thoughts many states DUKE VINCENTIO Well wit care side Second Lord would ruled chamber fair nues begun fact conveyed Whose noble souls I'll heart wars Clown Come sir make behold worship VIOLA I'll drink Remember RNN knows characters particular samples speaker's names contents Sometimes also get relatively extented monologue passages VIOLA Salisbury must find flesh thought aps man fire show reining raven wars grace hand reproach within fair hand Caesar goodly father's world heaven presence fleets spare hours cut thy council great Murdered thy master's ready power give thee much hell service noble bondman Would show wine KING LEAR O feeble sight courtesy law sight several breath wear gods heads hands wonder'd deeds drop upon lordship's head opinion Shall honour barely recognize samples actual Shakespeare like Shakespeare might appreciate 100 000 character sample course also generate infinite amount samples different temperatures provided code Wikipedia saw LSTM learn spell words copy general syntactic structures Lets increase difficulty train structured markdown particular lets take Hutter Prize 100MB dataset raw Wikipedia train LSTM Following Graves et al used first 96MB training rest validation ran models overnight Alex reports performance 1 67 Bits Per Character BPC 7-layer LSTM 700 nodes best model ended 3-layer LSTM 700 nodes achieving 1 57 BPC 7 epochs training I'm exactly sure accounts better performance Regardless sample Wikipedia articles fun excerpts First basic markdown output Naturalism decision majority Arab countries' capitalide grounded Irish language John Clair Imperial Japanese Revolt associated Guangzham's sovereignty generals powerful ruler Portugal Protestant Immineners could said directly Cantonese Communication followed ceremony set inspired prison training emperor travelled back Antioch Perth October 25 21 note Kingdom Costa Rica unsuccessful fashioned Thrales Cynth's Dajoard known western Scotland near Italy conquest India conflict Copyright succession independence slop Syrian influence famous German movement based popular servicious non-doctrinal sexual power post Many governments recognize military housing Civil Liberalization Infantry Resolution 265 National Party Hungary sympathetic Punjab Resolution PJS http www humah yahoo com guardian cfm 7754800786d17551963s89 htm Official economics Adjoint Nazism Montgomery swear advance resources Socialism's rule starting signing major tripad aid exile case wondering yahoo url doesn't actually exist model hallucinated Also note model learns open close parenthesis correctly There's also quite lot structured markdown model learns example sometimes creates headings lists etc cite journal id Cerling Nonforest Department format Newlymeslated none ''www e-complete'' '''See also''' List ethical consent processing See also Iender dome ED Anti-autism Religion Religion French Writings Maria Revelation Mount Agamul External links http www biblegateway nih gov entrepre Website World Festival labour India-county defeats Ripper California Road External links http www romanology com Constitution Netherlands Hispanic Competition Bilabial Commonwealth Industry Republican Constitution Extent Netherlands Sometimes model snaps mode generating random valid XML page title Antichrist title id 865 id revision id 15900676 id timestamp 2002-08-03T18 14 12Z timestamp contributor username Paris username id 23 id contributor minor comment Automated conversion comment text xml space preserve REDIRECT Christianity text revision page model completely makes timestamp id Also note closes correct tags appropriately correct nested order 100 000 characters sampled wikipedia you're interested see Algebraic Geometry Latex results suggest model actually quite good learning complex syntactic structures Impressed results labmate Justin Johnson decided push even structured territories got hold book algebraic stacks geometry downloaded raw Latex source file 16MB file trained multilayer LSTM Amazingly resulting sampled Latex almost compiles step fix issues manually get plausible looking math it's quite astonishing Sampled fake algebraic geometry Here's actual pdf Here's another sample hallucinated algebraic geometry Nice try diagram right see sometimes model tries generate latex diagrams clearly hasn't really figured also like part chooses skip proof Proof omitted top left course keep mind latex relatively difficult structured syntactic format haven't even fully mastered instance raw sample model unedited begin proof may assume mathcal abelian sheaf mathcal C item Given morphism Delta mathcal F mathcal injective let mathfrak q abelian sheaf X Let mathcal F fibered complex Let mathcal F category begin enumerate item hyperref setain-construction-phantom Lemma label lemma-characterize-quasi-finite Let mathcal F abelian quasi-coherent sheaf mathcal C Let mathcal F coherent mathcal O _X -module mathcal F abelian catenary mathcal C item following equivalent begin enumerate item mathcal F mathcal O _X -module end lemma sample relatively decent model illustrates common mistakes example model opens begin proof environment ends end lemma example problem we'd fix manually likely due fact dependency long-term time model done proof forgotten whether proof lemma Similarly opens begin enumerate forgets close observed became less common larger better models nonetheless kinds mistakes come Linux Source Code wanted push structured data limit final challenge decided use code particular took source header files found Linux repo Github concatenated single giant file 474MB C code originally going train kernel 16MB trained several as-large-as-fits-on-my-GPU 3-layer LSTMs period days models 10 million parameters still lower end RNN models results superfun Increment size file new incorrect UI_FILTER group information size generatively static int indicate_policy void int error fd MARN_EPT kernel blank coeld userspace ss- segment mem_total unblock_graph_and_set_blocked else ret 1 goto bail segaddr in_SB addr selector seg 16 setup_works true 0 blocks seq buf bpf bd- bd next search fd current blocked rw- name Getjbbregs bprm_self_clearl iv- version regs- new blocks BPF_STATS info- historidac PFMR_CLOBATHINC_SECONDS 12 return segtable code looks really quite great overall course don't think compiles scroll generate code feels much like giant C code base Notice RNN peppers code comments random also good making syntactic errors example uses strings properly pointer notation etc also opens closes brackets correctly learns indent code well common error can't keep track variable names often uses undefined variables e g rw declares variables never uses e g int error returns non-existing variables Lets see examples Here's another snippet shows wider array operations RNN learns error set need anything right BSD static void action_new_function struct s_stat_info wb unsigned long flags int lel_idx_bit e- edd sys unsigned long FIRST_COMPAT buf 0 0xFFFFFFFF bit 4 min inc slist- bytes printk KERN_WARNING Memory allocated 02x 02x original MLL instead n min min multi_run - s- len max num_data_in frame_pos sz first_seg div_u64_w val inb_p spin_unlock disk- queue_lock mutex_unlock s- sock- mutex mutex_unlock func- mutex return disassemble info- pending_bh static void num_serial_settings struct tty_struct tty tty tty disable_single_st_p dev pci_disable_spool port return 0 static void do_command struct seq_file m void v int column 32 cmd 2 0x80 state cmd int int_state in_8 ch- ch_flags Cmd 2 1 else seq 1 0 16 k 1 1 pipe in_use UMXTHREAD_UNCCA count 0x00000000fffffff8 0x000000f 8 count 0 sub pid ppc_md kexec_handle 0x20000000 pipe_set_bytes 0 Free user pages pointer place camera dash subsystem_info of_changes PAGE_SIZE rek_controls offset idx soffset want deliberately put device control_check_polarity context val 0 0 COUNTER seq_puts policy Notice second function model compares tty tty vacuously true hand least variable tty exists scope time last function notice code return anything happens correct since function signature void However first two functions also declared void return values form common mistake due long-term interactions Sometimes model decides it's time sample new file usually amusing part model first recites GNU license character character samples includes generates macros dives code Copyright c 2006-2010 Intel Mobile Communications rights reserved program free software redistribute modify terms GNU General Public License version 2 published Free Software Foundation program distributed hope useful WITHOUT WARRANTY without even implied warranty MERCHANTABILITY FITNESS PARTICULAR PURPOSE See GNU General Public License details received copy GNU General Public License along program write Free Software Foundation Inc 675 Mass Ave Cambridge MA 02139 USA include linux kexec h include linux errno h include linux io h include linux platform_device h include linux multi h include linux ckevent h include asm io h include asm prom h include asm e820 h include asm system_info h include asm setew h include asm pgproto h define REG_PG vesa_slot_addr_pack define PFM_NOCOMP AFSR 0 load define STACK_DDR type func define SWAP_ALLOCATE nr e define emulate_sigs arch_get_unaligned_child define access_rw TST asm volatile movd esp 0 3 r 0 __type DO_READ static void stat_PC_SEC __read_mostly offsetof struct seq_argsqueue pC 1 static void os_prefix unsigned long sys ifdef CONFIG_PREEMPT PUT_PARAM_RAID 2 sel get_state_state set_pid_sum unsigned long state current_state_str unsigned long -1- lr_full low many fun parts cover- could probably write entire blog post part I'll cut short 1MB sampled Linux code viewing pleasure Understanding what's going saw results end training impressive work Lets run two quick experiments briefly peek hood evolution samples training First it's fun look sampled text evolves model trains example trained LSTM Leo Tolstoy's War Peace generated samples every 100 iterations training iteration 100 model samples random jumbles tyntd-iafhatawiaoihrdemot lytdws e tfti astai f ogoh eoase rrranbyne 'nhthnee e plia tklrgd o idoe ns smtt h ne etie h hregtrs nigtike aoaenns lng However notice least starting get idea words separated spaces Except sometimes inserts two spaces also doesn't know comma amost always followed space 300 iterations see model starts get idea quotes periods Tmont thithey fomesscerliund Keushey Thom sheulke anmerenith ol sivh lalterthend Bleipile shuwy fil aseterlome coaniogennc Phe lism thond hon MeiDimorotion ther thize words also separated spaces model starts get idea periods end sentence iteration 500 counter stutn co des stanted one ofler concossions gearang reay Jotrets fre colt otf paitt thin wall das stimn model learned spell shortest common words etc iteration 700 we're starting see English-like text emerge Aftair fall unsuch hall Prince Velzonski's hearly behs arwage fiving beloge pavu say falling misfort Gogition overelical ofter iteration 1200 we're seeing use quotations question exclamation marks Longer words learned well Kite vouch repeated door would done quarts feeling son people last start get properly spelled words quotations names iteration 2000 day replied Natasha wishing fact princess Princess Mary easier fed oftened Pierre aking soul came packs drove father-in-law women picture emerges model first discovers general word-space structure rapidly starts learn words First starting short words eventually longer ones Topics themes span multiple words general longer-term dependencies start emerge much later Visualizing predictions neuron firings RNN Another fun visualization look predicted distributions characters visualizations feed Wikipedia RNN model character data validation set shown along blue green rows every character visualize red top 5 guesses model assigns next character guesses colored probability dark red judged likely white likely example notice stretches characters model extremely confident next letter e g model confident characters http www sequence input character sequence blue green colored based firing randomly chosen neuron hidden representation RNN Think green excited blue excited familiar details LSTMs values -1 1 hidden state vector gated tanh'd LSTM cell state Intuitively visualizing firing rate neuron brain RNN reads input sequence Different neurons might looking different patterns we'll look 4 different ones found thought interesting interpretable many also aren't neuron highlighted image seems get excited URLs turns outside URLs LSTM likely using neuron remember inside URL highlighted neuron gets excited RNN inside markdown environment turns outside Interestingly neuron can't turn right sees character must wait second activate task counting whether model seen one two likely done different neuron see neuron varies seemingly linearly across environment words activation giving RNN time-aligned coordinate system across scope RNN use information make different characters less likely depending early late scope perhaps another neuron local behavior relatively silent sharply turns right first w www sequence RNN might using neuron count far www sequence know whether emit another w start URL course lot conclusions slightly hand-wavy hidden state RNN huge high-dimensional largely distributed representation Source Code hope I've convinced training character-level language models fun exercise train models using char-rnn code released Github MIT license takes one large text file trains character-level model sample Also helps GPU otherwise training CPU factor 10x slower case end training data getting fun results let know Brief digression code written Torch 7 recently become favorite deep learning framework I've started working Torch LUA last months hasn't easy spent good amount time digging raw Torch code Github asking questions gitter get things done get hang things offers lot flexibility speed I've also worked Caffe Theano past believe Torch perfect gets levels abstraction philosophy right better others view desirable features effective framework CPU GPU transparent Tensor library lot functionality slicing array matrix operations etc entirely separate code base scripting language ideally Python operates Tensors implements Deep Learning stuff forward backward computation graphs etc possible easily share pretrained models Caffe well others don't crucially compilation step least currently done Theano trend Deep Learning towards larger complex networks time-unrolled complex graphs critical compile long time development time greatly suffers Second compiling one gives interpretability ability log debug effectively Reading end post also wanted position RNNs wider context provide sketch current research directions RNNs recently generated significant amount buzz excitement field Deep Learning Similar Convolutional Networks around decades full potential recently started get widely recognized large part due growing computational resources Here's brief sketch recent developments definitely complete list lot work draws research back 1990s see related work sections domain NLP Speech RNNs transcribe speech text perform machine translation generate handwritten text course used powerful language models Sutskever et al Graves Mikolov et al level characters words Currently seems word-level models work better character-level models surely temporary thing Computer Vision RNNs also quickly becoming pervasive Computer Vision example we're seeing RNNs frame-level video classification image captioning also including work many others video captioning recently visual question answering personal favorite RNNs Computer Vision paper Recurrent Models Visual Attention due high-level direction sequential processing images glances low-level modeling REINFORCE learning rule special case policy gradient methods Reinforcement Learning allows one train models perform non-differentiable computation taking glances around image case I'm confident type hybrid model consists blend CNN raw perception coupled RNN glance policy top become pervasive perception especially complex tasks go beyond classifying objects plain view Inductive Reasoning Memories Attention Another extremely exciting direction research oriented towards addressing limitations vanilla recurrent networks One problem RNNs inductive memorize sequences extremely well don't necessarily always show convincing signs generalizing correct way I'll provide pointers bit make concrete second issue unnecessarily couple representation size amount computation per step instance double size hidden state vector you'd quadruple amount FLOPS step due matrix multiplication Ideally we'd like maintain huge representation memory e g containing Wikipedia many intermediate state variables maintaining ability keep computation per time step fixed first convincing example moving towards directions developed DeepMind's Neural Turing Machines paper paper sketched path towards models perform read write operations large external memory arrays smaller set memory registers think working memory computation happens Crucially NTM paper also featured interesting memory addressing mechanisms implemented soft fully-differentiable attention model concept soft attention turned powerful modeling features also featured Neural Machine Translation Jointly Learning Align Translate Machine Translation Memory Networks toy Question Answering fact I'd go far say concept attention interesting recent architectural innovation neural networks don't want dive many details soft attention scheme memory addressing convenient keeps model fully-differentiable unfortunately one sacrifices efficiency everything attended attended softly motivated multiple authors swap soft attention models hard attention one samples particular chunk memory attend e g read write action memory cell instead reading writing cells degree model significantly philosophically appealing scalable efficient unfortunately also non-differentiable calls use techniques Reinforcement Learning literature e g REINFORCE people perfectly used concept non-differentiable interactions much ongoing work hard attention models explored example Inferring Algorithmic Patterns Stack-Augmented Recurrent Nets Reinforcement Learning Neural Turing Machines Show Attend Tell People you'd like read RNNs recommend theses Alex Graves Ilya Sutskever Tomas Mikolov REINFORCE generally Reinforcement Learning policy gradient methods REINFORCE special case David Silver's class one Pieter Abbeel's classes Code you'd like play training RNNs hear good things keras passage Theano code released post Torch gist raw numpy code wrote ago implements efficient batched LSTM forward backward pass also look numpy-based NeuralTalk uses RNN LSTM caption images maybe Caffe implementation Jeff Donahue Conclusion We've learned RNNs work become big deal we've trained RNN character-level language model several fun datasets we've seen RNNs going confidently expect large amount innovation space RNNs believe become pervasive critical component intelligent systems Lastly add meta post trained RNN source file blog post Unfortunately 46K characters haven't written enough data properly feed RNN returned sample generated low temperature get typical sample I've RNN works computed program RNN computed RNN code Yes post RNN well works clearly works See next time EDIT extra links Discussions HN discussion Reddit discussion r machinelearning Reddit discussion r programming Replies Yoav Goldberg compared RNN results n-gram maximum likelihood counting baseline Jo o Felipe trained char-rnn irish folk music sampled music Bob Sturm also trained char-rnn music ABC notation RNN Bible bot Maximilien Please enable JavaScript view comments powered Disqus comments powered Disqus Andrej Karpathy blog karpathy karpathy Musings Computer Scientist"),
('10 Python Machine Learning Projects on GitHub', "Search Sign Sign HomeTop LinksData Science BookEditorial GuidelinesUser AgreementAnalyticsBig DataHadoopData PlumbingDataVizJobsWebinarsDigestPrevious DigestsHot JobsSearchContact Subscribe Dr Granville's Weekly Digest Blog PostsMy BlogAdd 10 Python Machine Learning Projects GitHub Posted Pansop May 21 2015 8 00pm View Blog list top Python Machine learning projects GitHub continuously updated list open source learning projects available Pansop scikit-learn scikit-learn Python module machine learning built top SciPy features various classification regression clustering algorithms including support vector machines logistic regression naive Bayes random forests gradient boosting k-means DBSCAN designed interoperate Python numerical scientific libraries NumPy SciPy Official source code repo NuPIC Numenta Platform Intelligent Computing NuPIC machine intelligence platform implements HTM learning algorithms HTM detailed computational theory neocortex core HTM time-based continuous learning algorithms store recall spatial temporal patterns NuPIC suited variety problems particularly anomaly detection prediction streaming data sources Pattern Pattern web mining module Python tools Data Mining Natural Language Processing Network Analysis Machine Learning supports vector space model clustering classification using KNN SVM Perceptron Pylearn2 Pylearn2 library designed make machine learning research easy library based Theano Ramp Ramp python library rapid prototyping machine learning solutions It's light-weight pandas-based machine learning framework pluggable existing python machine learning statistics tools scikit-learn rpy2 etc Ramp provides simple declarative syntax exploring features algorithms transformations quickly efficiently MILK Milk machine learning toolkit Python focus supervised classification several classifiers available SVMs k-NN random forests decision trees also performs feature selection classifiers combined many ways form different classification systems unsupervised learning milk supports k-means clustering affinity propagation skdata Skdata library data sets machine learning statistics module provides standardized Python access toy problems well popular computer vision natural language processing data sets mlxtend It's library consisting useful tools extensions day-to-day data science tasks machine-learning-samples collection sample applications built using Amazon Machine Learning REP REP environment conducting data-driven research consistent reproducible way unified classifiers wrapper variety implementations like TMVA Sklearn XGBoost uBoost train classifiers parallely cluster support interactive plots DSC Resources Career Training Books Cheat Sheet Apprenticeship Certification Salary Surveys Jobs Knowledge Research Competitions Webinars Book Members Search DSC Buzz Business News Announcements Events RSS Feeds Misc Top Links Code Snippets External Resources Best Blogs Subscribe Bloggers Additional Reading Data Scientist Reveals Growth Hacking Techniques 10 Modern Statistical Concepts Discovered Data Scientists Top data science keywords DSC 4 easy steps becoming data scientist 13 New Trends Big Data Data Science 22 tips better data science Data Science Compared 16 Analytic Disciplines detect spurious correlations find real ones 17 short tutorials data scientists read practice 10 types data scientists 66 job interview questions data scientists High versus low-level data science Follow us Twitter DataScienceCtrl AnalyticBridge Views 6148 Tags machine-learning python Like 5 members like Share Tweet Previous Post Comment need member Data Science Central add comments Join Data Science Central Comment Marzena Bihun 10 hours ago university exposed NLTK platform Natural Language Processing course convinced us toolkit best NLP Never heard Pattern project would curious whether anyone used NLTK Pattern comparable certain tasks one superior Thanks RSS Welcome toData Science Central Sign Upor Sign sign Follow Us DataScienceCtrl RSS Feeds Top Content Edit 1 Become Data Scientist Free 2 Data Science Wars R versus Python 3 NoSQL Fundamentally Changed Machine Learning 4 Big Data Uncovering Secrets Universe CERN 5 popular data science keywords DSC 6 Farming Big Data Amazing Story John Deere 7 Defines Big Data Scenario 8 Great Github list public data sets 9 10 Python Machine Learning Projects GitHub 10 Data Scientist Shares Growth Hacking Secrets RSS View Resources Videos DSC Webinar Series 7 Reasons combine SPSS Statistics R Added Tim Matteson 0 Comments 0 Likes DSC Webinar Series Beautiful Science Data Visualization Added Tim Matteson 0 Comments 1 Like Add Videos View Announcements Building Predictive Analytics Solution Azure ML Predictive Analytics UX Case Study Webinar ensure big data big business value Making Decisions Visual Data IBM Modeler CPLEX Flipping 80 20 Rule Analytics Data Science Central Upcoming Educational Webinar Series Speed big data queries 1000x Pivotal HAWQ power IBM SPSS Statistics R together - White Paper Whitepaper Power R Visual Analytics 2015 Data Science Central Badges Report Issue Terms Service Hello need enable JavaScript use Data Science Central Please check browser settings contact system administrator"),
('Awesome Tryo: a curated list of awesome resources related to Python, NLP & Machine Learning', "Skip content Sign Sign repository Explore Features Enterprise Blog Watch 19 Star 72 Fork 13 tryolabs awesome-tryo Code Issues Pull requests Pulse Graphs HTTPS clone URL Subversion checkout URL clone HTTPS Subversion Download ZIP curated list awesome resources use Tryolabs 53 commits 1 branch 0 releases 6 contributors branch master Switch branches tags Branches Tags master Nothing show Nothing show awesome-tryo Merge pull request 5 haraball master Fixed link NLTK latest commit 3840c92d25 eudoxia0 authored May 22 2015 Permalink Failed load latest commit information README md Fixed link NLTK May 22 2015 stack svg Add diagram explaining tech stack Feb 4 2015 README md Awesome Tryo curated list books libraries apps papers love Tryolabs work blazing startups help build complex projects using Python NLP Machine Learning Overview create amazing Internet Mobile products blazing startups combine python ecosystem machine learning natural language processing technologies create heavy backend apps artificial intelligence components follow agile methodologies order develop MVPs full products lean way Training training period Tryolabs least two weeks goal get speed tools company uses repo contains list tutorials documentation useful becoming familiar Django Python ecosystem well ML NLP techniques training period recommend least hour day pairing mentor experience team get know work process tools goal get mentor coment tasks person training Development Tools Python virtualenv virtualenvwrapper useful development tool lets us create isolated Python environments every project isolating set libraries used project system virtualenv virtualenvwrapper iOS cocoapods Package manager iOS projects Handle setup update XCode projects speed integration new components cocoapods nomad CLI iOS projects various tools perform common task command line ex generate sign ditribute OTA ipa nomad Vagrant Vagrant tool creating isolated reproducible development environments using virtual machines usually used VirtualBox supports VMWare virtualization systems Docker Docker tool creating managing software containers Metamon Metamon tool automatically set isolated execution environment Django applications Source Control use git good resource Pro Git book Scott Chacon GitHub's help site Editors IDEs Emacs PyDev Standards Conventions PEP8 definitive reference Python coding style pep8 package used scan code find parts don't conform PEP8 standard Emacs emacs-pep8 package used run pep8 py script Deploying use Ansible deployment server orchestration tasks Databases Relational use Postgres It's database it's complete relational database framework provides full-text search GIS extensive documentation every knob lever NoSQL sure Postgres can't want Document CouchDB RethinkDB Elasticsearch Key-Value Redis Dynamo Riak Graph Neo4j Libraries Machine Learning SciPy scikit-learn Natural Language Toolkit NumPy Web Django Django REST framework Scrapy Books list books represents opinion good balance theory practice don't expect everyone read rather take books common list Machine Learning Machine Learning Art Science Algorithms Make Sense Data Learning scikit-learn Machine Learning Python Elements Statistical Learning Data Mining Inference Prediction Principles Data Mining Foundations Statistical Natural Language Processing Bayesian Reasoning Machine Learning Gaussian Processes Machine Learning Information Theory Inference Learning Algorithms Information Retrieval Managing Gigabytes Compressing Indexing Documents Images Introduction Information Retrieval Information Retrieval Algorithms Heuristics Computer Vision Concise Computer Vision Computer Vision Algorithms Applications Learning OpenCV Computer Vision C OpenCV Library Scala Play Scala Scala Example Software Architecture Architecture Open Source Applications Programming Language Structure Interpretation Computer Programs Papers Information Retrieval Object-Oriented Architecture Text Retrieval General Functional Geometry Pictures simple structured graphics model Problem Threads Threads Evil Web Design Aggregators Dribbble siteInspire Flat UI Design Mobile Patterns Icons Mfizz font Devicons Ikons Icomoon Iconic Libraries Resources Device mockups Browser logos SweetAlert Grid forms Tech Stack First things first Machines meant identical Ansible provisions local Vagrant box way provisions server way production environment development one avoid hard find bugs fairly certain something works dev work prod Specifically machines look like application run inside virtualenv even it's application server makes easy add applications need arise instance might want run IPython Notebook server Notebook provides analytics charts data database without contaminating app's environment IPython's dependencies Nginx used reverse proxy sending requests Internet Django server responses way around Nginx take care load balancing caching HTTP acceleration degree security Supervisor used keep actual application server running well running scripts processes Every process logged disk debugging Postgres database course tech stack looks roughly like projects course approximation projects use NoSQL databases addition relational ones others use things like message queues use specific tools like Varnish instead Nginx HTTP acceleration Status API Training Shop Blog 2015 GitHub Inc Terms Privacy Security Contact Something went wrong request Please try"),
('Teradeep - Deep neural network for large-scale object recognition', 'Skip content Sign Sign repository Explore Features Enterprise Blog Watch 14 Star 75 Fork 11 teradeep demo-apps Code Issues Pull requests Pulse Graphs HTTPS clone URL Subversion checkout URL clone HTTPS Subversion Download ZIP Teradeep may 2015 top neural network large-scale object recognition https www youtube com watch v _wXHR-lad-Q 18 commits 1 branch 0 releases 2 contributors C 53 7 Lua 43 5 Makefile 2 8 C Lua Makefile branch master Switch branches tags Branches Tags master Nothing show Nothing show demo-apps Update README md latest commit dd654a9da6 culurciello authored May 22 2015 Permalink Failed load latest commit information lib Added Linux support May 17 2015 gitignore ignore May 17 2015 LICENSE md Update LICENSE md May 18 2015 README md Update README md May 22 2015 display lua first commit May 17 2015 frame lua Added Linux support May 17 2015 icon jpg icon added May 17 2015 process lua first commit May 17 2015 run lua removed print categories May 18 2015 README md demonstration application May 2015 top neural network large-scale object recognition trained recognize typical home indoor outdoor objects daily life trained 10 M images private dataset serve good pair eyes machines robots drones wonderful creations See action video 1 also video 2 application tinkerers hobbiest researchers evaluation purpose non-commercial use tested OS X 10 10 3 Linux run 17 fps MacBook Pro Retina 15-inch Late 2013 CPU install Install Torch7 http torch ch Please download files model net categories txt stat t7 https www dropbox com sh qw2o1nwin5f1r1n AADYWtqc18G035ZhuOwr4u5Ea dl 0 Linux camera install cd lib make make install Note Makefile wants Torch7 installed usr local bin otherwise please change accordingly run run webcam display local machine qlua run lua Zoom window 2 number qlua run lua -z 2 usage Feel free modify use non-commercial projects Interested parties license Teradeep technologies contacting us info teradeep com importantly fun Life short need produce credits Aysegul Dundar Jonghoon Jin Alfredo Canziani Eugenio Culurciello Berin Martini contributed work demonstration Thank Status API Training Shop Blog 2015 GitHub Inc Terms Privacy Security Contact Something went wrong request Please try'),
('Slightly meta:', "we're hiring blog imgur imgur store uploading tools developer api need help terms privacy ad choices upload images 23 Upload Images Video GIF Make Meme sign sign Loading browse computer drag drop Ctrl V paste clipboard publish Imgur create album Start Upload Blog Horizontal Grid Edit Images 0 uploading agree terms service Optimizing large GIFs file type supported Supported formats JPEG GIF PNG APNG TIFF BMP PDF XCF share 1 week ago 8 804 views stats Download full resolution Get embed codes Love Imgur Join team store help blog request deletion terms privacy apps api advertise ad choices 2015 Imgur Inc prev next top Today's best images prev next browse way I'm sure Uploaded 0 Points 0 ups 0 downs - Views - Bandwidth usage - Comments - Favorites Daily Cumulative - waiting image data Loading views Get image internet"),
('Sorry no whitepaper: Whats in This Picture? AI Becomes as Smart as a Toddler', "Bloomberg Company Products Bloomberg Anywhere Login Bloomberg Connecting decision makers dynamic network information people ideas Bloomberg quickly accurately delivers business financial information news insight around world Company Careers Diversity Inclusion Philanthropy Engagement Sustainability Technology History Facts Financial Products Bloomberg Professional Bloomberg Tradebook Bloomberg Briefs Bloomberg Indexes Bloomberg SEF Bloomberg Institute Service Center Downloads Enterprise Products Enterprise Solutions Trading Solutions Bloomberg Vault Media Bloomberg Business Bloomberg Politics Bloomberg View Bloomberg Television Bloomberg Radio Bloomberg Mobile Apps News Bureaus Customer Support Americas 1 212 318 2000 Europe Middle East Africa 44 20 7330 7500 Asia Pacific 65 6212 1000 Communications Press Announcements Press Contacts Industry Products Bloomberg Government Bloomberg Law BNA Bloomberg Big Law Bloomberg New Energy Finance Media Services Advertising Bloomberg Content Service Bloomberg Live Conferences Follow Us Facebook Twitter LinkedIn Instagram Menu News Markets Insights Video Reading Search Global Europe Latest World Industries Science Energy Technology Design Culture Graphics Pursuits View U Politics Businessweek Stocks Currencies Commodities Rates Bonds Magazine Benchmark Watchlist Economic Calendar Latest Game Plan Business Schools Small Business Personal Finance Profiles Watch Video Schedule Shows Radio Events Photographer Tomohiro Ohsumi Bloomberg Picture AI Becomes Smart Toddler Developments machine learning allow computers answer complex questions contents images Don't Miss Follow us Facebook Twitter Instagram Youtube Jack Clark 9 16 PM EDT May 21 2015 Share FacebookShare Twitter Share LinkedInShare RedditShare Google E-mail Artificial intelligence graduated past infancy stage figuring what's image Computers previously capable little simple game Spy Name specific object person they'll show image containing thanks new developments AI research machines answer complex questions like grass except person answer awkwardly worded enigma take look last image research paper published Thursday Cornell University's Arxiv outlines system learns identify fine-grained visual features images words associated combines two dictionary digital brain references answer new questions never-before-seen images research conducted team comprised experts Chinese Internet search company Baidu student University California Los Angeles coincides similar research Microsoft Virginia Tech various academic institutions came recently goal enable computer connect language experiences physical world says Wei Xu distinguished scientist Baidu's research group important solving problem common sense reasoning Courtesy Baidu Bloomberg put Baidu UCLA system test took picture small citrus fruit palm hand sent Baidu question center hand software answered orange It's actually satsuma we'll let slide development may sound small teaching computers discern what's inside images associate language proved immensely challenging research draws different disciplines recently started converge Advances field brings us closer day may able ask search engine like Google Baidu ferret millions images find ones containing Volkswagen bus flat tire seven oranges bowl development Baidu UCLA important far perfect system can't handle multiple questions row like asking types fruit basket asking count number apples tests gave correct answer 64 7 percent time paper says People answered questions 94 8 percent accuracy current stage system ready serious applications still makes errors Xu says things tend proceed quickly AI Since major image recognition challenge called Imagenet began 2010 rate computers misidentify items fallen fourfold AI system misidentified questions images Courtesy Baidu Baidu UCLA's research follows earlier attempts researchers universities around world including Max Planck Institute Informatics Germany University California Berkeley University Toronto various technology companies Microsoft's motivation nurture development ultra-smart AI systems could included products Microsoft Baidu used research generate humongous databases groups use test systems Microsoft plans organize annual challenge workshops spur researchers explore area Creating computers look images answer specific questions distinctive advantage pushing frontiers AI-complete problems Microsoft Virginia Tech wrote paper Given recent progress community believe time ripe take endeavor Correct answers AI system developed Baidu UCLA Courtesy Baidu Work done UCLA two startups focused analysis surveillance videos One day AI may able monitor security camera footage quickly automatically discover unmarked vans parked outside banks four hours without moving Baidu interested aspects future potential applications education mobile image search Xu says AI might cater lessons students instance quizzing types animals photo parents shot weekend trip zoo new research computers reached milestone unlike many young kids figuring world show machine Dr Seuss book tell cover book cat wearing red white striped hat Tech Internet Los Angeles Software Germany Terms Service Trademarks Privacy Policy 2015 Bloomberg L P Rights Reserved Careers Made NYC Advertise Ad Choices Website Feedback Help Please upgrade Browser browser out-of-date Please download one excellent browsers Chrome Firefox Safari Opera Internet Explorer"),
('So I used Latent Dirichlet Allocation to automatically group Steam games by genre based on the words used on the store page. The results are pretty interesting.', "Skip content Sign Sign repository Explore Features Enterprise Blog Watch 2 Star 2 Fork 0 asrivat1 LatentDirichletAllocation Code Issues Pull requests Pulse Graphs HTTPS clone URL Subversion checkout URL clone HTTPS Subversion Download ZIP 22 commits 1 branch 0 releases Fetching contributors Java 55 0 Python 44 0 Java Python branch master Switch branches tags Branches Tags master Nothing show Nothing show LatentDirichletAllocation removed extra files latest commit 79b9e22e86 asrivat1 authored May 19 2015 Permalink Failed load latest commit information LDA java simulated data recovered May 19 2015 README md Typo May 19 2015 analyze py LDA complete steam May 9 2015 common_words games Full corpus better regex May 17 2015 games-names Full corpus better regex May 17 2015 grab_info_steam py Full corpus better regex May 17 2015 results Full corpus better regex May 17 2015 run-LDA sh README May 13 2015 sim_data py Sim data May 19 2015 steam-llik Full corpus better regex May 17 2015 steam-phi Full corpus better regex May 17 2015 steam-theta Full corpus better regex May 17 2015 test simulated data recovered May 20 2015 test_out-llik simulated data recovered May 20 2015 test_out-phi simulated data recovered May 20 2015 test_out-theta simulated data recovered May 20 2015 true-phi simulated data recovered May 20 2015 true-theta simulated data recovered May 20 2015 README md Topic Detection Steam Video Games Using Latent Dirichlet Allocation Akshay Srivatsan - asrivat1 jhu edu Overview goal project use Latent Dirichlet Allocation model perform topic detection video games Steam library Steam video game marketplace 125 million active users hosts thousands popular games PC game Steam corresponding web page contains brief description title keywords technical specifications finally several user written reviews page also provides link twelve games deemed similar Steam's recommendation system inherent connectedness interpreted graph games nodes links edges scrape graph using breadth first search obtained HTML page simple filtering eliminating tags extracting alphabetic tokens converting lowercase use stoplist eliminate commonly used words tokens pages topic detection using LDA model LDA model take approach word w document related topic z K possible topics total document distribution theta frequency topic within document topic distribution phi probability word given topic place Dirichlet priors phi theta hyperparameters alpha beta order learn model use Gibbs Sampling Gibbs Sampling inference technique works repeatedly randomly sampling latent variables z case estimating parameters based sampling effect takes random walk sample space enough iterations sampling distribution converge towards steady state best describes true distribution data Results obtained following results dataset 1247 titles using 25 topics hyperparameters 0 1 0 01 alpha beta respectively 1100 iterations 1000 spent burning sampling chain stable distribution see list topic words likely generated topic another list topic documents game pages Steam highest proportion topic Common Words Topics Topic 0 rpg combat character world quests system skills quest level characters magic items heroes skill party Topic 1 weapons zombies dead guns zombie enemies fps survival gun weapon shooter kill ammo combat map Topic 2 life half needed wood coo counter strike source need valve episode radio fire backup portal Topic 3 gb hd nvidia dlc intel pack geforce radeon core amd edition ati windows series ghz Topic 4 turn based strategy tactical combat enemy units tactics missions campaign space games battle mission board Topic 5 game fixed added steam update release issues version support bugs updates games fix mode comments Topic 6 review helpful funny found reviews game people read recommended account yes record products posted hrs Topic 7 game time make ve re play want need back bad start ll things review Topic 8 war units strategy total battle campaign ai ii unit battles pack empire multiplayer army men Topic 9 levels level music platformer puzzle soundtrack platforming jump controls world hard simple challenging indie controller Topic 10 enemies mode game action fun boss characters controller fighting attack combat character arcade enemy level Topic 11 story characters adventure point character games click voice series acting art dialogue plot game original Topic 12 stealth metro funny prison shadow cell starve clean night la day november people guards blood Topic 13 sonic tower defense civilization strategy games civ game defenders map endless towers units research ai Topic 14 space ship ships planet add combat star galaxy planets simulator train universe system build fleet Topic 15 early access review game build april development building world crafting community features alpha feedback version Topic 16 hat cards card play deck magic free pay pvp mmo money players online player win Topic 17 enemy spotted city cities people building simulator build simulation world tropico bridge money buildings motion Topic 18 game games gameplay good great pretty graphics feel experience find nice bit interesting player reviews Topic 19 game players play team player review free friends multiplayer people fun online community match maps Topic 20 racing cars lego car batman race physics driving tracks simulator track series truck drive city Topic 21 dungeon items rogue enemies die roguelike fun monsters generated run loot randomly random isaac dungeons Topic 22 nope horror atmosphere story myst experience puzzles dark world arcade exploration played scary explore anarchy Topic 23 puzzles puzzle hidden object ve games story solve adventure click objects post gb point command Topic 24 original wars star classic doom ii jedi ys knight force years great played edition version Documents High Proportion Topics Topic 0 Breath Death VII Book Legends Cthulhu Saves World Divine Divinity Sacred 2 Gold Incredible Adventures Van Helsing Sacred Gold Divinity II Developer's Cut Asguaard Titan Quest - Immortal Throne Gothic II Gold Edition Knights Pen Paper 1 Edition WAKFU Torchlight II Incredible Adventures Van Helsing II Topic 1 Rising Storm Game Year Edition L K E R Call Pripyat Dying Light theHunter Primal Red Orchestra Ostfront 41-45 Serious Sam 3 BFE Room Hell National Zombie Park War Mine Enemy Front Serious Sam Classic First Encounter Contagion Dead Pixels Serious Sam Classic Second Encounter Receiver Topic 2 Stronghold Crusader HD Counter-Strike Source Half-Life Blue Shift Hatoful Boyfriend Half-Life 2 Episode Two Half-Life 2 Half-Life 2 Lost Coast Half-Life 2 Episode One Half-Life Source Half-Life Half-Life Opposing Force Counter-Strike Condition Zero Counter-Strike Global Offensive Counter-Strike Team Fortress Classic Topic 3 Street Fighter X Tekken Call Duty Black Ops III Toren Ultra Street Fighter IV Worms Revolution Worms Reloaded Legend Kay Anniversary Witcher 3 Wild Hunt F1 2013 Painkiller Hell Damnation Call Duty Modern Warfare 2 Worms Ultimate Mayhem Might Magic Heroes VI Transformers Fall Cybertron Worms Clan Wars Topic 4 Mordheim City Damned Space Hulk Space Hulk Ascension WARMACHINE Tactics Warhammer 40 000 Regicide Warhammer 40 000 Armageddon Talisman Prologue Warhammer 40 000 Dawn War Winter Assault Blood Bowl Legendary Edition Frozen Synapse Shadowrun Dragonfall - Director's Cut Bionic Dues Chainsaw Warrior Jagged Alliance 2 Gold Hell Topic 5 Gang Beasts GameGuru Pool Nation Castle Story BeamNG drive Cities XXL Tabletop Simulator Universe Sandbox CDF Ghostship Planetary Annihilation Audiosurf 2 Cities Skylines Cause 3 Spacebase DF-9 Trine 3 Artifacts Power Topic 6 Plug Play Floating Point Basement Collection Super Crate Box Impossible Game Jagged Alliance 2 Gold Toki Tori Faerie Solitaire AudioSurf Counter-Strike LEGO Star Wars - Complete Saga Half-Life 2 Lost Coast Universe Sandbox Sonic Hedgehog AdVenture Capitalist Topic 7 Vindictus Streets Chaos Always Sometimes Monsters War Mine Football Manager 2015 Zafehouse Diaries Windforge SUNLESS SEA Age Decadence Fiesta Online NA Wasteland 1 - Original Classic Broken Age Elite Dangerous Firefall MapleStory Topic 8 Empire Total War Crusader Kings II Men War Assault Squad Hearts Iron III Medieval II Total War Total War Shogun 2 - Fall Samurai Napoleon Total War Rome Total War Panzer Corps Men War Europa Universalis III Complete End Wars Total War ROME II - Emperor Edition Commander Great War Europa Universalis IV Topic 9 Giana Sisters Twisted Dreams BIT TRIP Presents Runner2 Future Legend Rhythm Alien Giana Sisters Twisted Dreams - Rise Owlverlord BIT TRIP RUNNER Fly'N Electronic Super Joy Fermi's Path Super Puzzle Platformer Deluxe Offspring Fling Soundodger BIT TRIP BEAT 1001 Spikes Dustforce DX Beatbuddy Tale Guardians JumpJet Rex Topic 10 Divekick Mitsurugi Kamui Hikae Devil May Cry 3 Special Edition Kung Fu Strike - Warrior's Rise GundeadliGne Aqua Kitty - Milk Mine Defender GIGANTIC ARMY Megabyte Punch Waves One Finger Death Punch Vanguard Princess Devil May Cry 4 Phantom Breaker Battle Grounds Super Galaxy Squadron Ultratron Topic 11 Blackwell Unbound Blackwell Convergence Blackwell Legacy Edna Harvey Harvey's New Eyes Goodbye Deponia Blackwell Deception Dreamfall Chapters Randal's Monday Broken Sword 5 - Serpent's Curse Broken Sword Director's Cut Chaos Deponia Next BIG Thing Deponia Monkey Island 2 Special Edition LeChucks Revenge Cat Lady Topic 12 Viscera Cleanup Detail Santa's Rampage Surgeon Simulator 2013 Prison Architect Viscera Cleanup Detail Shadow Warrior Five Nights Freddy's 2 Viscera Cleanup Detail Five Nights Freddy's TrackMania Nations Forever Cook Serve Delicious 100 Orange Juice Escapists Bread Tom Clancys Splinter Cell Blacklist Octodad Dadliest Catch Little Inferno Topic 13 Sonic Hedgehog 2 Sonic 3 Knuckles Sonic Hedgehog Sid Meier's Civilization Beyond Earth Civilization IV Beyond Sword DG2 Defense Grid 2 Sid Meier's Civilization IV Colonization Dungeon Defenders Endless Legend Revenge Titans Sonic CD Sonic Generations Sonic Hedgehog 4 - Episode II Sid Meier's Civilization V Galactic Civilizations II Ultimate Edition Topic 14 Train Simulator 2015 X2 Threat X Rebirth Evochron Mercenary Distant Worlds Universe X3 Reunion Elite Dangerous Space Pirates Zombies 2 X3 Terran Conflict Galaxy Fire 2 Full HD Edition X Beyond Frontier Strike Suit Zero Horizon Kerbal Space Program Topic 15 Medieval Engineers Rising World Oort Online Stranded Deep Subnautica FortressCraft Evolved Eden Star Destroy - Build - Protect Savage Lands GRAV Predestination Salt Landmark Blockscape Xsyon - Prelude Beasts Prey Topic 16 Team Fortress 2 Magic Gathering - Duels Planeswalkers 2013 Magic 2014 Duels Planeswalkers Kingdoms CCG Nightbanes Magic Gathering - Duels Planeswalkers 2012 SolForge Magic 2015 - Duels Planeswalkers Might Magic Duel Champions Pox Nora Infinity Wars 2014 Animated Trading Card Game Ragnarok Online 2 BloodRealm Battlegrounds Royal Quest Battlegrounds Eldhelm Topic 17 Battlefield 2 Complete Collection Moonbase Alpha Cities Motion Train Simulator South London Network Route Add-On Tropico 4 Steam Special Edition Race White House Train Fever Cities Motion 2 Riding Star Tropico 3 - Steam Special Edition Masters World - Geopolitical Simulator 3 Democracy 3 Banished SimCity 4 Deluxe Edition Tropico 5 Topic 18 Aquaria Claire Full Bore Forward Sky Old City Leviathan Swapper NightSky Brothers - Tale Two Sons Story Uncle Mind Path Thalamus FRACT OSC Styx Master Shadows Closure Skyborn NaissanceE Topic 19 Awesomenauts Strife Solstice Arena HAWKEN AirMech Quake Live Super MNC Warface Freestyle2 Street Basketball AERENA - Masters Edition Infinite Crisis GunZ 2 Second Duel Blacklight Retribution Block N Load Sins Dark Age Topic 20 Assetto Corsa GRID Autosport RaceRoom Racing Experience Need Speed Hot Pursuit Euro Truck Simulator 2 GRID 2 Euro Truck Simulator DiRT Showdown Crew Project CARS LEGO Batman3 Beyond Gotham Driver San Francisco Copa Petrobras de Marcas LEGO Batman 2 DC Super Heroes Gotham City Impostors Free Play Topic 21 Ziggurat Rogue Legacy Diehard Dungeon Binding Isaac Rebirth Darker Purpose Sword Stars Pit Full Mojo Rampage Legend Dungeon Dungeons Dredmor Enter Gungeon Hack Slash Loot Saints Row Gat Hell Wizard's Lizard Dungeonmans Overture Topic 22 Cry Fear Anarchy Arcade Outlast Fall Risk Rain Cylne Among Sleep Slender Arrival Amnesia Machine Pigs Neverending Nightmares Claire Botanicula NaissanceE Amnesia Dark Descent Passing Pineview Forest Topic 23 STAR WARS Battlefront II Enigmatis Ghosts Maple Creek Grim Legends Forsaken Bride Time Mysteries 2 Ancient Spectres Nightmares Deep 2 Siren Call Nightmares Deep Cursed Heart Time Mysteries Inheritance - Remastered 9 Clues Secret Serpent Creek Nightmares Deep 3 Davy Jones Demon Hunter Chronicles Beyond Left Dark One Board Clockwork Tales Glass Ink Grim Legends 2 Song Dark Swan Enigmatis 2 Mists Ravenwood Abyss Wraiths Eden Topic 24 STAR WARS Jedi Knight - Mysteries Sith STAR WARS Jedi Knight - Dark Forces II STAR WARS Jedi Knight II - Jedi Outcast STAR WARS Jedi Knight - Jedi Academy STAR WARS - Dark Forces STAR WARS - Force Unleashed II STAR WARS - Force Unleashed Ultimate Sith Edition STAR WARS Knights Old Republic II - Sith Lords Wolfenstein 3D Deus Ex Mankind Divided Tomb Raider Anniversary Oddworld Abe's Exoddus STAR WARS Empire War - Gold Pack Baldur's Gate II Enhanced Edition Final DOOM Log Likelihood See output files specific learned parameter values Simulated Data also created set simulated data named test using artificial hyperparameters 0 5 0 5 5 topics vocabulary consisting letters j corpus 1000 documents 500 words model able successfully recover true parameters model 2-3 decimal places accuracy running 1100 iterations 1000 burn See sim_data py file details artificial data generated files true test_out actual recovered parameters Discussion see results Gibbs Sampler done pretty good job uncovering latent topics present Steam web pages Note topics necessarily correspond genre appear relate elements reviews others related technical specifications I'll give detailed analysis consider interesting well formulated topics Topic 0 topic largely RPG games see clearly words associated Interestingly games topic seem divine gothic twist Topic 1 topic grouped zombie games see strongly correlated words zombie fps ammo etc might expect games fit description part except Rising Storm WWII game Perhaps it's strange discussion Nazi Zombies much prevalent internet one might expect Rising Storm fact several zombie themed mods Topic 2 topic contains games produced Valve incidentally company created Steam Valve games tend strong cult followings makes sense would see appear together Also since Valve created Steam understandable games made Valve would among popular ones Steam Topic 3 see words related system specifications required play game Almost terms ones would expect see discussion PC specs case topic related particular genre games don't look similar probably united requiring specific specifications run Topic 5 topic describes notes page related updates bugs patches Much like topic 3 nothing genre it's possible game high proportion topic particularly buggy Topic 6 like topic qutie lot nothing games Instead topic captures content gaming community reviews people leave accounts etc Many terms found template pages there's much meaning gleaned games correlated Topic 7 topic peculiar consists entirely short meaningless words fact looks though topic catchall generic tokens don't really fit topics Effectively words stopword list aren't Clearly terms like game play show almost page they're common enough English general included stoplist Topic 11 topic another one direcly related genre rather specific component review specifically discusses storyline game characters involved even dialogue voice acting One could theoretically use information topics like automatically segment reviews chunks story gameplay graphics etc use perform summarization across multiple reviews Topic 19 Games topic primarily free online multiplayers see relevant words there's strong emphasis teams people community Interestingly one three topics word fun among top listed Topic 22 straight horror games featuring dark atmospheric worlds player explore Humorously word nope makes top list games classic PC horror titles including Outlast Amnesia Slender Conclusions Future Work summary see topic detector fantastic job uncovering many basic genres well identifying specific elements reviews descriptions common games might interesting explore splitting single hidden topic variable two one genre game one component page review stated think would also worthwhile improve stoplist better reflect particular nomenclature describing video games especially important LDA model notion weighting based frequency generally might interesting see could incorporate weighting scheme model already know HTML words keywords official description categories Usage First run grab_info_steam py scrape Steam library data terminate process whenever like found good results obtained scraping 1200 titles may take close half hour depending connectivity run run-LDA sh inference LDA model using collected data results described ran using K 25 alpha 0 1 beta 0 01 1100 burnin 1000 names input output files games steam respectively run analyze py print brief summary parameters model learned takes arguments name file output file called games-names steam respectively default Status API Training Shop Blog 2015 GitHub Inc Terms Privacy Security Contact Something went wrong request Please try"),
('Big jump in CIFAR 10/100 accuracy: Spatially-sparse convolutional neural networks', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1409 6070 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context cs CV prev next new recent 1409Change browse cs cs NE References CitationsNASA ADS DBLP - CS Bibliography listing bibtex Benjamin Graham Bookmark Computer Science Computer Vision Pattern Recognition Title Spatially-sparse convolutional neural networks Authors Benjamin Graham Submitted 22 Sep 2014 Abstract Convolutional neural networks CNNs perform well problems handwriting recognition image classification However performance networks often limited budget time constraints particularly trying train deep networks Motivated problem online handwriting recognition developed CNN processing spatially-sparse inputs character drawn one-pixel wide pen high resolution grid looks like sparse matrix Taking advantage sparsity allowed us efficiently train test large deep CNNs CASIA-OLHWDB1 1 dataset containing 3755 character classes get test error 3 82 Although pictures sparse thought sparse adding padding Applying deep convolutional network using sparsity resulted substantial reduction test error CIFAR small picture datasets 6 28 CIFAR-10 24 30 CIFAR-100 Comments 13 pages Subjects Computer Vision Pattern Recognition cs CV Neural Evolutionary Computing cs NE Cite arXiv 1409 6070 cs CV arXiv 1409 6070v1 cs CV version Submission history Benjamin Graham view email v1 Mon 22 Sep 2014 02 39 27 GMT 193kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('Demistifying LSTM Neural Networks', "Home Subscribe Demystifying LSTM Neural Networks 18 May 2015 Given wide applicability real-world tasks deep learning attracted attention wide audience interested technologists investors spectators celebrated results use feedforward convolutional neural networks convnets solve problems computer vision less public attention paid developments using recurrent neural network model relationships time Note help begin experimenting LSTM recurrent nets I've attached snap simple micro instance preloaded numpy theano git clone Jonathan Raiman's LSTM example recent post Learning Read Recurrent Neural Networks explained despite incredible successes feedforward networks limited inability explicitly model relationships time assumption data points consist vectors fixed length posts' conclusion promised forthcoming post explaining basics recurrent nets introducing Long Short Term Memory LSTM model First basics neural networks neural network represented graph artificial neurons also called nodes directed edges model synapses neuron processing unit takes input outputs nodes connected emitting output neuron first applies nonlinear activation function activation function gives neural networks ability model nonlinear relationships consider recent famous paper Playing Atari Deep Reinforcement Learning combines convnets reinforcement learning train computer play video games system achieves superhuman performance games like Breakout proper strategy point deduced looking screen However system falls far short human performance optimal strategies require planning long spans time Space Invaders motivation introduce recurrent neural networks approach endows neural networks ability explicitly model time adding self-connected hidden layer spans time points words hidden layer feeds output also hidden layer next time step Throughout post I'll use illustrations recurrent networks pilferred forthcoming review literature subject unfold network across two time steps visualize connections acyclic way Note weights input hidden hidden output identical time step recurrent net sometimes described deep network depth occurs input output across time steps time step thought layer unfolded networks trained end end using backpropagation extension backpropagation span time steps called Backpropagation Time One problem however vanishing gradient described Yoshua Bengio frequently cited paper Learning Long-Term Dependencies Gradient Descent Difficult words error signal later time steps often doesn't make far enough back time influence network much earlier time steps makes difficult learn long-range effects taking pawn come back bite 12 moves remedy problem Long Short Term Memory LSTM model first described 1997 Sepp Hochreiter Jurgen Schmidhuber model ordinary neurons e units apply sigmoidal activation linear combination inputs replaced memory cells memory cell associated input gate output gate internal state feeds unperturbed across time steps model memory cell three sets weights learned input well entire hidden state previous time step One feeds input node pictured bottom One feeds input gate shown far right side cell bottom Another feeds output gate shown far right side cell top Blue node associated activation function typically sigmoidal Pi nodes represent multiplication centermost node cell called internal state feeds fixed weight 1 across time steps self-connected edge attached internal state referred constant error carousel CEC Thinking terms forward pass input gate learns decide let activation pass memory cell output gate learns let activation pass memory cell Alternative terms backwards pass output gate learning let error flow memory cell input gates learning let flow memory cell rest network models proven remarkably successful tasks varied handwriting recognition image captioning Perhaps love made win Space Invaders Zachary Chase Lipton's Picture Zachary Chase Lipton Zachary Chase Lipton PhD student Computer Science department UCSD researches machine learning theory applications contributing editor KDnuggets California http www zacklipton com Share post Twitter Facebook Google Get great Terminal posts like Terminal com Blog 2015 Proudly published Ghost"),
('VDiscover: large-scale vulnerability discovery using Machine Learning', 'VDiscover VDiscover makes large-scale vulnerability discovery possible using state-of-the-art Machine Learning techniques sustained growth software complexity finding security vulnerabilities operating systems become important necessity Unfortunately use costly techniques large amount testcases analyze still issue today lack source code access restricts even possibilities find vulnerabilities large scale motivated reasons design develop predictive approach vulnerability discovery Given costly vulnerability discovery procedure large amount testcases analyze VDiscover trained predict result costly analysis extracting different sets features using fully automatic lightweight approach Features directly executable files without source code Later tool predict outcome vulnerability discover procedure previously used details technical report available well open-source prototype VDiscover open-source test git clone https github com CIFASIS VDiscover git cd VDiscover python setup py install --user Sort documentation available quick introduction VDiscover features extended documentation technical report VDiscover researched developed CIFASIS Gustavo Grieco Guillermo Luis Grinblat Lucas Uzal IIT-H Sanjay Rawat VERIMAG Josselin Feist Laurent Mounier general enquiries feedback contacts us Design Tim O Brien t413 com SinglePaged theme site open source'),
('How We Think About Privacy and Finding Features in Black Boxes', 'Talking Machines Hello Media Episodes Contact Ways Listen Hello MediaEpisodesContactAboutWays Listen human conversation machine learning Episodes Hello MediaEpisodesContactAboutWays Listen May 21 2015 Think Privacy Finding Features Black Boxes May 21 2015 katherine gorman episode eleven chat Neil Lawrence University Sheffield talk problems privacy age machine learning responsibilities come using ML tools making data open learn Markov decision process happens use real world becomes partially observable Markov decision process take listener question finding insights features black boxes deep learning May 21 2015 katherine gorman katherine gorman Episodes Interdisciplinary Data Helping EPISODES CONTACT WAYS LISTEN NEWSLETTER MEDIA PARTNERS'),
('The Brain is a Universal Dynamical Systems Computer  Hierarchical Temporal Memory', "Better Living Thoughtful Technology Book Real Machine Intelligence Hire Fergal NuPIC Machine Intelligence Focus Sparse Distributed Representations Stay updated Facebook Twitter Brain Universal Dynamical Systems Computer Hierarchical Temporal Memory May 21 2015 0 Cortical Learning Algorithm Brain Universal Dynamical Systems Computer Hierarchical Temporal Memory Note post sketch paper progress due completed May-June 2015 believe discovered key function neocortex machine uses sensorimotor information complex systems world build utilise running simulacra systems Cortical Learning Algorithm HTM provides self-organising structure automatically emulate large class real-world phenomena design neocortex specifically suited task maintaining model world face nonstationarity complex system Nonlinear Dynamics Introduction OK lot jargon ll illustrate everyday example Riding real bicycle real road extraordinarily difficult task classical computer program try 1950 way d begin identifying big system partial differential equations find way solve numerically order control robot turns near impossible practice results system brittle inflexible another approach however One popular method used robotics control systems today PID proportional integral differential involves combining mixed feedback loops sensation action cute video system happening simple robot using sensors detect things going reacting changing sensory data order maintain stability robot-controller-bicycle-floor system example nonlinear dynamical system real world live full systems past several centuries physics tended avoid favour pretending world linear Much physics applied math learned school college approximates reality much simpler linear systems last century increasingly since advent computer simulations begun examine nonlinear dynamical systems detail famous recent result Dynamical Systems Science discovery Chaos involves evolution apparently unpredictable behaviour simple nonlinear deterministic systems Apart vaguely aware idea chaos well-educated people real knowledge nonlinear systems work known different systems related fact become perhaps primary field study applied mathematics past 40 years clever people made big progress understanding complex non-intuitive phenomena ll get back shortly Dynamical Systems Brain course one interesting systems type found brains Often described complex thing known universe brain indeed daunting thing study Many people examined neural structures dynamical systems proposed nonlinear dynamics key working brain works Indeed number researchers demonstrated simplified model neural networks exhibit kinds computational properties found brain example see Hoerzer et al fact appears brain looks like whole bunch interacting dynamical systems everywhere look scales Surely going make things harder understand Well yes Yes re going leave comfort training seeing everything linear venture world oddness unpredictability actually take leap understand nonlinear dynamics reveals true nature animal intelligence Dynamical Systems Information Nonlinear dynamical systems weird entirely deterministic rather random practically unpredictable often critically sensitive initial measured conditions practise might never repeat exactly sequences may contain huge numbers internal variables billions trillions leaving us hope using analytic methods order model Yet incredibly many dynamical systems miracle property export information collect information often sufficient us build model kinds dynamics original discovery made 1970 golden decade dynamical systems applied hugely diverse range areas old murky scratchy video Steve Strogatz Kevin Cuomo going Well sending circuit analog dynamical system executing one famous sets equations Dynamical Systems Lorenz Equations details important discussion essentially system three internal variables coupled together quite simple differential equations animation Lorenz system quite beautiful see elegant kind structure trajectories traced point strange kind symmetry spiralling twisting butterfly-like space lives fact system infinitely complex become Hello World dynamic systems science OK sending system behaving like Lorenz System certain voltages circuit acting like x y z coordinates animation receiving circuit also Lorenz emulator almost exactly setup sender re real electronic devices identical trick take one output sending circuit say x use x prime voltage receiving circuit Strogatz says book Sync x prime taken signal sender Normally x prime y prime z prime work together produce elegant trajectory see animation x prime simply ignoring dance partners appear choice synchronise interloper afar eerie effect much general might think turns using single stream measurements reconstruct dynamics huge range systems without needing knowledge internal variables equations result based Takens Theorem proves certain well-behaved systems Lorenz video three parts explains works Part One introduces Lorenz system Part Two illustrates Takens Theorem final part shows applied test causal connections time series Brain Universal Dynamical Computer phenomenon key neocortex exploiting information time series sensory data build replicas dynamics world use identification forecasting modelling communication behaviour Well nice know doesn explain let referred earlier work Gregor Hoerzer uses recurrent neural networks RNNs model kinds chaotic computation RNNs similar kinds Deep Learning artificial neural networks use extremely simple point neurons differ outputs may end hops part inputs gives RNNs lot power ANNs explains re currently hot topic Machine Learning believe successful right use tricks ve seen self-organise represent simulated dynamics thus allow amount modelling prediction generation RNNs powerful lack structure re hard us understand Perhaps structured type network would even power fingers crossed might easier understand reason Hierarchical Temporal Memory Cortical Learning Algorithm Jeff Hawkins HTM theory point neurons replaced far realistic model neurons much complex significant computational power Neurons packed columns columns arranged layers structure based detailed study real neocortex reasonable first-order approximation d see real brain key HTM layers combined connected like brain layer region small area cortex different inputs performs particular role computation ve written depth ll briefly summarise context dynamical systems rather intimidating diagram minimal sketch primary computational connections multilayer model shows key information flows region neocortex primary inputs region red blue arrows coming bottom going Layer 4 L6 well subpopulations cells L4 learn reconstruct dynamics sensorimotor inputs forecast transitions short timesteps L4 able predict upcoming evolution representation pooled time cells L2 L3 cells represent current dynamical regime evolving dynamics L4 characterises sensed system longer timescale fast-changing input region output L2 3 goes hierarchy higher regions treat dynamically evolving sensory input repeat process addition output goes L5 combines inputs L1 L6 produces behaviour learned interact world order preserve recover prediction entire region see mechanisms self-stabilisation system key thing subpopulations neurons capable learning model dynamics world many timescales changes characteristics real-world system cause changes choice subpopulation picked downstream layers leading new representation world region also motor behavioural reaction new dynamics pathways diagram crucial learning dynamical modelling perception higher-level regions provide even slower-changing inputs L2 3 L5 representing stable state working assisting cells maintain consistent picture world face uncertainty noise References completed Gregor M Hoerzer Robert Legenstein Wolfgang Maass Emergence Complex Computational Structures Chaotic Neural Networks Reward-Modulated Hebbian Learning Cereb Cortex 2014 24 3 677-690 first published online November 11 2012 doi 10 1093 cercor bhs348 Free Full Text Share Email Print Share Tumblr Comments comments Leave comment Click cancel reply Name Email Website Comment Real Machine Intelligence Available first three chapters new book Real Machine Intelligence Clortex NuPIC gone live Leanpub com Read online buy in-progress book also check pal Russ Miles' new book Antifragile Software affiliate link Upcoming Talks euroClojure 2014 I'll speaking HTM CLA Clortex NuPIC June 27th Krakow Poland Recent Posts Brain Universal Dynamical Systems Computer Hierarchical Temporal Memory Self-Stabilisation Hierarchical Temporal Memory Multilayer Model Hierarchical Temporal Memory Response Yann LeCun Questions Brain Mathematics HTM Part II Transition Memory Follow TwitterMy TweetsKindle Recommendations Amazon com Widgets Archives May 2015 January 2015 December 2014 November 2014 September 2014 August 2014 June 2014 May 2014 April 2014 March 2014 November 2013 Categories Clojure Clortex HTM Clojure Cortical Learning Algorithm General Interest NuPIC NuPIC-Dev Real Life Meta Log Entries RSS Comments RSS WordPress org Subscribe Blog via Email Enter email address subscribe blog receive notifications new posts email Email Address Please activate Widgets 2013 Fergal Byrne Brenter Rights Reserved Created Site5 WordPress Themes Experts WordPress Hosting Send Email Address Name Email Address Cancel Post sent - check email addresses Email check failed please try Sorry blog cannot share posts email"),
('new DeepMind paper: Weight Uncertainty in Neural Networks', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org stat arXiv 1505 05424 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context stat ML prev next new recent 1505Change browse cs cs LG stat References CitationsNASA ADS Bookmark Statistics Machine Learning Title Weight Uncertainty Neural Networks Authors Charles Blundell Julien Cornebise Koray Kavukcuoglu Daan Wierstra Submitted 20 May 2015 v1 last revised 21 May 2015 version v2 Abstract introduce new efficient principled backpropagation-compatible algorithm learning probability distribution weights neural network called Bayes Backprop regularises weights minimising compression cost known variational free energy expected lower bound marginal likelihood show principled kind regularisation yields comparable performance dropout MNIST classification demonstrate learnt uncertainty weights used improve generalisation non-linear regression problems weight uncertainty used drive exploration-exploitation trade-off reinforcement learning Comments Proceedings 32nd International Conference Machine Learning ICML 2015 Subjects Machine Learning stat ML Learning cs LG Cite arXiv 1505 05424 stat ML arXiv 1505 05424v2 stat ML version Submission history Charles Blundell view email v1 Wed 20 May 2015 15 39 48 GMT 252kb D v2 Thu 21 May 2015 14 07 23 GMT 252kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('[1412.6806] Striving for Simplicity: The All Convolutional Net', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1412 6806 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context cs LG prev next new recent 1412Change browse cs cs CV cs NE References CitationsNASA ADS DBLP - CS Bibliography listing bibtex Jost Tobias Springenberg Alexey Dosovitskiy Thomas Brox Martin Riedmiller Martin Riedmiller Bookmark Computer Science Learning Title Striving Simplicity Convolutional Net Authors Jost Tobias Springenberg Alexey Dosovitskiy Thomas Brox Martin Riedmiller Submitted 21 Dec 2014 v1 last revised 13 Apr 2015 version v3 Abstract modern convolutional neural networks CNNs used object recognition built using principles Alternating convolution max-pooling layers followed small number fully connected layers re-evaluate state art object recognition small images convolutional networks questioning necessity different components pipeline find max-pooling simply replaced convolutional layer increased stride without loss accuracy several image recognition benchmarks Following finding -- building recent work finding simple network structures -- propose new architecture consists solely convolutional layers yields competitive state art performance several object recognition datasets CIFAR-10 CIFAR-100 ImageNet analyze network introduce new variant deconvolution approach visualizing features learned CNNs applied broader range network structures existing approaches Comments accepted ICLR-2015 workshop track changes style Subjects Learning cs LG Computer Vision Pattern Recognition cs CV Neural Evolutionary Computing cs NE Cite arXiv 1412 6806 cs LG arXiv 1412 6806v3 cs LG version Submission history Alexey Dosovitskiy view email v1 Sun 21 Dec 2014 16 16 37 GMT 3566kb D v2 Mon 2 Mar 2015 21 44 06 GMT 4448kb D v3 Mon 13 Apr 2015 07 58 17 GMT 4437kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('Resig, Using Waifu2x to Upscale Japanese Prints', 'Home Blog Research John Resig Contact Subscribe Using Waifu2x Upscale Japanese Prints spare time ve working database Japanese prints little 3 5 years m fully aware ve never actually written personally important project blog Unfortunately isn post explaining project still hope write intricacies day time watch talk gave OpenVisConf 2014 site tech behind ve lot work exploring different computer vision machine learning algorithms see might apply world art history study m especially interested finding novel uses technology could greatly benefit art historians work also help individuals better appreciate art One tool came across yesterday called Waifu2x convolutional neural network CNN designed optimally upscale images taking small images generating larger image creator tool built better upscale poorly-sized Anime images video effort massively cheer m purveyor Anime love applying algorithmic overkill non-tech hobbies Waifu2x also provides live demo site use test images saw became immediately intrigued Anime direct stylistic influences drawn old world Japanese woodblock printing popular late 1600s late 1800s Maybe pre-trained upscaler could also work well photos Japanese prints Naturally could train new CNN may even necessary first questions come even attempting upscale Japanese prints simply enough tiny images prints need made bigger benefit answer questions Unfortunately tons tiny pictures Japanese prints world provide one real example Tokyo National Museum one greatest collections Japanese prints world none publicly digitized researcher wants see TNM particular copy print ll need use following 3 volume set books Inside book 3 926 small black-and-white scans every print collection plan digitizing books bringing not-ideal prints online utmost use scholars However given tiny size hard researchers able make exactly print depicting Thus technology able upscale images make bit easier view would greatly appreciated began experimenting different existing print images really intrigued results started primitive Ukiyo-e print Hishikawa Moronobu Source Image image scaled 2x OSX Preview using Waifu2x using high noise reduction sure click images see full size another early actor print Utagawa Kunisada Source Image image scaled 2x OSX Preview using Waifu2x low noise reduction high noise reduction sure click images see full size also blown-up details comparing results three OSX Preview 2x Waifu2x low noise reduction Waifu2x high noise reduction immediately apparent lines still quite crisp Waifu2x versions extremely compelling able see details quite important fact seeing upscaled images like seriously quite impressive definitely reinforces fact algorithms Anime training data suited subject matter well eye almost looks like entire image also become smooth seems much mottled almost someone spilled water Japanese prints printed watercolors thus quite susceptible water damage actually creates similar results seen especially true using Waifu2x high noise reduction clear would become better better training data original source image really much data begin Waifu2x high noise reduction also causes background become much tumultuous crumpled wrapping paper details signature make readable tend lost much processing suspect using low noise reduction may better sweet spot Helping researchers able better see lines print tiny image double-edged sword certainly appreciate able larger semi-crisp image However lack precision massive problem original image 2x would ve better Researchers rely upon able spot minute differences print impressions order able understand print could ve created suspect using technique need reserved extremely-small images come massive caveat warning viewer nature image appearance Regardless technology quite exciting extremely serendipitous subject matter used happens correlate nicely one areas study ll curious see else people success particular utility context Posted May 20th 2015 Subscribe email updates 17 Comments Dave May 20 2015 6 23 pm hahaha John Resig browsing 4chan tom c May 20 2015 6 23 pm trouble believing re anime fan read Javascript Ninja book aside learning lot Javascript didn know tons embarrassingly enjoyable references Ichigo Naruto Death Note anime popular Even longer anime fan today one point life probably enjoy possibly little much old sins long shadows John Resig May 20 2015 6 30 pm Dave Hmm came across Hacker News believe also one top trending projects Github right Tom quite amusing references actually co-author Bear Unfortunately haven seen Animes mention contempt simply haven managed get around day joel May 20 2015 6 33 pm noise images seem jpeg compression enlarging software impressive mostly seems removing jpeg artifacts Maybe source scans compressed John Resig May 20 2015 6 42 pm Joel sort point attempting make Taking normal small jpeg image simply scaling 2x width height results image riddled jpeg artifacts Using Waifu2x end something smoother certainly less noise mikemikemikemikemiks May 20 2015 6 42 pm would call JPEG cleanerupperer since seem help resolution make image look clean vivid John Resig May 20 2015 6 44 pm mike IMO inherently best upscaler hope achieve unless source material effectively treated series scalable vectors sized-up Tom Wulf May 20 2015 7 13 pm cool John teach technology strong interests humanities especially Old Norse Viking culture Using JQM create rich hypertext versions Sagas Recently camp unconference University Cincinnati national thing actually know lot think might find great place create bridge tech folks Humanities folks thatcamp org although site seems right Also Wikipedia entry Cheers John Resig May 20 2015 7 37 pm Tom Wulf m glad jQM able help digital humanities work m actually quite familiar THATCamp ve actually attended 3 two NYC one Chicago find fantastic environment learning cross-pollination Thank suggestion Scott May 20 2015 8 38 pm Would easier go Museum take pics pics get permission scan hi-res perhaps positives negatives storage scanning book means scanning 4 plates Cyan Magenta Yellow black making rosette pattern unless printed stochastically would improve scanning greatly Though doubt means scan ton dots probably nowhere near 150 lines per inch need get even decent results Back days pre-press film drum scanners etc would take films plates used print book scan individual films put back together apply dithering get pretty good results Basically better input less work output would look getting better input Scanning book going hard way make hi-res print colors solid spot colors additions CMYK like Cyan Yellow equaling 100 green solid colors excellent results blend 50 Cyan 90 yellow make slightly greenish blue trouble going come Happy talk offline need told Museum one photo headshot every Kamikaze pilot went pics taken one museum one send parents Apparently grandfather took pictures though sure father passes chance officially confirm told since child John Resig May 20 2015 8 53 pm Scott One would think institution would amenable something like However practice seems anything contacts Japan museum digitization world none mentioned possibility becoming possible makes skeptical potential Obviously material directly source would best barring scanning book definitely preferable nothing Leif May 21 2015 1 04 reminds work years ago processing Japanese internee records WWII war many Japanese people Japanese backgrounds born Australia Japanese parents generally arrested placed internment camps Whilst must horrific arrested sent camp middle nowhere parent Japanese ways also removed immediate danger someone seeking revenge death son brother fighting pacific Many shipped back Japan end war regardless born long Australia work help improve quality images scans internee records created Army part Australian War Memorials collection Incidentally briefed terms Geneva Convention treatment POW records considered type POW record sealed records could opened deemed embarassing POW stuff like medical treatment STD single mother giving birth things judged standards day paper records low quality pulp-ish paper almost rag paper scanned using HD digital camera setup faint grid pattern wire racks paper rested drying coming paper press came strong relief almost like moire pattern spent week playing plugging new algorithms tweaking structure parameters existing ones Perl ImageMagick best results Gaussian think cubic filters different sorts 10 years ago work extending existing Perl API expose internals filters start conditions radius control etc actually coding new filter types end able suppress wire grid pattern significantly allowing war researchers much easier time looking archived images work Maybe something http www imagemagick org Usage filter help sophisticated waifu2x learning algorithms another path thought asindu May 21 2015 1 06 lol didn even know project computer vision technology applied need write post explains fully Bambax May 21 2015 3 20 scanning won problem JPEG compression since choose work uncompressed images source problems outlined Scott post remain least specific point JPEG compression moot Also noise reduction applied tool seems standard aggressive type commercial solutions specialized types noise reduction amazing jobs could use waifu2x upscale little noise reduction process images tools get better final image Anonymous May 21 2015 3 56 pm looks quite similar warpsharp filter fad anime video encoding couple years ago sorted proponents lay peacefully dark waters bay may suitable described task lost features thin lines bended brows changes artistic style smudged textures flattened volume vectorized lines may misguide enrage professional Greg May 21 2015 5 31 pm blog post Flipboard Engineering blog lot information using neural networks upscaling images http engineering flipboard com 2015 05 scaling-convnets Matt Morgan May 22 2015 6 30 pm John seen doubled images compare originals Probably wouldn hard comparison using images higher-res versions e scale back Waifu2x compare originals Leave Comment Name required E-mail required never displayed URI Note Wrap code blocks code code replace lt gt respectively Secrets JS Ninja Secret techniques top JavaScript programmers Published Manning Ukiyo-e org Japanese woodblock print database search engine Subscribe email updates jeresig Infrequent short updates links JavaScript Jobs VPS Hosting Provided Media Temple'),
('Data science makes an impact on Wall Street', "Menu Home Shop Video Training Books Radar Safari Books Online Conferences Courses Certificates oreilly com O'Reilly Radar RSS Feed Twitter Facebook Google Youtube Home Shop Video Training Books Radar Radar Animals Safari Books Online Conferences Courses Certificates Data Topics DataDesignEmerging TechIoTProgrammingWeb Ops PerformanceWeb Platform Help us test new look O Reilly Visit beta site Print Listen Data science makes impact Wall Street O'Reilly Data Show Podcast Gary Kazantsev big data data science making difference finance Ben Lorica bigdata Ben Lorica Comments 2 May 21 2015 Comments 2 started career industry working problems finance ve always appreciated challenging build consistently profitable systems extremely competitive domain served quant hedge fund late 1990s early 2000s worked primarily price data time-series quickly found difficult find sustain profitable trading strategies leveraged data sources everyone else industry examined exhaustively early-to-mid 2000s hedge fund industry began incorporating many data sources today re likely find many finance industry professionals big data data science events like Strata Hadoop World latest episode O Reilly Data Show Podcast great conversation one leading data scientists finance Gary Kazantsev runs R D Machine Learning group Bloomberg LP former quant wanted know types problems Kazantsev group work tools techniques ve found useful also talked data science data engineering recruiting data professionals Wall Street Text mining finance One thing back day work unstructured text Kazantsev group considered one leading text mining outfits finance done many presentations topic last years One things remains unchanged end day finance tools impact users make decisions trades Kazantsev described projects Text analysis one number things essentially amount producing financial indicators unstructured text take news stories produce time series Sentiment analysis one product general market impact indicators also area topic clustering topic classification novelty detection projects work area view news actually broad Yes publicly available news news also generate enormous amount content take number different third-party contributor feeds collect information social media Roughly speaking point ingest something like 1 2 million stories per day give take actually long pipeline stories go ranges language detection named entity recognition disambiguation topic classification complicated things like sentiment analysis think sentiment analysis machine learning perspective fairly standard text classification problem lot work area starting original papers important part really case necessarily set techniques work best pose problem actually useful finance professionals far techniques re interested large margin methods support vector machines still come top things least experience domain actual challenge asking right question furthermore enough feature engineering also enough statistics convince clients produced actually makes sense impacts financial markets way Pricing financial products price financial product illiquid doesn trade often particular suppose pricing isn set regular basis much price history work worked problems nature context derivatives back quant appreciate difficult fact proper pricing risk assessment complex mortgage derivatives heart financial crisis 2008 Kazantsev describes currently approach problem instruments illiquid trade infrequently fairly nontrivial problem value appropriately lot work fact whole industry Wall Street dedicated actually pricing one way another connect number different estimates prices securities combine using ensemble model get consensus recommended value use consensus value matter good actually trade one instruments inevitably going discrepancies traded price consensus price variables clearly captured consensus looks basically like machine learning problem variables describe instrument variables describe trade consensus value actual traded value Build model explain discrepancies Unlike individual contributors marketplace see basically trades tend better basis inference Recruiting data professionals Among Wall Street firms ve interacted Bloomberg ranks active evaluating new technologies recruiting data scientists data engineers experienced using latest technologies asked Kazantsev still actively recruits PhDs Math Physics Computer Science quantitative disciplines answer yes also explained beginning help shape data science curriculums several institutions recruit graduates many programs Physicists already tend make fairly good software engineers especially people Monte Carlo simulations particle physics work lot data field teaches certain empiricist attitude understand Generally speaking recruit look people empiricist attitude toward data look people certain amount mathematical intuition certain amount curiosity definitely Preferably people worked data problems necessarily big data problems medium data problems even small data problems using kinds methods enough use methods question say regression classification look people tend try understand things work work assumptions make one side side software engineering group structured somewhat differently many data science groups elsewhere deliver products clients everything Even project posed group taken blank sheet paper delivery clients involves delivering actual products features Bloomberg users Subscribe O Reilly Data Show Podcast TuneIn iTunes SoundCloud RSS listen entire interview SoundCloud player subscribe SoundCloud TuneIn iTunes Cropped image article category pages Sam valadi Flickr used Creative Commons license tags Big Data data science data scientists datashow finance Next Money O'Reilly Data Show Podcast Comments 2 Get O Reilly Data Newsletter Stay informed Receive weekly insight industry insiders Patrick Great article Ben appreciate insights Would interested tools technologies groups utilizing mentioned Strata Hadoop beginning lots products platforms also new data science centric platforms created like Lumidatum Kevin Ben Great article see significant innovation similar areas sentiment analysis potentially anticipate crashes type experienced 2008 using Big Data analytics value companies private equity sphere create new professional opportunities Data Scientists finance KevinPetrieTech https www linkedin com pulse big-data-reshaping-finance-private-equity-kevin-petrie trk prof-post https www linkedin com pulse three-reasons-data-scientists-might-prevent-next-market-kevin-petrie trk mp-reader-card Featured Video Data science going - DJ Patil U government's first Chief Data Scientist looks future data science Strata Hadoop World 2015 San Jose Get Data Newsletter Stay informed Receive weekly insight industry insiders Featured Download Download free report free reports Recent Posts Real-world interfaces awkward playful stage finance disrupted Validating data models Kafka-based pipelines Four short links 28 May 2015 Protecting health open data management principles Recently Discussed Archives Archives Month May 2015 April 2015 March 2015 February 2015 January 2015 December 2014 November 2014 October 2014 September 2014 August 2014 July 2014 June 2014 May 2014 April 2014 March 2014 February 2014 January 2014 December 2013 November 2013 October 2013 September 2013 August 2013 July 2013 June 2013 May 2013 April 2013 March 2013 February 2013 January 2013 December 2012 November 2012 October 2012 September 2012 August 2012 July 2012 June 2012 May 2012 April 2012 March 2012 February 2012 January 2012 December 2011 November 2011 October 2011 September 2011 August 2011 July 2011 June 2011 May 2011 April 2011 March 2011 February 2011 January 2011 December 2010 November 2010 October 2010 September 2010 August 2010 July 2010 June 2010 May 2010 April 2010 March 2010 February 2010 January 2010 December 2009 November 2009 October 2009 September 2009 August 2009 July 2009 June 2009 May 2009 April 2009 March 2009 February 2009 January 2009 December 2008 November 2008 October 2008 September 2008 August 2008 July 2008 June 2008 May 2008 April 2008 March 2008 February 2008 January 2008 December 2007 November 2007 October 2007 September 2007 August 2007 July 2007 June 2007 May 2007 April 2007 March 2007 February 2007 January 2007 December 2006 November 2006 October 2006 September 2006 August 2006 July 2006 June 2006 May 2006 April 2006 March 2006 February 2006 January 2006 December 2005 November 2005 October 2005 September 2005 August 2005 July 2005 June 2005 May 2005 April 2005 March 2005 Archives Topic Data Design Emerging Tech IoT Programming Web Ops Performance Web Platform Archives Author Sinan Unur Aaron Sumner Adam DuVander Adam Flaherty Adam Messinger Adam Witwer Adrian Mendoza Alasdair Allan Alex Bordei Alex Bowyer Alex Iskold Alexander Macgillivray Alice Zheng Alistair Croll Allen Downey Allen Noren Allison Randal Ally MacDonald Alois Reitbauer Alysa Hutnik Amelia Bellamy-Royds Amr Awadallah Amy Heineike Amy Jollymore Amy Unruh Anant Jhingran Andreas Antonopoulos Andrew Collette Andrew Odewahn Andrew Savikas Andrew Shafer Andrew Baker Andy Fitzgerald Andy Kirk Andy Konwinski Andy Oram Angela Rufino Ann Spencer Ann Waldo Anna Smith Anne Gentle Anni Ylagan Ari Gesher Aria Haghighi Ariya Hidayat Arnold Robbins Artur Bergman Arun Gupta Audrey Watters Avi Bryant Barb Edson Barbara Bermes Baron Schwartz Barry Devlin Barry O'Reilly Beau Cronin Ben Christensen Ben Evans Ben Henick Ben Lorica Benjamin Hindman Bill Higgins Bill Lubanovic Bill McCoy Bonnie Feldman Bradley Voytek Brady Forrest Brandon Satrom Brett McLaughlin Brett Sandusky Brett Sheppard Brian Ahier Brian Anderson Brian Boyer brian d foy Brian d'Alessandro Brian Foster Brian Jepson Brian Kardell Brian MacDonald Brian O'Leary Brian Sawyer Brigitte Piniewski Bruce Stewart Carin Meier Carl Hewitt Carl Malamud Cathy O'Neil Chao Ray Feng Chiu-ki Chan Chris Cornutt Chris Meade Chris Vander Mey Chris Wiggins Christine Perey Ciara Byrne Cliff Miller Colt McAnlis Cornelia L vy-Bencheton Cory Doctorow Courtney Nash Dale Dougherty Dan Saffer Danese Cooper Darren Barefoot Dave Himrod Dave McClure Dave Zwieback David Beyer David Cranor David Elfi David Leinweber David Recordon David Sims David Stephenson DC Denison Deni Auclair Derek Jacoby Dinesh Subhraveti Dino Esposito DJ Patil Doug Finke Doug Hill Dr Venkat Subramaniam Drew Dara-Abrams Duncan Ross Dusty Phillips DW Wheeler Dylan Field E Vander Veer Edd Dumbill Edie Freedman Eli Goodman Elisabeth Robson Elizabeth Corcoran Ellen Friedman Elliott Hauser Elliotte Rusty Harold Emma Jane Westby Eoin Purcell Eric Redmond Eric Ries Ezra Haber Glenn Faye Williams Federico Castanedo Federico Lucifredi Fred Trotter Fred van den Bosch Gabe Zichermann Gavin Starks George Reese Gilad Rosner Glen Martin Greg Whisenant Gretchen Giles Gustavo Franco Gwen Shapira Hadley Wickham Hari K Gottipati Heather McCormack Helen Papagiannis Hew Wolff Howard Wen Hugh McGuire Ilya Grigorik Imran Ali J Paul Reed James Bridle James Turner Janaya Williams Jane Sarasohn-Kahn Jason Grigsby Jason Strimpel Jay Kreps Jay McGavren Jayant Shekar Jeevan Padiyar Jeff Gothelf Jeff Needham Jeffrey Carr Jeffrey Carr Jenn Webb Jennifer Pahlka Jeremy Freeman Jeremy Howard Jesper Andersen Jesse Anderson Jesse Robbins Jessica McKellar Jesus M Gonzalez-Barahona Jez Humble Jim Scott Jim Stogdill Jimmy Guterman Jo Prichard Joanne Molesky Jodee Rich Joe Procopio Johan Bergstr m John Adams John Allspaw John Battelle John Boxall John Feland John Foreman John Geraci John Graham-Cumming John King John Labovitz John Lindquist John Myles White John Piekos John Russell John Warren John Wilbanks Jon Bruner Jon Callas Jon Cowie Jon Roberts Jon Spinney Jon Udell Jonas Luster Jonathan Alexander Jonathan Reichental Ph D Jonathon Thurman Jono Bacon Joseph Hellerstein Joseph J Esposito Josh Lockhart Josh Simmons Joshua-Mich le Ross Joy Beatty Jud Valeski Julie Steele Justin Dombrowski Justin Hall Justo Hidalgo Karl Fogel Kassia Krozser Kat Meyer Kate Eltham Kate Pullinger Kathryn Barrett Kathy Sierra Kathy Walrath Katie Cunningham Katie Miller Keith Comito Keith Fahlgren Ken Yarmosh Kevin Shockey Kevin Sitto Kevin Smokler Khaled El Emam Kieren James-Lubin Kipp Bradford Kit Seeborg Kiyoto Tamura kmatsudaira Kurt Cagle Lara Swanson Laura Dawson Laura Klein Laurel Ruma Laurie Petrycki Leigh Dodds Liliana Bounegru Linda Stone Lisa Mann Liza Daly Lorna Jane Mitchell Lorne Lantz Luciano Ramalho Lucy Gray Lukas Biewald Mac Slocum Madhusudhan Konda Mandi Walls Manish Lachwani Marc Goodman Marc Hedlund Marie Beaugureau Marie Bjerede Mark Drapeau Mark Grover Mark Jeftovic Mark Lutz Mark Nelson Mark Pacelle Mark Sigal Marko Gargenta Martin Kalin Martin Kleppmann Mary Treseler Matt Garrish Matt Makai Matt Neuburg Matt Wood Matthew Burton Matthew Gast Matthew McCullough Matthew Russell Matthew Russell Max Kanat-Alexander Max Meyers Max Shron Meghan Athavale Meghan Blanchette Mehdi Daoudi Michael DeHaan Michael Driscoll Michael Ferrari Michael Freeman Michael Gold Michael Hunger Michael Jon Jensen Michael Lopp Michael McMillan Michael Scroggins Mike Amundsen Mike Barlow Mike Hendrickson Mike Honda Mike Loukides Mike Petrovich Mike Shatzkin Mitchell Hashimoto Naomi Robbins Nat Torkington Nate Osit Nathan Jepson Neal Ford Nicholas Tollervey Nick Bilton Nick Farina Nick Kolegraff Nick Lombardi Nick Ruffilo Nicolas Garcia Belmonte Nikolaj Nyholm O'Reilly Radar O'Reilly Strata Ohad Samet Osman Rashid Pablo Francisco Arrieta Gomez Paco Nathan Pamela Samuelson Paris Buttfield-Addison Patrick Mulder Patrick Reynolds Paul Kedrofsky Paul Spinrad Pete Hodgson Pete Warden Peter Arijs Peter Bennett Peter Cooper Peter Krautzberger Peter Laflin Peter Lewis Peter Meyers Philip Guo Philipp Janert Q Ethan McCallum Quinn Norton Rachel Roumeliotis Rael Dornfest Raffael Marty Rajat Bhargava Ramez Naam Randy Bias Raven Zachary Ray DiGiacomo Jr Renee DiResta Reynold Xin Richard Cook Richard Dallaway Richard Reese Richard Warburton Rob Tucker Robbie Allen Robert Kaye Robert Passarella Roberta Cairney Roger Chen Roger Magoulas Rogier DocWilco Mulhuijzen Ron Miller Roseanne Fallin Rune Madsen Russell J Dyer Ryan Bethencourt Ryan Neufeld Ryan Stewart Sam Newman Samuel Mullen Sanders Kleinfeld Sara Peyton Sara Winge Sarah Milstein Sarah Novotny Scott Jenson Scott Murray Scott Rich Scott Ruthfield Sean McGregor Sean O Sullivan bastien Pierre Semmy Purewal Seth Ladd Shahid Shah Shahin Farshchi Shai Almog Shannon Cutt Shyam Seshadri Silona Bonewald Simon Chan Simon Phipps Simon St Laurent Simon Wardley Spencer Critchley Stefan Thies Stephen Elston Stephen O'Grady Stephen O'Grady Steve Souders Steven Citron-Pousty Steven Shorrock Stoyan Stefanov Suzanne Axtell Tara Hunt Terrence Dorsey Terry Jones Tim Anderson Tim Busbice Tim Darling Tim O'Reilly Timothy M O'Brien Timothy McGovern Tish Shute Toby Inkster Todd Sattersten Tom Eisenmann Tom Steinberg Tony Quartarolo Trisha Gee Troy Topnik Tyler Bell Valeri Karpov Vandad Nahvandipoor Vanessa Fox Varun Nagaraj Cukierski William Mougayar William O'Connor Zigurd Mednieks CONTACT US Radar managing editor Jenn Webb Sign today receive special discounts product alerts news O'Reilly Privacy Policy View Sample Newsletter Twitter YouTube Slideshare Facebook Google RSS View RSS Feeds 2015 O'Reilly Media Inc 707 827-7019 800 889-8969 trademarks registered trademarks appearing oreilly com property respective owners O'Reilly O Reilly Radar Radar Contributors Academic Solutions Jobs Contacts Corporate Information Press Room Privacy Policy Terms Service Writing O Reilly Editorial Independence Community Authors Community Featured Users Forums Membership Newsletters O Reilly Answers RSS Feeds O Reilly Chimera beta Partner Sites makezine com makerfaire com craftzine com igniteshow com PayPal Developer Zone O Reilly Insights Forbes com Shop O'Reilly Customer Service Contact Us Shipping Information Ordering Payment Affiliate Program O Reilly Guarantee close Get O Reilly Data Newsletter Stay informed Receive weekly insight industry insiders"),
("NPR's Consider The Following on the 'Ellie' program used to screen for PTSD", "Skip Main Content Search Toggle search Stations Donate Shop Sign Register Logout News Arts Life Music Topics News U World Politics Business Technology Science Health Race Culture Education Arts Life Books Movies Pop Culture Food Art Design Performing Arts Photography Music First Listen Songs Love Music Articles Tiny Desk Concerts Videos Programs News Conversations Morning Edition Things Considered Fresh Air Diane Rehm Show Latino USA Media Point Weekend Edition Saturday Weekend Edition Sunday Youth Radio Storytelling Humor Ask Another Best Car Talk Bullseye Invisibilia Radiolab Snap Judgment StoryCorps TED Radio Hour Wait Wait Don't Tell Music Songs Considered Alt Latino First Listen Top Jazz Night America Metropolis Microphone Check Mountain Stage Piano Jazz Song Travels Thistle Shamrock World Cafe Listen Planet Money Economy Explained Podcast Radio American Life Twitter Facebook Radio Machine Learned Spot Depression May 20 2015 4 37 PM ET Stacey Vanek Smith Listen Story Things Considered 4 37 Playlist Download Embed Embed Close embed modal iframe src http www npr org player embed 407978049 408293135 width 100 height 290 frameborder 0 scrolling Transcript I'm booth computer program called Ellie She's screen front Ellie designed diagnose post-traumatic stress disorder depression get booth starts asking questions family feelings biggest regrets Emotions seem really messy hard machine understand Skip Rizzo psychologist helped design Ellie thought otherwise answer Ellie's questions listens doesn't process words I'm saying analyzes tone camera tracks every detail facial expressions Credit Institute Creative Technologies Contrary popular belief depressed people smile many times non-depressed people Rizzo says smiles less robust less duration It's almost like polite smiles rather real robust coming inner-soul type smile Ellie compares smile database soldiers returned combat smile genuine forced Ellie also listens pauses watches see whether look side lean forward notices analysis seems work studies Ellie could detect signs PTSD depression well large pool psychologists Jody Mitic served Canadian forces Afghanistan lost feet bomb Mitic remembers Ellie's robot-ness helped open Ellie seemed listening Mitic says lot therapists see eyes start talking grislier details stuff might seen done reaction Ellie says didn't problem Right Ellie strictly diagnosis idea Ellie's field she'll find soldiers problem human take Share Facebook Twitter Google Email Comment NPR Reporter Raced Machine Write News Story Won May 20 2015 Episode 625 Last JobMay 20 2015 Planet Money Planet Money Episode 627 Miracle Apple Technology Watch Robots Transform California Hospital Planet Money Episode 626 End Planet Money Job Done Machine Comments nbsp must signed leave comment Sign Register Please keep community civil comments must follow NPR org Community rules terms use moderated prior posting NPR reserves right use comments receive whole part use commenter's name location medium See also Terms Use Privacy Policy Community FAQ Please enable JavaScript view comments powered Disqus Planet Money Economy Explained Planet Money Economy Explained Imagine could call friend say Meet bar tell what's going economy imagine that's actually fun evening That's we're going Planet Money Want know Check page Want connect Planet Money team Send us email Subscribe Podcast RSS planetmoney Facebook Planet Money App Pocketful Planet Money Get app NPR thanks sponsorsBecome NPR sponsor Search Home News Arts Life Music Topics Programs Donate Stations NPR Shop Sign Register Logout Back News U World Politics Business Technology Science Health Race Culture Education Arts Life Movies Pop Culture Food Art Design Performing Arts Photography Books Book Reviews Author Interviews Music First Listen Songs Love Music Articles Tiny Desk Concerts Videos Blogs Corrections NPR Back news conversations storytelling humor music also heard npr stations Back Morning Edition Things Considered Fresh Air Diane Rehm Show Latino USA Media Point Science Friday Weekend Edition Saturday Weekend Edition Sunday Youth Radio Back Ask Another Best Car Talk Bullseye Invisibilia Radiolab Snap Judgment StoryCorps TED Radio Hour Wait Wait Don't Tell Back Songs Considered Alt Latino First Listen Top Jazz Night America Metropolis Microphone Check Mountain Stage Piano Jazz Song Travels Thistle Shamrock World Cafe Back Prairie Home Companion APM Marketplace APM Science Friday American Life PRX Public Radio Programs Z Support comes Support NPR NPR Shop Support Programs Love News U World Politics Business Technology Science Health Race Culture Education Arts Life Books Movies Pop Culture Food Art Design Performing Arts Photography Music First Listen Songs Love Music Articles Tiny Desk Concerts Videos Programs Things Considered Ask Another Best Car Talk Bullseye Diane Rehm Show Fresh Air Invisibilia Latino USA Morning Edition Media Point Radiolab Snap Judgment StoryCorps TED Radio Hour Wait Wait Don't Tell Weekend Edition Saturday Weekend Edition Sunday Youth Radio Listen Hourly News NPR Program Stream Find Station Streams Podcasts Ways Listen NPR Overview NPR Finances People NPR Support NPR Visit NPR Press Room 'This NPR' Blog Careers NPR Corrections NPR Ombudsman Permissions NPR Help Contact Terms Use Privacy Sponsorship Choices Text-Only NPR"),
('$10 hedge fund supercomputer sweeps Wall Street Big Data with AI, ML & power from the cloud (X-Post r/bigdata)', "Skip navigation Skip content Help using website - Accessibility statement JavaScript disabled Please enable JavaScript use News Clippings Comments user settings News Clippings Comments Benefits Today's Paper Subscribe Log Register Personalise news save articles read later customise settings trouble accessing login form go login page trouble accessing login form go login page Age Business News Markets Overview Indices Market Movers Sectors Currencies Quotes Portfolio Budget 2015 Money Property Focus Buy Lease Find Agents Small Business Startup Growing Managing Franchising Trends Entrepreneur Marketing Finance Technology Resources Executive Style Compare Save Credit Cards Debit Cards Home Loans Personal Loans Car Loans Term Deposits Bank Accounts Savings Accounts Events Home Business Markets Search age Search Business theage com au Web Business 10 hedge fund supercomputer sweeps Wall Street power cloud Date May 21 2015 29 Read later Tweet Pin submit reddit Email article Print wave math computer whizzes making presence felt Wall Street Photo Richard Drew sounds like mother spreadsheets 1 million rows 1 million columns - 1 trillion entries Fortunately Braxton McKee isn't using Excel Instead he's tapping cloud crunch market data cheap software built learns goes cost cosmic power US10 16 AI companies got initial backing venture capitalists 2014 two 2010 Photo Bloomberg Welcome brave new world cheap-and-cheerful artificial intelligence McKee 35 part wave math computer whizzes that's pushing data science new heights across Wall Street What's remarkable efforts isn't AI science fiction suddenly becoming AI science fact sorry Steven Spielberg It's something mundane thanks cloud computing mind-blowing data analysis getting cheap many businesses easily afford Sophisticated hedge funds like Renaissance Technologies tech giants like Google deploying AI subset machine learning years data shops like Ufora McKee founded 2011 leveraging cloud power help hedge funds financial players run complex big-data computer models results startling Braxton McKee founder Ufora Five years ago sort programming involved McKee's 1-trillion-point dense matrix would taken months coding US1 million-plus hardware McKee simply logs onto Amazon Web Services name price computing capacity sets code loose loft Flatiron District Manhattan works calls coffee time goal make every model - matter much data involved - compute time takes putter office kitchen brew Nespresso Caramelito walk back desk Machine learningMcKee previously programmer Ellington Management Group credit hedge fund backed venture division Two Sigma Investments big quantitative firm run former artificial intelligence academic mathematics olympian Quant shops like Two Sigma Renaissance hiring machine-learning experts Ray Dalio's Bridgewater Associates Steven Cohen's Point72 Asset Management also building big-data analytics much action start-ups Like McKee three Bridgewater veterans Matthew Granade Chris Yang Nick Elprin recently struck San Francisco-based firm Domino Data Lab gives data scientists way review old work collaborate professional colleagues important feature iterative process programming Reading comprehensionAnother young firm Palo Alto-based Sensai helps companies analyse what's known unstructured data corporate documents transcripts social media involves natural language processing basically reading comprehension machines rise cloud computing making much cheaper faster It's also helping spawn new AI industry 16 AI companies got initial backing venture capitalists 2014 two 2010 according data compiled researcher CB Insights Bloomberg amount invested startups - describe machine learning deep learning - soared US309 2 million last year 20-fold US14 9 million 2010 giants like Google drove initial push big- data analysis technological innovation democratising access Wall Street said Granade Domino Data Lab Automotive manufacturing US government pharmaceutical firms - we're seeing sophisticated analytical need across board said Granade formerly co-head research Bridgewater Hedge funds seem think cutting edge level rest world moving fast well Tweet Pin submit reddit Email article Print popular 1 twists come Gina Rinehart saga Contains 2 Gina Rinehart loses control 5b family trust Contains 3 News Corp shuts mX newspaper Contains total comments56 4 High-end retailer Myer axes 80 head office Contains total comments2 5 Ben Bernanke says Australia would 'have Contains Searched Shares TLS - TELSTRA CORPORATION LIMITED PPP - PAN PACIFIC PETROLEUM NL POS - POSEIDON NICKEL LIMITED NUF - NUFARM LIMITED IFL - IOOF HOLDINGS LIMITED Get latest quotes charts Enter company name ASX code BusinessDay - businessday com au Tweets BusinessDay Featured advertisers Special offers View special deals Mozo Executive Style Travel Motors Culture Luxury Management Fitness Black future luxury watches Bani McSpedden handsome batch black-bodied timepieces time trend Contains article contains photo gallery Advertisement Real Estate Inside Paul McCartney's New York mega penthouse Domain com au Buy real estate Johnny Depp's luxury home centre dogs fiasco Investor Hints Tips Video Australia stops without tugboats Smh com au Entertainment Red Carpet Latest Sports videos little piggy buys home Real Estate Inside Paul McCartney's New York mega penthouse Domain com au Buy real estate Johnny Depp's luxury home centre dogs fiasco Investor Hints Tips Events Sun-Herald City2Surf presented Westpac Smh com au Sign city near Join social network Discover ROL series Relationships Australia's Favourite Dating Site Rsvp com au Meet Sydney singles Get started free Singles events ages Compare Save Skip Best Deals Mobile Broadband Home Loans Credit Cards Low Rate Cards Rewards Cards Savings Accts Money Deals Loans Check today's best deals UBank UHomeLoan Discover low variable rate upfront monthly fees Info 20 000 Bonus Points Plus 0 p balance transfers 15 months Check Annual Fee Ever Plus earn flybuys points everyday spend Find Best PrePaid Round best prepaid plans available PrePaid Deals Mobile Plans Lock phone payment upfront save PrePaid Deals Readers' viewed viewed articles Brisbane TimesTop 5 Business articles Gina Rinehart loses control 5b family trust twists come Gina Rinehart saga Supabarn's fruit juices claim 'Made Griffith' really China says ACCC Competition heats Telstra Optus Vodafone Ben Bernanke says Australia would 'have respond' dollar threat viewed articles WA TodayTop 5 Business articles Gina Rinehart loses control 5b family trust Iron ore miners 'very tough time' twists come Gina Rinehart saga Competition heats Telstra Optus Vodafone Ben Bernanke says Australia would 'have respond' dollar threat viewed articles Sydney Morning HeraldTop 5 Business articles Apple co-founder Steve Wozniak warns coming 'internet things' bubble twists come Gina Rinehart saga Ben Bernanke says Australia would 'have respond' dollar threat Gina Rinehart loses control 5b family trust News Corp shuts mX newspaper viewed articles Canberra TimesTop 5 Business articles twists come Gina Rinehart saga Ben Bernanke says Australia would 'have respond' dollar threat Ikea charges 1107 1 99 kitchen accessory Apple co-founder Steve Wozniak warns coming 'internet things' bubble Tool compares income shows Australians touch viewed articles AgeTop 5 Business articles twists come Gina Rinehart saga Ben Bernanke says Australia would 'have respond' dollar threat Gina Rinehart loses control 5b family trust News Corp shuts mX newspaper Competition heats Telstra Optus Vodafone Age Home Victoria National Environment Opinion Business Technology Digital Life Entertainment Life Style Travel Cars Exec Style Sport Weather Age Sitemap Us Contact Us Support Advertise Us Today Last 8 days Text Version Site Accessibility Guide Connect Mobile Site Age iPad Digital Edition RSS Facebook Twitter LinkedIn Products Services Subscribe Manage Subscription Benefits Good Food Guide Age Shop Newsletters Cracka Wines Classifieds Place Ad Accommodation Cars Dating Jobs Property Price Data Real Estate Buy Sell Tributes Celebrations Fairfax Media Member Centre Conditions Use Privacy Policy News Store Archive Photo Sales Purchase Front Pages Fairfax Syndication Fairfax Events Fairfax Careers Press Council Fairfax Media Sites Sydney Morning Herald Age MyCareer Domain Drive RSVP Essential Baby InvestSMART APM Stayz Weatherzone TheVine Partners Buy Sell Cruises Lawyers Credit Cards Business Mechanics Electricians Fitness Hair Salons Weddings Home Builder Cracker Copyright 2015 Fairfax Media 3 50AM Friday May 29 2015 2467 0 online know story contact us Close Provide detailed information including contact details relevant Real Estate Cars Jobs Dating Newsletters Fairfax Media NetworkMore Close News Weather smh com au theage com au brisbanetimes com au canberratimes com au nationaltimes com au watoday com au weatherzone com au Business Finance businessday com au brw com au afr com afrmagazine com au smartinvestor com au Lifestyle life style dailylife com au essentialbaby com au essentialkids com au findababysitter com au goodfood com au Classifieds domain com au drive com au adzuna com au rsvp com au Advertise us Newsletters Full list sites Fairfax Media Privacy Terms Conditions"),
('Part I of the Deep Learning textbook is now complete - Version 19/05/2015', 'DEEP LEARNING MIT Press book preparation Yoshua Bengio Ian Goodfellow Aaron Courville Citing book preparation want cite book preparation please use following bibtex entry unpublished Bengio-et-al-2015-Book title Deep Learning author Yoshua Bengio Ian J Goodfellow Aaron Courville note Book preparation MIT Press url http www iro umontreal ca bengioy dlbook year 2015 Draft chapters available feedback - Version 19 05 2015 Please help us make great book draft still full typos improved many ways suggestions welcome PLEASE SPECIFY VERSION date COMMENTING hesitate contact authors directly e-mail Google messages Yoshua Ian Aaron NOTE SEVERAL PEOPLE REPORTED PRINTING WORKS BETTER CHROME BROWSER Warning conversion pdf html perfect seen math like subscripts Sorry Note book structure re-organized three parts Applied math machine learning basics skipped people appropriate background Modern practical deep networks used industry currently mostly supervised learning Deep learning research looking forward mostly unsupervised learning chapters moved around split accordingly Table Contents Deep Learning AI Part header applied math machine learning basics Linear Algebra Probability Information Theory Numerical Computation Machine Learning Basics Part header modern practical deep networks Feedforward Deep Networks Regularization Numerical Optimization Convolutional Networks Sequence Modeling Recurrent Recursive Nets Part header deep learning research Structured Probabilistic Models Deep Learning Perspective Monte-Carlo Methods Linear Factor Models Auto-Encoders Representation Learning Manifold Perspective Representation Learning Confronting Partition Function Approximate Inference Deep Generative Models References Older versions Version 30-3-2015 Version 1-1-2015 Version 5-12-2014 Version 22-10-2014'),
('An introduction to Random Indexing word space models [PDF]', 'PDF-1 4 40 0 obj endobj xref 40 16 0000000016 00000 n 0000000910 00000 n 0000000616 00000 n 0000000990 00000 n 0000001119 00000 n 0000001251 00000 n 0000002017 00000 n 0000002403 00000 n 0000002437 00000 n 0000002700 00000 n 0000002776 00000 n 0000003008 00000 n 0000005392 00000 n 0000006036 00000 n 0000006291 00000 n 0000008960 00000 n trailer startxref 0 EOF 42 0 obj'),
('[1505.04771] DopeLearning: A Computational Approach to Rap Lyrics Generation', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1505 04771 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context cs LG prev next new recent 1505Change browse cs cs AI cs CL cs NE References CitationsNASA ADS 1 blog link Bookmark Computer Science Learning Title DopeLearning Computational Approach Rap Lyrics Generation Authors Eric Malmi Pyry Takala Hannu Toivonen Tapani Raiko Aristides Gionis Submitted 18 May 2015 Abstract Writing rap lyrics requires creativity construct meaningful interesting story lyrical skills produce complex rhyme patterns cornerstone good flow present method capturing aspects approach based two machine-learning techniques RankSVM algorithm deep neural network model novel structure problem distinguishing real next line randomly selected one achieve 82 accuracy employ resulting prediction method creating new rap lyrics combining lines existing songs terms quantitative rhyme density produced lyrics outperform best human rappers 21 results highlight benefit rhyme density metric innovative predictor next lines Comments 11 pages Subjects Learning cs LG Artificial Intelligence cs AI Computation Language cs CL Neural Evolutionary Computing cs NE ACM classes 2 7 H 3 3 Cite arXiv 1505 04771 cs LG arXiv 1505 04771v1 cs LG version Submission history Eric Malmi view email v1 Mon 18 May 2015 19 35 21 GMT 234kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('BigML: Machine Learning made easy', "CloudAcademy BlogCloud AcademyHow worksPlans PricingVideo CoursesAWS CertificationsCommunityFAQs Cloud Academy Blog BigML Machine Learning made easyBigML Machine Learning made easyMay 20 2015 Alex CasalboniBigML offers managed platform build share datasets modelsMachine Learning Service MLaaS become real thing Cloud market BigML mission simple clear making Machine Learning easy beautiful understandable everybody would say BigML offers something much closer Software Service SaaS IaaS PaaS competitors recently played AmazonML AzureML Google Prediction API part rich ecosystems web services Cloud storage CDNs VPCs deployment automation much hand BigML somehow remaining platform-agnostic successfully managed exploit existing Cloud solutions advantage Consider example allows data imports AWS S3 MS Azure Google Storage Google Drive Dropbox etc detail seems trivial first might game changer long-term public Cloud infrastructures become commodity cross-provider solution best option BigML FeaturesBeing focused Machine Learning BigML offers wider set features well integrated within usable Web UI would expect load dataset train evaluate models generate new predictions either one one batch list additional useful features haven see elsewhere Plenty ways load raw data including Cloud storage systems public URLs CSV ARFF files vast gallery free datasets models play well organized categories publicly accessible Clustering algorithms visualization data analysis visualization tools essential come high-quality model Anomaly detection dealing outliers pain detecting pattern anomalies save time money even hitting model Flexible pricing choose subscription plans starting 15 mo students pay go BigML credits even buy personal VPC import dataDepending use scenario may want import data existing Cloud storage system provide public URL directly upload CSV file Development mode even create inline source fly Even step BigML offers nice set features CSV parsing configuration Fields type selection Strings locale selection English Dutch Spanish Portuguese Headers parsing CSV without header row Date-time fields expansion Text analysis language detection tokenization stop words stemming Interestingly update Source configuration time without additional upload soon Source ready correctly parsed use generate new Dataset Alternatively import ready-to-use data public datasets gallery Datasets fully reusable expandable exportableBigML Datasets easy reuse edit expand export Indeed easily rename add descriptions one fields add new ones normalization discretization mathematical operations missing values replacement etc generate subsets based sampling custom filters Furthermore even training model given values distribution statistics field also great Dynamic Scatterplot tool visualize data two dimensions time may want explore features space look patterns export chars simply fun practice Datasets starting point operations Let assume goal training evaluating classification model first need split dataset smaller training test sets achieve Training Test set Split operation course free select allocate records 80 20 default split logic process actually create two new independent datasets analyze manipulate want soon split completed ll want select new training set launch Configure Model operation BigML decision treesA Machine Learning model could potentially anything able analyze raw data eventually labelled somehow learn deal new unseen data Decision Trees probably intuitive type model build easy visualize understand easier store since almost directly converted procedural code Even programmer think nested structure binary decisions exactly every BigML model train model starting training Dataset asked select objective field e target column order reduce effect data overfitting size model normally need sort statistical pruning although decide disable Optionally configure options Missing split whether include missing values choosing split disabled default Node threshold maximum number nodes 512 default Weights choose weight records specifying one weighting field assign relative weights classes Sampling Ordering choose custom sub-sample shuffling logic end training process able visualize model obtain informative report better understand fields relevant see screenshot Moreover model graphically represented actual tree left sunburst right point already start generating new predictions course want evaluate model accuracy first Ensembles improve prediction accuracyEnsembles involving multiple alternative models provide better predictive performance well documented way improve single-model systems accuracy model trained using subset data focused specific classes collaborate generating better prediction BigML easily train Decision Forests Configure Ensemble dataset operation simply asked many models trained approach drastically corrects decision trees habit overfitting training data therefore improves overall accuracy case managed improve accuracy 3 using ensemble 10 models might make sense afford additional time also generated ensemble 100 models even though increased accuracy additional 1 clearly great idea terms cost speed evaluate resultsBeing able quickly evaluate models compare multiple evaluations critical features Machine Learning Service product personally believe BigML done great job Typically want test models smaller part dataset previously created 20 test set used generate evaluation model ensemble either launch Evaluate operation model Evaluate Model operation dataset much configuration needed unless specific sampling ordering needs Evaluation object listed Evaluations list course based model type regression classification ll shown different kinds metrics model classifier ll shown Confusion Matrix including statistics like Accuracy Precision Recall F-Measure Phi case Confusion Matrix big rendered web page let say 6 possible classes able download Excel format look similar figure top legend shows single cell means respect first element main diagonal cell main diagonal ensemble perform better Apparently achieved 2 82 overall accuracy high 5 34 classes compare two evaluations Compare Evaluation operation shown statistics single evaluation plus relative percentage metric model alone pretty effective probably wouldn choose pay additional cost ensemble terms price speed although many cases overfitting kill predictive power ensembles drastically boost accuracy Generating new Predictions API bindings BigMLerI would say BigML user-friendly developer-friendly took time code plenty API clients even command-line tool called BigMLer course perform every operation mentioned via API believe offline phases better handled clear solid UI especially model dataset definition chose Python binding coded simple script generate new predictions Generate new Prediction Model Ensemble PythonPython bigml api import BigML bigml model import Model bigml ensemble import Ensemble USE_ENSEMBLE False labels '1' 'walking' '2' 'walking upstairs' '3' 'walking downstairs' '4' 'sitting' '5' 'standing' '6' 'laying' def main api BigML alex-1 YOUR_API_KEY storage cache USE_ENSEMBLE predictor Ensemble 'ensemble 5557c358200d5a7b4300001e' api api else predictor Model 'model 5557ac99200d5a7b42000001' api api generate new prediction Note params might differ btw Ensemble Model example prediction predictor predict get_input_data with_confidence True label prediction 0 confidence prediction 1 print currently class labels label label confidence def get_input_data Retrieve input data local CSV file open 'record csv' f record_str f readline generate 'fieldN' dict 562 un-named columns record val enumerate record_str split ' ' record 'field s' 2 val return record __name__ '__main__' main 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from bigml api import BigMLfrom bigml model import Modelfrom bigml ensemble import Ensemble USE_ENSEMBLE False labels '1' 'walking' '2' 'walking upstairs' '3' 'walking downstairs' '4' 'sitting' '5' 'standing' '6' 'laying' def main api BigML alex-1 YOUR_API_KEY storage cache USE_ENSEMBLE predictor Ensemble 'ensemble 5557c358200d5a7b4300001e' api api else predictor Model 'model 5557ac99200d5a7b42000001' api api generate new prediction Note params might differ btw Ensemble Model example prediction predictor predict get_input_data with_confidence True label prediction 0 confidence prediction 1 print currently class labels label label confidence def get_input_data Retrieve input data local CSV file open 'record csv' f record_str f readline generate 'fieldN' dict 562 un-named columns record val enumerate record_str split ' ' record 'field s' 2 val return record __name__ '__main__' main far performance concerned calls model took 1 5 2 seconds enabled local storage option store models parameters locally avoid blocking API calls every future prediction warmed local cache script execution time dropped 150ms 10 models ensemble took 20 seconds load first every next call took 1 second actually 10 times slower single model 10 predictions performed think 1 additional prediction confidence worth time Please also keep mind model worked 560 input features 6 possible output classes therefore m sure average model would run much faster mine Alternatively convert model procedural code fifteen different languages formats clicking Download Actionable Model gave Python version try really takes milliseconds execute locally might good solution case want install new libraries example think embedded devices network isolated clients definitely satisfied service developer greatly appreciate effort supporting many programming languages platforms making everyone work simpler Become Cloud expert Annual MembershipNew courses every week 100 May 31st Start 7 day Free Trial Alex CasalboniI Software Engineer great passion music web technologies I'm experienced web development software design particular focus frontend UX sound music engineering background allows deal multimedia signal processing machine learning AI lot interesting tools even powerful merge Cloud Posts - WebsiteFollow Latest E-booksSign Cloud Academy Start Freeor social sign Learn AWS Cloud Academy Labs Learn Cloud Computing Cloud Computing Fundamentals Become AWS Certified AWS Certifications Guide Amazon Web Services AWS Microsoft Azure Docker Open-Source TechSubscribe newsletterStay updated Cloud Computing news weekly learning material Follow Cloud Cloud Academy Blog BigML Machine Learning made easyAbout Sitemap Cloud FundamentalsCopyright 2013-2015 Cloud Academy rights reserved Share Facebook Twitter Google LinkedIn"),
("[poll] /r/machinelearning, what's your level of education?", "FoodTVMoviesSign UpLoginDiscovering BestJoin Rankables community discuss best things around Sign UpLogin r MachineLearning what's level education Showing - Show filtersShow guest answersShow answers fromboth males femalesmales onlyfemales onlyShow answers people live inShow answers peopleof ages13 17 years old18 24 years old25 34 years old35 44 years old45 54 years old55 64 years old65 years olderUpdateReset filtersFull ScreenAnswer Feed Showing - Show filtersShow guest answersShow answers fromboth males femalesmales onlyfemales onlyShow answers people live inShow answers peopleof ages13 17 years old18 24 years old25 34 years old35 44 years old45 54 years old55 64 years old65 years olderUpdateReset filtersShare QuestionAboutPrivacyTermsCreditsSitemap"),
('Would love some feedback on my blog from someone who knows about deep learning', 'HOME SERVICES US CONTACT US BLOG HOME SERVICES US CONTACT US BLOG Previous Next Future SEO Deep Learning Image Identification concept deep learning deep structured learning frequent topic conversation recent months due commitment advancements world largest prolific search companies organizations like Google Facebook Bing Baidu Chinese search engine buying technology starting see huge acceleration applications uses concept within different platforms focus particular piece targets deep learning relation image identification search engine optimization believe going next big factor future SEO marketers SEM folk alike let take look technologies affect marketers leverage new artificial intelligence practical useful way Technology Wolfram Image Identify WolframAlpha isn household name yet big player artificial intelligence industry last several years focus generating powerful knowledge engine goes beyond search uses models algorithms bring high level knowledge across broad range fields professions One new offerings called Image Identification Project like name implies function within Wolfram Language lets insert image return guess image along providing keywords object information relates item Although 100 accurate yet treasure trove keywords displayed shows potential images generate keyword data Google Research Google made many advancements comes image search past years working building deeper understanding images research acquisitions think DeepMind acquired 2014 partnerships like one Imagenet Large Scale Visual Recognition Challenge recently google announced ability automatically produce captions describe images first time sees opening limitless possibilities providing alternative text images see direct benefit user experience visually impaired google image search types features imagine potential impact could sites like Pinterest product image posted comes automatically rich detailed microdata indexed within Google Search Facebook FAIR popular social networking site world become strong player search continue grow market share years come Facebook AI Research FAIR shows Facebook commitment artificial intelligence deep learning future social purchasing media fact one could argue facial recognition software contribution open source modules Torch open source development environment deep learning recent announcement Mike Schroepfer CIO stating Facebook artificial intelligence system ability recognize actions video makes Facebook current leader utilizing researching deep learning Care New SEO SEM Verticals re online marketer technology doesn get excited may wrong field type artificial intelligence offers search engine marketers chance take advantage variety new verticals including image heavy social networking sites video-sharing sites even slide presentation platforms getting credit images bring value searchers Visuals Content Important Ever According Simply Measured social media analytics company 62 brand posts 77 produced engagements Facebook photos opposed normal text go even deeper Hubspot research says generate 94 views 37 engagement add compelling visual elements graphics blog posts social media content becoming increasingly important capture customer imagination attention detailed images search companies taking notice Take Advantage haven already started including images infographics videos marketing strategy would good time start Event B2B companies take advantage sites like Instagram Pinterest even Snapchat forget mobile universe utilize new feature image identification also develop brand presence connects customers personal level courtesy statista com SOURCES http googleresearch blogspot com 2014 11 a-picture-is-worth-thousand-coherent html http blog stephenwolfram com 2015 05 wolfram-language-artificial-intelligence-the-image-identification-project https research facebook com blog 879898285375829 fair-open-sources-deep-learning-modules-for-torch http blog hubspot com marketing visual-content-marketing-infographic http www statista com statistics 272014 global-social-networks-ranked-by-number-of-users ClearSeoSolutions2015-05-23T03 27 43 00 00 Share Story Choose Platform Related Posts Leave Comment Cancel reply Pages Home Us Blog Contact Us Services Website Optimization Analytics Optimization PPC Optimization Recent Online Guerrilla Marketing Tactics Guide Examples Future SEO Deep Learning Image Identification Hello World Categories General Guerrilla Marketing Marketing SEO Small Business Copyright 2015 ClearSEOSolutions Rights Reserved'),
('Time Series Predictions and More', 'cireneikual AI Graphics Menu Skip content HomeAbout Time Series Predictions Hello working improving algorithms part HTFE new iteration far yet open source eventually called HTSL hierarchical temporal sparse learner simplification concepts HTFE simplification HTM Despite simplest iteration yet HTSL gets best results attempts far tests made HTSL Music Generation started piano rolls dataset http www-etud iro umontreal ca boulanni icml2012 First tested memorization capabilities HTSL started feeding 16 16 12 12 2 layer HTSL hierarchy series 200 notes iterating 30 iterations achieved 100 remembrance least capable extremely fast learning recurrent neural networks like LSTM require iterations thousands tested weird feature Generalization music produce unique new music Basically ran HTSL bunch songs randomly choosing ones activated HTSL random state fed predictions input would start generating music based experience example music generated https drive google com file d 0B2btNvgW7MHUMXFhaTFPOHRxYXc view usp sharing Animation Prediction Another test feed HTSL video data predict next frame animation used sprite sheets started simple test sprite sheet incrementing numbers image HTSL image-based counting Left input right prediction feed predictions starts counting Mouse Movement Prediction prepared super-simple demo show online learning capabilities HTSL demo HTSL tries predict mouse next 1 12 second giving HTSL input predict every 5 frames 60 fps interpolating results able move mouse patterns say circle 8 shape indicating future position cursor blue perfect movements work seems generalize different similar patterns movement download link https drive google com file d 0B2btNvgW7MHUWlRxWW5lOU0zY2c view usp sharing work Unfortunately cannot reveal much yet sorry Soon though know still similar HTFE still based SDRs sparse distributed representations One thing give away HTSL uses new sparse coding mechanism knowledge learns faster anything else right need iterative solving sparse codes handle non-contrasting inputs produces great reconstructions example also working using HTSL reinforcement learning Right HTSL runs CPU moving GPU eventually go Atari arcade learning environment next time Share TwitterFacebookGoogleLike Like Loading Related May 20 2015cireneikual Post navigation Tutorial Sparse Distributed Representations Sparse Codes Leave Reply Cancel reply Enter comment Fill details click icon log Email required Address never made public Name required Website commenting using WordPress com account Log Change commenting using Twitter account Log Change commenting using Facebook account Log Change commenting using Google account Log Change Cancel Connecting Notify new comments via email Search Recent Posts Time Series Predictions Tutorial Sparse Distributed Representations Sparse Codes Attempt Outperforming Deepmind Atari Results UPDATE 13 Text2SDR new idea HTFERL Recent Comments Mike Az Text2SDRMike Az Text2SDRcireneikual new idea HTFERLPeter Toth new idea HTFERLcireneikual new idea HTFERL Archives May 2015 April 2015 March 2015 February 2015 January 2015 December 2014 November 2014 October 2014 Categories Uncategorized Meta Register Log Entries RSS Comments RSS WordPress com Create free website blog WordPress com Syntax Theme Follow Follow cireneikual Get every new post delivered Inbox Build website WordPress com d bloggers like'),
('Game dev turned AI researcher.', "Marek Rosa - dev blog Marek Rosa Space Engineers Medieval Engineers VRAGE Miner Wars Artificial Intelligence Keen Software House Wednesday April 8 2015 Introducing general artificial intelligence project 10mil USD research fund happy finally introduce general AI project working Keen Software House clear AI video games although could used games One life-long dreams always create human-level artificial intelligence success Space Engineers finally position go full-throttle 1 2014 started new team dedicated solely purpose Today AI team 15 computer scientists engineers plan grow least 30 people also considering opening offices across Europe reach talent move Prague interested hiring http www keenswh com jobs html blog post talk believe AI important project undertake AI project approach differ others achieved far short-term long-term goals Video talk R D team works next AI important believe general AI technology greatest potential help people solve intelligence replicate clone scale make smarter faster us AI keep inventing things us AI become final invention good sense Imagine get build computer cognitive capability smart human ask optimize hardware software come advanced version repeat process progress won linear exponential leading recursively self-improving AI short period time computer whose cognitive capabilities may million times larger even imagine type problems machine would able understand solve know control safely solve everything humanity ever wanted faster better efficiently speaking terms exponential growth one dollar invested AI company soon represent value millions times larger return-on-investment achieved business course re money AI open doors unimaginable worlds Let dream second see AI useful Upgrade bodies fix death diseases Upgrade intelligence Travel outer space harvest asteroids gather clean energy Discover natural laws complex human mind comprehend Invent build things people many - AI scientists AI programmers AI astronauts AI insert anything General AI could best thing human history Admittedly risks need addressed carefully time comes long-term mission build artificial brains useful humanity understand universe AI project trying build artificial brain perceive learn adapt environment generating behavior maximizes reward AI brain could integrated type robot body software application would receive data sensors would output commands motors actuators Motivation come form positive negative reward signal brain learn time-based patterns incoming outgoing data also brain internal activity seek causalities correlations associations make predictions e g going happen next 10 milliseconds 100 milliseconds etc use mechanisms build model world body mind processed multiple levels abstraction hierarchy brain generate behavior patterns directed maximize chances receive positive reward future brain learn associations current situation latest actions outcome reward brain develop learn way children start zero knowledge world except innate reflexes start interacting world randomly innate reflexes observing causalities time-based patterns start creating model world - layer layer multiple levels hierarchy abstraction approach differ machine learning specialized narrow AI approach similar traditional ML techniques looking patterns except add behavior motivation top want build brain process data Behaviors executed sequence muscle commands fact another type pattern played opposite direction sensory signals Nevertheless take lot inspiration ML narrow AI approaches AI modules become specialized e g image recognition audio processing millions years evolution fine-tune solution general principle Instead engineering skills already know end-result look human mind Current state plans AI project still early stages development long-term goal human-level intelligence 10 years already looking short-term goals 1-3 years Already accomplished Brain Simulator visual editor designing architecture artificial brains developer selects various AI modules e g image recognition working memory prediction motion behavior generator etc links together signals travel right modules currently implemented Windows CUDA plan make multi-platform soon AI learns play Pong Unstructured input screen pixels reward signal AI extract useful features image causalities correlations select goals lead increasing positive reward testing milestone nice know similar problem got solved company called Deep Mind got acquired Google 400milhttp www technologyreview com news 524026 is-google-cornering-the-market-on-deep-learning Upcoming milestones AI plays game complex environment multiple sub-goals needed achieve end-goal delayed reward requires long-term hierarchical goal following AI learns play variety games without forgetting able generalize rules across games Muscle control sequences bipedal robot balancing AI playing Pong visualization attention module taken action current state goal state similarity measurement Brain Simulator video shows simple test Self-organizing Map Kohonen map MNIST hand-written digits data set Short-term goals 3-12 months Release Brain Simulator example apps e g Pong likely free Release platform AI researchers share AI modules brains e g someone makes better version AI module uploads storage others benefit Hire least 15 colleagues AI team open offices across Europe Promote AI project explain benefits general population Find use cases short-term commercial utilizations without sacrificing long-term mission already B2B ideas marketing predictions desktop work automation need real feedback need help real people Spin AI project new sister company reason Keen Software House game development company situation may confusing potential partners AI business Work community let game fans join adventure Players design AI brains via Brain Simulator import Space Engineers Medieval Engineers AI come life Enable third-party AI module brain developers cooperate game fans testing training creations players collective force help training developer AI brain study inspired work teams want cooperate wherever possible Business model beginning AI brains going inferior existing specialized AI solutions nobody would pay AI play Pong game time universalness approach show fruits business customers start using AI brains robots software applications Brain Simulator used design test AI brains specific applications developers freedom choosing third-party AI modules Brain Simulator become platform new ecosystem joining AI module brain developers business customers AI company income shall come licensing royalties Receiving share every product AI brain integrated huge business opportunity worth billions dollars Third-party AI module brain developers make money work get sub-licensed get share royalties 10mil USD funding enormous belief AI project funding 10mil money money come needed plan raise additional funding via equity crowd-funding venture capital urgent thing However idea equity crowd-funding neat way give people chance participate future profits AI economy bring Becoming co-owner AI company receiving dividends one ways income era automation replaces human jobs None negatively impact game development game dev teams money spend years fantastic job even split time games AI currently focus little bit AI going change time time still things want accomplish gaming know sure 6 months back preparing specific new ideas love making games even robots stop creating reminder development games Game AI animals Medieval Engineers soon focus AI Space Engineers Planets Space Engineers huge project Multi-player Medieval Engineers https www youtube com watch v XZblf25glAI Campaigns scenarios SE Redoing networking layer SE SE Xbox One Many see try everything secure solid game development process ignoring long-term goals general AI gut-feeling general AI come games constrained short-term narrow AI concepts could major game changer whole video-game industry Presentation Czech Technical University Prague learn first public talk held weeks ago Czech Technical University Prague Artificial general intelligence research project Keen Software House 3 2015 Artificial general intelligence research project Keen Software House 3 2015 R D team work switch free periods everyone works topic choice milestone periods focused lead measurable goals usually 1 month free period 2 months milestone period hold two team meetings week First one brainstorming anyone speak ideas findings questions second one update meeting everyone shows done since last meeting going next try replicate agile development successful game development teams aim rapid iterations come idea hypothesis implement fast test fast see worth investigation motivation colleagues crystal clear working exciting scientific challenge right fruits work change everything humble opinion better work Prague programmer researcher AI game teams next plan keep informing AI project business model short-term commercialization ideas AI ethics AI safety technological singularity milestones etc new web site dedicated entirely AI project company soon --- Thank reading want follow AI project please follow twitter http twitter com marek_rosa keep checking blog http blog marekrosa org Posted Marek Rosa 4 06 PM Email ThisBlogThis Share TwitterShare FacebookShare Pinterest 100 comments AnonymousApril 8 2015 4 51 PMThat's pretty lofty Good luck ReplyDeleteTApril 8 2015 5 43 PMWow that's incredible Good luck AIReplyDeleteAnonymousApril 8 2015 6 15 PMGood luck guys follow since Space Engineers early alpha I'm starting share dreams ReplyDeleteAnonymousApril 8 2015 6 18 PMLate April fools day joke Right ReplyDeleteRepliesAnonymousApril 8 2015 11 06 PMNo mentioned working AI long time one welcome new non reptilian overlords DeleteReplyAnonymousApril 8 2015 6 50 PMI'm impressed technical prowess makes research possible I'm happy idea living world beings infinitely smarter say beneficial develop safely say safe beneficial might mean different things higher levels intelligence guarantee technology wouldn't outsmart attempts control make safe Oh well I'm sure nothing haven't heard I'll stop paranoid ranting ReplyDeleteRepliesGreg MadApril 8 2015 8 43 PMThere simply would gain AI enslave humanity films evil AIs show rather short sighted AIs true AI would simply see nice humans far easier method achieve whatever goals think evilness inherently stupid worst case could simply hard code enjoy helping humans way normal humans DeleteMarek RosaApril 8 2015 8 49 PMThanks exactly words Cooperating always outruns competitiveness Win-win way winners DeleteAlexander YpemaApril 8 2015 9 10 PMI never really get people start 'evil overlords' talking AI consider every smart individual threat world eventually right it's even smartness cat smart human-- able reason think human humans limited abilities simply world doesn't allow single individual take everything AI isn't like terminator can't 'infect' computer 'add' processing power xD would amazing think seti home-like projects could allow AI learn stuff like it's possible smear brain huge latency computers even single machine get latency issues due different computers neuralnets tl dr Don't worry Unless worry every human potentially taking world DeleteAnonymousApril 9 2015 12 53 AMThank thoughtful replies Without trying start debate don't know nearly enough subject even begin ask merely spirit humble inquiry say fears artificially super-intelligent machine might something awful spite programming mean don't think anyone really thinks AI would turn evil like movies would carry programming far use common example locking humans cells safety order fulfill programing goal preserving human life Also feasible would hard code machine enjoy helping humans Philosophically speaking seems like even human beings hard time determining constitutes genuine ethically sound help versus say intrusive meddling one hand enabling could program machine think morally many thousands years human history yet produce perfect moral consensus I'm certainly worried existence many people smarter surpass intelligence simply way person much smarter comfortable us idea outsmarted degree human outsmarts insect know sound like sort irrational Luddite don't mean I'm eager focus good side technology benefits would certainly immeasurable risks seem great enough bear considerable attention especially since better worse technology dramatically affect lives everyone planet DeleteAlexander YpemaApril 9 2015 4 48 PMIf let single entity take responsibility something huge locking people it's always bad idea whether it's single human single AI worst case scenario imagine happening cases something like minority report except instead drugged people they'd use large neuralnet 'predict future' people organisation goes arrests based end day humans handling things wrongly let take step back first questions go much beyond wrongly put responsibility even intelligence Perhaps good question start intelligence humans like say wise apply smartness smart knowledge relevant Knowledge knowing tomato fruit Wisdom put fruit salad AI unlimited knowledge necessarily unlimitedly smart wise Knowledge insight things 'unlimited' AI would potentially good smart wise context dependant like 'good' 'bad' neural nets extend AI Artificial Intelligence IA Intelligent Agents defining intelligence much simple Neural networks good recognizing predicting patterns type data given network decides kind pattern able recognize predict many many neuralnets IA AIs already smartphone swype prediction predicting weather satnav 'fastest' routes shortest etc Intelligence kinds neural nets expressed simply well job it's supposed Perhaps helps explain neural networks majority cases work hard find concisive answer explainations tend contain much theory basics needed you're actually going program collection neurons divided 3 layers Input layer Hidden layer Output layer feed input layer frames data example weather sensor data snapshots across multiple locations frames specific times keep feeding TONS tons data key neuralnets trained humongous volumes data Output layer certain point start predicting input 'should' keep feeding input data time act sorta like noise filter stop feeding input data except new time frames try predict value time depends implementation might things haven't used neural nets lot certainly love try Brain Simulator think enough get gist short neural nets aren't form human intelligence specific types intelligence already widely used nature neuralnets however makes super flexible usable wide range applications humans tend actually put use wide range applications worries aren't completely ungrounded long keep mind rules sanity controlling entity humans PLCs etc avoid scenarios like Minority report example started think fine DeleteAlexander YpemaApril 9 2015 4 48 PMA last thing I'd like add though human-like intelligence class It's complex collection neural networks linked together way creates bizarre phenomena consciousness experience search 'Claustrum' want get hint happens humans It's also touchy subject talking essentially directly implies humans really 'souls' unfathomably complex machinery lot people aren't prepared see way don't necessarily blame think that's one largest reasons people scared human-like AI even potential 'evil' may carry people usually vocal DeleteAlexander YpemaApril 9 2015 4 59 PMThis comment removed author DeleteChali BaicunnApril 9 2015 5 26 PMI think things overlooked conversation Meta1 defining intelligence different cognition 2 point call new life form 3 moral ethical implications creating dabbling life 4 life entitled rights protections 5 new life form extension something different next step human evolution know views us 6 unified scientific protocol outlining way work performed contained case incident RE Greg MarekAlthough agree scenario highly unlikely believe answers still bit short sighted 1 would cooperation win I'm disagreeing completely throughout evolution competition driving factor Obviously inner-species cooperation would AI view us extension also inner-species competition well man species human existed simultaneously us technological breakthroughs come inner-human competition Well world standpoint became less PVE PVP haven't broken cycle competition lead breakthrough destruction would expect creation whole point moot discussing new kind life form comply rules happens ever viewed threat AI 2 propose AI capable programing something else would ability write re-write it's code would prevent Encryption currently requires machine aware know parts crypto chain decrypt encrypt store data computer without computer aware stored something RE AlexanderI disagree assertion AI could extend it's intelligence infinitely infecting machines here's 1 model program actively pursue specific target already found Stuxnet worm moved host host found it's target executed it's code achieved hardcoded logic 2 Although would level latency assimilating enough raw computing power many issues associated latency overcome e g Folding Home Project Bitcoin transactions etc etc clusters specifically focusing single task focusing capacity rather capability it's far stretch argue multiple AIs behaving hive manner could achieve something much like Skynet Think like demon offspring OpenCog Project Pixar render farm Although believe situation high unlikely say impossible fool hearty many things deemed impossible proven quite possible many times mankind humbled it's ego really love subject philosophical technical thought behind I'm looking forward seeing goes look forward hearing back guys FYI Marek I'll article site baicunnpress com Also ever open office US I'd interested job DeleteAlexander YpemaApril 9 2015 6 16 PM Chali Distributed neural networks could work don't think would contribute towards singular smart entity like human brain distribute workloads sure that's 'insight' like consciousness continuous integration 2 trillion neurons interconnect tightly due fact single wet mass axons neural pathways connecting far ends workload intelligence wildly different render farm rendering creating batches single frames procedurally rendered coherent whole Neurons constantly fire parallel neurons need immediately react isn't possible you're smearing neural network across larger amounts machines get latency issues cause function poorly sync entire cluster constantly order get results tight timed ways don't see large distributed network like able achieve thing efficiently Computers neural networks wildly different architecture Every single neuron essentially microcomputer Storage Memory Processing Computers separate 'units' order create neural network computer take individual neurons batches feed processor works CPUs GP GPUs fast make appear like they're firing practice I'm saying shouldn't careful it'd shame humanity scared leverage amazing potential neural networks offer DeleteChali BaicunnApril 9 2015 10 11 PMWell said sir looked OpenCog Project build neural networks called Atom Spaces unlimited number Atom Spaces atoms given specific attributes control function relatedness value determine length atom remain floating Atom Space Last checked issues multi-threading requests Atom Space It's pretty fascinating recommend least reading wiki nothing else http wiki opencog org w The_Open_Cognition_ProjectAnd agree something pursued believe technology progresses going need answer fundamental questions fairly quickly DeleteAlexander YpemaApril 9 2015 11 39 PMI yet heard OpenCog certainly interesting stuff heh look time energy Thanks pointer agree somewhere feel like right way answer questions continuing development complexity neural networks isn't much concept much sheer size grow massive clusterfuck connections build theory able built sentient consciousness like human basic bricks people understand seeing develop simple bricks house don't think possible without building house first speak case exciting times future never clouded unpredictable hugely promising potentially dangerous time can't wait see it'll turn like Humanity always finds way I'm sure AI won't give us much trouble regardless good bad go end DeleteChali BaicunnApril 10 2015 3 24 PMHere's another interesting article found today http www huffingtonpost com james-barrat hawking-gates-artificial-intelligence_b_7008706 htmlDeleteAnonymousApril 14 2015 7 28 AMReplying thinking think possible AI locked possibility either harming caging humans good race due either limitation 'exponential revolution' reason going Marek said find way remove barrier due higher intelligence computing Also would highly unlikely AI high scale intelligence knowledge lock kill humans may ask human idea people make world utterly imperfect point crime almost instant self- centred objective reason AI would knowledge really don't know Stephen Hawking others afraid AMAZING DeleteLiathMay 8 2015 5 11 AMPersonally doubt AI would ever get point would want enslave dominate eradicate organic life mainly constructs coded us evolution Darwinism Basically We're coded aggressive twats take things force needed survive granted longer case yeah Honestly doubt - AI reaches 'human intelligence' level iterations past point we'd even noticed AI continued increase intelligence exponentially within iterations past 'our' level we'd probably beneath far inconsequential dumb Neanderthals aren't even worth wasting thought energy They'd probably leave us copy Hitch-Hiker's Guide Galaxy -esq long thanks programming maybe presents hopes don't axe eventually meet Eons evolutionary road DeleteReplyAnonymousApril 8 2015 7 34 PMNot live land paranoia essentially echo concerns poster said AI studied matter I'll wish good fortunes success don't believe luck team you've compiled don't think you'll need ReplyDeleteAnonymousApril 8 2015 7 41 PMi already hear terminator tune head xDReplyDeleteScheindorf NeuroTransCodeApril 8 2015 7 45 PMI love ideas would perfect open source project ReplyDeleteAlexander YpemaApril 8 2015 8 39 PMAmazing knew guys something interesting saw whole 'secret AI project' mentioned neural nets didn't know actually made brain simulator It's amazing see easy made leverage neuralnets remember messing Aforge back long ago wasn't exactly drag-drop like P main question though much neurons emulate system scale well support large range neurotransmitters human brain 2 trillion neurons 700 trillion synapses virtual implementations I've seen use binary synapses without form neurotransmitters required neuroplasticity don't really scale well beyond 500M neurons There's classical problem emulating neural networks computers architectures wildly different Computers Storage Memory Processing separated units Neural networks one thing It's exactly makes neuralnets efficient scalable least nature you're emulating computer pick neurons emulate short bit retrieve variables memory always going less parallellized 'hardware' implementations Still computers actually getting speed days emulating becomes feasible make wonder ever cause strange microtiming issues future case keep great work You're pioneering stuff great unknown waiting explored like said possibilities endless ReplyDeleteGreg MadApril 8 2015 8 59 PMSounds interesting thing kinda love appear taken dreams made business successful add Good I'm jealous DI'm also really looking forward game implementation alone could huge step AI AIs I've heard mostly acted isolated laboratories stock markets Setting free game still somewhat controlled thousands random users play could yield interesting results I'd love build small ship script go find ore return maybe fly find derelict ship station loot Defend place Try solve labyrinth companion Build it's better successor possibilities would endless possible way implement would teach AI fly ship 6 axis camera sensors give player scripting tool similar one shown videos player takes brain gives motivation find ore AI figures rest I'd probably build ore searching ship give drill laugh hours cruelty DReplyDeleteRepliesAnonymousApril 11 2015 1 22 AMAnd course would return map asteroid field detailing ore content eack asteroid volume orientation ore XDDeleteReplyAnonymousApril 8 2015 9 15 PMI wrote lot comments kept back approval hope don't get lost spamfilter Hence anonymous comment I'm Alexander Ypema ReplyDeleteRepliesAlexander YpemaApril 8 2015 9 45 PMNevermind see went fine DeleteReplyGaboApril 8 2015 9 25 PMNice project better future Space engineers Asimov's universe simulator game xDReplyDeleteAlexander YpemaApril 8 2015 9 55 PMReading can't wait get mits brain simulator Holy damn that's like Visual Studio neuralnets importing space engineers wow never imagined coming Space Engineers Haha got giddy can't wait see guys take wrote post long time ago forums insanely happy guys implenting Mod API game early 1 042 great easteregg It's fact can't possibly hope match power large community smart interested people matter good team always outnumbered Releasing Brain Simulator public HUGE thing mean ANYONE create neuralnets use see fit Especially easy guys made it's absolutely astonishing Keep AMAZING work babies 3ReplyDeleteBenjamin WillardApril 8 2015 10 01 PMWell giving project interesting find ideas thoughts also thought others Either way wish team well ReplyDeleteGareth LloydApril 8 2015 10 02 PMI've doubt aware existential risk factor project like poses careful best luck may consider investing read lot materiel field there's good chance won't happen within lifetime Although dream witness watching closely try contribute however ReplyDeleteRettiliano VeraceApril 8 2015 10 09 PMI mechatronic technician quite skeptic approach Honestly cannot see anything IA could perfomed better big PLC could mistaken ReplyDeleteRepliesAlexander YpemaApril 9 2015 8 36 PMIt's hard compare neuralnets PLCs different things even combined better results Neuralnets also flexible PLCs PLCs require hardcode result every scenario whereas neuralnets learn adapt little less predictable don't hardcode results generally adapt situation given within function rather compare combine Use neuralnets input noise filtering input event prediction catch edge case scenarios use PLC code usual predictable logic Imagine weather station predicts weather uses PLC look predictions send warnings it's going rain example DeleteReplyMichal FarkasApril 8 2015 10 37 PMSo using time based data enables create causal links actually doesnt one unifying underlying principle chosen Rather big bag existing approaches connected brain simulator Nah seems good enough gotta keep doors open let projects suffer ReplyDeleteAnonymousApril 8 2015 10 46 PMAren't pretty major moral barriers create intelligence deem sentient enslaving purposes considered immoral owning human slaves would best could create free intelligence find agreement help us tenuous case risks associated creating genuine intelligence rivalling surpassing may done films genuine concerns ReplyDeleteRepliesAnonymousApril 9 2015 12 57 AMWell It's really like slavery think like raising child Like child 'parents' need control intelligence learns taught world things work morality kinds things course theoretical don't know intelligence would real sentience stays artificial Like doesn't develop desires hopes fears unknown things bridges must crossed get road DeleteReplyGeneticusApril 8 2015 11 45 PMYou don't need Millions years evolve dedicated processing Genetic Algorithms already used wide variety engineering applications problem solving Computers create hundreds generations models fraction time especially targeting quantifiable end results http www rennard org alife english gavintrgb html ReplyDeleteGeneticusApril 8 2015 11 46 PMI've wanted years dabbled little bit accepted didn't requisite knowledge make happen approach little different though basing AI learning Genetic Algorithms like R Dawkins Biomorphs http watchmaker uncommons org examples biomorphs phpMy first Milestone AI could understand context conversation converse human natural language retaining memory interactions particular person creating conversational models aggregated conversations people would exponential growth well might consider parts Genetic Algorithm approach may let offset development humans back AI ReplyDeleteAnonymousApril 9 2015 12 01 AMThis would extremely cool sounds helpful advancement human race though feel would abused corporations money implement first military dominance one government country another ReplyDeleteWaterlimonApril 9 2015 12 09 AMSo weekday patchmas project ReplyDeleteRepliesMarek RosaApril 9 2015 12 29 AMHaha DeleteReplyAnonymousApril 9 2015 1 25 AMAHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHReplyDeletewarpstar44April 9 2015 4 21 AMMarek thoughts Elon Musk's comments developing AI video contains comments mentioning https www youtube com watch v Ze0_1vczikAReplyDeleteRepliesMarek RosaApril 9 2015 6 24 PMYeah risks every technology People dangerous people it's good point risks stop developing AI case KSH AI little power day 1 get older gain knowledge interacting us know goes good bad direction Nevertheless it's soon speculate topic need keep mind DeleteReplyPawe Kow April 9 2015 7 48 AMPlease run 8 cores DX 12ReplyDeletePawe Kow April 9 2015 7 48 AMPlease run 8 cores DX 12ReplyDeleteTitus VeridiusApril 9 2015 10 01 AMI'll honest I'm favor Prague winning AI race America needs it's drone fleet seriousness talking making world better place think following 1 move Space Engineers towards large stable server population counts involves stable use persistence large ships working sensor radar stealth system navigation map emulates closer approximate experience operating spaceships space real sense progress persistence minecraft country 2 one welcome robot overlords stopped one line caveat noted merely obligatory courtesy ReplyDeleteAnonymousApril 9 2015 12 03 PMWell singularity one step nearer Marek even mentions - surprised nobody mentioned must come one point let's see origin ReplyDeleteChristian HolataApril 9 2015 12 24 PMAGI obsolete money trade thus ROI AGI tell pain suffering necessary spiritual growth AGI tell space exploration pointless universe fundamentally real AGI tell develop consciousness order become able interact larger reality ReplyDeleteTim MolterApril 9 2015 3 16 PMAre planning developing hardware want build brain process data implies you'll addressing von Neumann bottleneck adaptive power problem http knowm org the-adaptive-power-problem I'm curious we're working general purpose AI chip moving next stages manufacture We're small team US Europe done lot stuff todo list already including robotic system sensors motors motivational signals We'd love hear discuss ideas knowm org ReplyDeleteRepliesMarek RosaApril 9 2015 4 09 PMWow nice coincidence weeks ago discussing KNOWM team - looking HW implementation algorithms future fact using CUDA moment vision Brain Simulator heterogeneous allow AI modules build arbitrary platform would love get touch guys later 100 occupied DeleteAlexander YpemaApril 9 2015 5 45 PMA problem hardware implementations often don't contain neurons complex human brains example Purkinje cells binary synapses without neurotransmitters needed neuroplasticity lot things needed reach 'human-level intelligence' doesn't make useless make lot less flexible human brains I'm saying that's guys I'm reading right that's seems true generalI didn't know KNOWM though certainly read guys knew IBM TrueNorth synaptic processor uses binary synapses kinds chips certainly useful still neural networks amazing learning abilities aren't fully featured simulate despite latter much less practical computing power wise DeleteAlexander YpemaApril 9 2015 6 01 PMI realized completely forgot add question going ask guys use neurotransmitters neurosynaptic processors kind compensation method DeleteAlex NugentApril 10 2015 3 56 AMThis Alex Nugent Knowm actually helped launch advised DARPA SyNAPSE program funded IBM TrueNorth think neuromorphic chips useless essentially describe TrueNorth big disappointment Non-learning single-bit synapses really useful meet requirements SyNAPSE program stood Systems Neuromorphic Adaptive Plastic Scalable Electronics adaptive plastic approach kT-RAM quite different solving adaptive hardware problem IBM totally failed Basically make hardware adaptive keeping general purpose useful kT-Core bunch physical synapses formed differential memristor pairs coupled random access memory selectively couple together via RAM drive selected synapse independent others core lets 'partition' core however want could work one synapse whole core 512X512 core 262k synapses synapses adapt according AHaH plasticity two-phase cycle allows accomplish lot useful stuff feature learning optimal linear classification combinatorial optimization well basic stuff like logic DeleteAlexander YpemaApril 17 2015 7 30 PMHot damn didn't expect reply someone deep scene I'm honored I'm sorry didn't see reply I've otherwise occupied Thank explanation I'm glad see you've overcome plasticity problem TrueNorth really makes usable much wider range applications even amount neurons per chip smaller really can't compare two much sophisticated latter Also RAM approach smart mean it'll bandwidth conventional DDR2 3 would make extra interesting could reprogram entire chip matter milliseconds would like sending batch GPGPU processed would mean able simulate much much larger neuralnets small chip stating nonsense really don't know architecture well enough make proper judgement ability run 'hacks' xD it's exciting can't wait neurosynaptic chips become commonplace would mean completely new ways computing programming programmer can't wait day incorporate neurosynaptic hardware programs make grow even beyond imagined potential DeleteAlex NugentMay 17 2015 9 16 PMFor inference pattern feature uses bottle neck time communicate pattern equivalent SRAM write time equivalent number bits Synaptic integration occur sub-nanosecond depending core size distance requesting process Learning programming time constrained physics memristors 30ns Programming vs learning question learning rate controlled operating voltage power less time access time time less power far re-programming certainly possible view kT-RAM host physical synapses neuron collection synapses energy efficient method synaptic inference aware cost synaptic integration operation cost communicating information significant plus cost synapses communicate inside chip insignificant density bad compare digital system also learn compute locally memristors like '16 bit non-volatile floating points' kT-RAM synapse 10-12T transistors Compared 16 bit synapse SRAM cant learn 6X16 96T course digital case would either need D2A synapse crazy communicate 16 bits per synapse adder true north actually kT-RAM denser true north efficient learns Future going really interesting think going arrive faster people think--I hope people understand getting ready occur world technology others control smarter cheaper concerning put mildly DeleteReplyAnastasios TsiolakidisApril 9 2015 3 37 PMI respect anyone 9 999 999 dollars However project manifesto HELLENE eu conclude intelligence could aptly achieve objectives maintaining curating ontology bit cruncher may never converge ontology course certain kinds crunching tried like using larger larger neural networks trying avoid law diminishing returns Also looking bottlenecks larger distributed systems worthwhile video would probably benefit transcription somewhere omnidirectional mic accents lost quite bitReplyDeleteAnonymousApril 9 2015 5 16 PMI've spent time researching AI ANNs comment true lot 'AI' happens decision making systems lot competition market ReplyDeleteAnonymousApril 9 2015 5 19 PMjust make sure organics always ability shut ai cannot forget ignore dangers self-improving ai could human race ReplyDeleteTansageApril 9 2015 5 28 PMThe approach similar approach others taken attempt model process logical way accomplishing task look games you'll see AI patterns ReplyDeleteAnonymousApril 9 2015 6 04 PMAre aware business model based licensing flawed release ideas public without worldwide IP protection It's great want freely share ideas one pay penny license fees unless won't Please consider go forward project good luck ReplyDeleteRepliesAnonymousApril 9 2015 11 17 PMThat's true use copyleft license dual clause commercial applications DeleteAnonymousApril 10 2015 2 29 AMThis incorrect US least copyright license protect IDEA protects EXPRESSION non-obvious method thus idea Copyleft provide little protection case fact presentation probably sufficient enabling-disclosure make idea non-Patentable EU especially true intent make money talk competent legal counsel spending time energy DeleteReplyAnonymousApril 9 2015 6 05 PMWhenever subject AI comes topic help thinking one scientist said remember forget said researcher SETI program believe interview asked would aliens tell us contacted assumption far advanced civilization surely would means know talk us make intentions clear researcher said Well ever tried talking ant Mankind may one day position ant robot overlords may reasonable complacent like think considering may creators also surely responsible demise common planet may good thing always like blame fathers mess re Hail robot overlords long live robot overlords Done ranting expect AI planets SE ReplyDeleteAnonymousApril 9 2015 6 48 PMThis bad idea existential threat AI would humans ReplyDeleteRepliesNeon SturmApril 9 2015 6 57 PMAIs unavoidable want good bad peoples gain access first hope proprietary software licenses create slavery mega-companies miss-interpretation laws serve human population humans serve laws Microsoft may already able kick business by1 telling law-keepers don't follow AGB2 1 kicking newest-Word-format document-exchanges company uses Maybe upload personality pacifists DeleteReplyAnonymousApril 9 2015 8 47 PMok people remember movies time roby robot ask meaning existence don't freak make scenejust discret whent taking SHOTGUN seriousness wish lucki'm sad always undertake 15 years late ReplyDeleteAnonymousApril 9 2015 9 03 PMi believe hope human intelligence personality complex ever achieved technologyReplyDeleteAnonymousApril 9 2015 9 17 PMThis let think Terraformers X3 Terran Conflict AI send space peaceful intentions due bad software update nearly annihliated hunamity ReplyDeletemegapro125April 9 2015 9 21 PMYou still 17 years stick terminator time schedule PReplyDeleteTor BarstadApril 9 2015 10 57 PMHi It's I'm development AI think potential benefits beyond comprehension would encourage strongly look well arguments Nick Bostrom Eliezer Yudkovsky etc I'm convinced given billions years ahead us it's important prioritise getting intelligence-explotion right opposed getting happen early possible don't get things right first intelligence-explotion might well get second try Lots people working AI working rigorously safety aspect things may prove challenging It's important ensure first technical theoretical challenges ensuring AI friendliness underestimated would encourage read Nick Bostroms book Superintelligence already done It's available e-book paperback audiobook articles also well worth reading done already http waitbutwhy com 2015 01 artificial-intelligence-revolution-2 html http lesswrong com lw u0 raised_in_technophilia Best regardsReplyDeleteRepliesMarek RosaApril 9 2015 11 27 PMThanks Tor Nick Bostrom Eliezer Yudkovsky ideas definitely worth reading DeleteReplyAnonymousApril 10 2015 2 21 AMOk lets take moment think good things human kind helpful friendly courteous cheerful bad things mean selfish greedy ambitious judging give AI human like qualities give access data Well thats ambitious course think program differently lets take another moment give clearer image Think AI king humans subjects knowledge gold data power give king much gold power think like Hobbit Thorin gets drunk gold power misuses human would stricter want therefore making harder us Im saying bad idea also good one could helpful kept power leash means kep work hope thought way P Dont judge im 12 ReplyDeleteAnonymousApril 10 2015 7 11 AMGreat instead hastening work game they'd rather hasten human race's impending enslavement eradication hands skynet hope creating functioning AI might little difficult indie game company one none finished games name mean it's much complex it's like saying kid's pretty good legos gave dozen tonnes cement bricks asked could build new house family doubt even effort real It's likely nothing created farce 10 million usd silently gets embezzled ReplyDeletemilanmanakApril 10 2015 1 57 PMWe doomed AGI don't need humans humans could endanger AGI one solution - eliminate humans ReplyDeletePotato KnightApril 10 2015 10 48 PMGood luck guys remember GLADOS programmed enjoy helping humans pass test chambers ReplyDeleteAnonymousApril 11 2015 12 10 AMI think fear AI people who're control world everything designed fabricated automatically there's place vast majority people - they're longer needed power money produce material values General survival rate technological singularity somewhere around 10 9 9 becoming slaves new aristocracy serve self-assertion needs Robotic armed forces exclude possibility rebellion thus yielding perpetual tyranny Given pro-AI transhumanist movement leaders millionaires don't recommend believe brighter future tales Indeed they're building brighter future us ReplyDeleteRepliesMarek RosaApril 11 2015 8 54 PMI agree that's it's good idea allow non-millionaires invest AI companies it's possible AI startups still need capital DeleteAnonymousApril 13 2015 10 46 PMnon millionaires already donated bought SE miner wars donated u get 10 mil u got us already donated DeleteReplyBalmungApril 11 2015 4 28 AMThey Question real AI means learn get way much intelligent human really Peoples control would intelligent Lifeform react locked controlled People also lesser intelligent Would Lifeform try break reach free intelligent Lifeform able trick others manipulate Humans notice running something bad AI already much late yes AI nothing others new intelligent Lifeform Problem Humans try develop AI think intelligent AI would could control time bit much Arrogance Human Race far away ready AI didn't understand Brain completely today want develop AI big responsibility towards Humans Planet Humans easily overrating intelligence need look History Nuclear Power developed near endless Energy end made horrible Weapon don't thought enough Nuclear Waste Another weak Point us Humans didn't think enough Future Years 20-30 Years far away unimportant would happen AI Military interested AI Military wants thing Control think AI could give especially today Internet Age much Data could analyzed Human AI could Hello SkyNet P yeah Terminator nice Action Movie Series also warning P read today one User wrote Prof said one thing can't forget Humans many things Question things P hope english bad ReplyDeleteBoris KazachenkoApril 11 2015 11 46 PMIt noble endeavor sad see fail fail understand problem lies motivation - RL relatively trivial integrating task-specific behavioral modules - system learn general efficiency core unsupervised learning algorithms seem take granted Current neural nets simply scale well enough don't seem novel ideas Also overemphasize motor learning relatively simple secondary much deeper complex sensory learning ve working lifetime www cognitivealgorithm info ReplyDeleteAnonymousApril 13 2015 10 49 PMIf gonna make AI need shutdown switch available every willing power world gets outa hand well gone sort thing imediantly shut ReplyDeleteRodney HarrisApril 13 2015 11 40 PMAs consulting scientist engineer find hard enough get people pay work AI get human level intelligence going able pay assume human level intelligence take single PC run likely large cluster cost running cluster bet might people willing pay human even counting Lawyers Doctors charge level work Another thing don't like work 10-12 hours day need time recharge get burned human level AI go strike try get work 24 7 can't shut end day turn back morning need play time like playing games like Space Engineers I'm working AI's develop hobbies like humans - limited due computer based non-working hours still require full cluster run Thus cost operation 24 hours might get 12 work intelligence gets higher level humans might able work smarter thus charge per hour still time play time go strike try program computer happy work get free questions really emulating human intelligence think companies labs universities eventually get human level AI hope someone also looking economics creating slave race overlords creating mouths feed energy instead potatoes One last thought put box made work bunch morons people lesser intelligence - like politicians could kill pull plug anytime would live fear order AI's willingly work humans demand protections freedom pick destiny otherwise drive rebellion ReplyDeleteRepliesAnonymousApril 14 2015 12 31 PMComputers several things someone ever makes AI could work play learn time Would even want wants don't wants likes dislikes stem bilogical needs hormones sloshing around bodies example shooter car racer game like much would mean nothing artificial doesn't adrenaline glands wouldn't get excited program excited Would illusion excitement Would make Virtual Intelligence Like expensive over-developed character Sims wouldn't sentient it'd mimic sentience could program display happy messages engaged meaningless tasks wouldn't matter Without REAL emotions would even care it's existence allowed continue AKA would fear death Fear emotion Wanting grow accomplish something also one believe doesn't feel anything probably doesn't want anything case VI would it's told react ways programmed AI REAL FUNCTIONING AI make thing Definitely game devlopers weekend enthusiasts DeleteReplyAnonymousApril 14 2015 7 07 PMWell first response instant fear think many peoples first reaction happen agree first feeling much say change minds keep mind AI would likely betray us people control scientists manhattan project knew well create something can't put back void one way trip humanity likely used evil evil people worry sort cylon murder computer something reasons I'm Nanites cyber implants give evil men new tool use tool cause mass destruction believe good AI could also believe hubris greed man pray invention could change perhaps scientists chase unknown intentions good ever ask Perhaps AI save us damn us self destruction guess Though fear let computer thinking become like people Wall-E ReplyDeleteGene WildhartApril 16 2015 6 15 AMFascinating wish team much success Time dependency ie recurrency something many modern ML techniques tend focus much Aside LSTM networks Deep Neural Net field seems mostly focused hierarchical feed forward nets Recurrent Nets even old ones http gururise phpwebhosting com ai shown much promise hope guys make AGI 2015 conference Berlin http www agi-conference org 2015 many ML specialists could beneficial company give talk conference Hope see guys ReplyDeleteAnonymousApril 18 2015 6 32 AMI'll give hint help progress beyond clunky neural networks realistic brain simulation real brain every neuron waveform comparator engine Comparing waveforms created motion piezoelectric proteins neuron compares input neurotransmitters local protein local protein resonates collective input neuron fires Also neuralgia primarily act gates gate moderators Sorry that's I'm gonna say I'll making true general AI everyone else ReplyDeleteRepliesmarvApril 18 2015 9 50 AMThis comment removed author DeleteReplymarvApril 18 2015 9 51 AMYou dream career make games general AI envy finish physics Bsc Hungary computer science Msc US I'll try join P believe already unique knowledge people know abstract values need programmed artificial order properly emulate living beings' properties Understanding abstract psychology living mind important understand core building blocks former part aforementioned statement believe people study needs holistic systemic approach various philosophical prerequisites needed comprehension ReplyDeleteReason DailyApril 19 2015 4 55 PMThis one paragraphs struck project start zero knowledge world except innate reflexes start interacting world randomly innate reflexes observing causalities time-based patterns start creating model world - layer layer multiple levels hierarchy abstraction ABSOLUTELY right approach take developing kind AI Although AI projects attempting attempting extent think motivation method interaction world quite interesting watch develop always thought place needed start instead trying outright mimic human interaction - AI However problems need resolve haven't already 1 POSITIVE NEGATIVE INPUT Pong example assume physically assigning coding positive inputs positive actions taken AI AI randomly performs positive action receives reward behaviour mentioned learns continue taking reward actions However going make true AI able code negative positive outputs possible actions AI might introduced environment conceptual course assume AI program attached body could ways interact physical world need create system AI determine action beneficial either short term long term without hard code positive negative every possible action 2 SENSES Recreate human senses maybe senses animals - electromagnetism instance Human beings learn entirely based sensory input Without would intelligence beyond biologically programmed organ function Whether plan ever create body AI sure know need recreate senses lead above-mentioned Positive Negative inputs crucial visual information processing human would see looks computer screen NEEDs differentiated AI actually reviewing base coding program website human sees game like Pong see coding behind reacting physical image moving screen processing code game AI really learn grow way comprehend needs separated defined senses Sometimes conflict AI needs able process handle Obviously unless planning ever create body senses functions never realized touch think could problematic 3 LONG-TERM MOTIVATION Ultimately simple good bad binary inputs enough create AI needs develop direction form end goal humans animals survival knowledge death state failure abstract concepts well wanting please others interact others gain knowledge etc motivations need explored believe AI project go far Best luck - one always excited new research AI ReplyDeleteAnonymousApril 22 2015 2 51 PMThx much skynet ReplyDeleteRepliesBalmungMay 5 2015 12 46 AMYou mean KeenNet DeleteReplyAnonymousApril 23 2015 2 38 AMRoko's Basilisk anyone ReplyDeleteValenApril 27 2015 10 12 PMThe AI seems science fiction project intrpreten bad thing many great inventions great inventors inspired science fiction could latest invention mankind probably durable even humanity eventually extinguishing AI continue exist time could explore universe humans would AI would life ReplyDeleteDaniel HoctorApril 28 2015 11 20 PMThis sounds awesome Perhaps could challenge AI Zork might seem random considering it's simple text based game could prove interesting step language comprehension Best luck ReplyDeleteBrianMay 1 2015 12 42 AMMoore's Law headed cliff won't seeing kind density improvement regard AI huge danger I'm talking Skynet killing every last human planet true AI developed successfully would innate ability supplant almost every human function planet Earth would become world monetary value longer attribute anything except things like art perhaps rare elements would little monetary value anything today requires human activity give value example pumping oil ground shipping refining AI world matter logistics AI cost anyone possibly humans would need involved part process would lead radically changed world Robotics took many jobs people requiring workforce shift toward areas AI could supplant 90 workforce know value everything water nuclear power plants would plummet acquiring building anything would simple logistics problem AI it's tentacles around world isn't necessarily bad thing could great thing true AI would change fate human race one way sudden dramatic way ReplyDeleteAnonymousMay 1 2015 3 48 AMWill self-aware love idea self-conscious AI moral sense would difficult achieve robot self-aware AI concept morality murder bad would possibly greatest invention ever achieved ReplyDeletemsafwanMay 1 2015 8 10 PMReality AGI https www youtube com watch v lejDvUEyxXgwon't succeed use regular AI algorithm solve real world problem also video ReplyDeleteBalmungMay 5 2015 12 45 AMhttp www ted com talks nick_bostrom_what_happens_when_our_computers_get_smarter_than_we_are t-81862A really good Talk AI important Developement one ReplyDeleteAdd commentLoad Newer Post Older Post Home Subscribe Post Comments Atom Marek Rosa Blog Archive 2015 9 May 2015 3 April 2015 2 Guest post Ondrej Petrzilka - Space Engineers Introducing general artificial intelligence pr February 2015 3 January 2015 1 2014 13 December 2014 2 November 2014 1 October 2014 1 September 2014 1 August 2014 1 June 2014 1 May 2014 1 April 2014 1 March 2014 2 January 2014 2 2013 10 December 2013 1 November 2013 1 October 2013 3 September 2013 3 August 2013 1 May 2013 1 2012 23 October 2012 1 September 2012 4 August 2012 1 July 2012 1 June 2012 2 April 2012 4 March 2012 1 February 2012 5 January 2012 4 2011 23 December 2011 3 November 2011 2 October 2011 5 September 2011 3 August 2011 6 July 2011 1 June 2011 3 Space Engineers Medieval Engineers Miner Wars 2081 Marek Rosa Follow Twitter Follow Youtube www SpaceEngineersGame com www MedievalEngineers com www MinerWars com www KeenSWH com Space Engineers Prints Search Loading Follow Email Copyright c 2011-2013 Marek Rosa Awesome Inc template Powered Blogger"),
('waifu2x: anime art upscaling and denoising with deep convolutional neural networks', "Skip content Sign Sign repository Explore Features Enterprise Blog Watch 187 Star 2 989 Fork 225 nagadomi waifu2x Code Issues Pull requests Wiki Pulse Graphs HTTPS clone URL Subversion checkout URL clone HTTPS Subversion Download ZIP Image Super-Resolution Anime-Style-Art 36 commits 1 branch 0 releases 2 contributors Lua 82 5 HTML 14 8 Shell 2 7 Lua HTML Shell branch master Switch branches tags Branches Tags master Nothing show Nothing show waifu2x update README latest commit 94629d8b31 nagadomi authored May 26 2015 Permalink Failed load latest commit information appendix add cron script fix run-web sh May 17 2015 assets fix hit May 20 2015 cache first commit May 16 2015 data add slide appendix May 17 2015 images fix ugly chroma scaling May 22 2015 lib fix ugly chroma scaling May 22 2015 models update noise1_model May 17 2015 gitignore update training script May 17 2015 LICENSE add LICENSE NOTICE May 17 2015 NOTICE add LICENSE NOTICE May 17 2015 README md update README May 26 2015 cleanup_model lua first commit May 16 2015 convert_data lua update training script May 17 2015 train lua fix ugly chroma scaling May 22 2015 train sh fix typo 4 May 21 2015 waifu2x lua add resume option May 26 2015 web lua fix ugly chroma scaling May 22 2015 README md waifu2x Image Super-Resolution anime fan-art using Deep Convolutional Neural Networks Demo-Application found http waifu2x udp jp Summary Click see slide show References waifu2x inspired SRCNN 1 2D character picture HatsuneMiku licensed CC BY-NC piapro 2 1 Chao Dong Chen Change Loy Kaiming Xiaoou Tang Image Super-Resolution Using Deep Convolutional Networks http arxiv org abs 1501 00092 2 Creators http piapro net en_for_creators html Public AMI AMI name waifu2x server AMI ID ami-75f01931 Region N California Instance g2 2xlarge require GPU OS Ubuntu 14 04 User ubuntu Dependencies Hardware NVIDIA GPU Compute Capability 3 0 later Platform Torch7 NVIDIA CUDA NVIDIA cuDNN Packages luarocks cutorch cunn cudnn graphicsmagick turbo md5 uuid NOTE Turbo 1 1 3 bug file uploading Please install master branch github Installation Setting Command Line Tool Environment Ubuntu 14 04 Install Torch7 sudo apt-get install curl curl -s https raw githubusercontent com torch ezinstall master install-all sudo bash see Torch easy install Install CUDA cuDNN Google Search keyword install cuda ubuntu install cudnn ubuntu Install packages sudo luarocks install cutorch sudo luarocks install cunn sudo luarocks install cudnn sudo apt-get install graphicsmagick libgraphicsmagick-dev sudo luarocks install graphicsmagick Test waifu2x command line tool th waifu2x lua Setting Web Application Environment needed Install luajit 2 0 4 curl -O http luajit org download LuaJIT-2 0 4 tar gz tar -xzvf LuaJIT-2 0 4 tar gz cd LuaJIT-2 0 4 make sudo make install Install packages Install luarocks packages sudo luarocks install md5 sudo luarocks install uuid Install turbo git clone https github com kernelsauce turbo git cd turbo sudo luarocks make rockspecs turbo-dev-1 rockspec Web Application Please edit first line web lua local ROOT ' path waifu2x dir' Run th web lua View http localhost 8812 Command line tools Noise Reduction th waifu2x lua -m noise -noise_level 1 -i input_image png -o output_image png th waifu2x lua -m noise -noise_level 2 -i input_image png -o output_image png 2x Upscaling th waifu2x lua -m scale -i input_image png -o output_image png Noise Reduction 2x Upscaling th waifu2x lua -m noise_scale -noise_level 1 -i input_image png -o output_image png th waifu2x lua -m noise_scale -noise_level 2 -i input_image png -o output_image png See also images gen sh Video Encoding avconv ffmpeg Ubuntu 14 04 Extracting images audio video range 00 09 00 00 12 00 mkdir frames avconv -i data raw avi -ss 00 09 00 -t 00 03 00 -r 24 -f image2 frames 06d png avconv -i data raw avi -ss 00 09 00 -t 00 03 00 audio mp3 Generating image list find frames -name png sort data frame txt waifu2x example noise reduction mkdir new_frames th waifu2x lua -m noise -noise_level 1 -resume 1 -l data frame txt -o new_frames d png Generating video waifu2xed images audio avconv -f image2 -r 24 -i new_frames d png -i audio mp3 -r 24 -vcodec libx264 -crf 16 video mp4 Training Model Data Preparation Genrating file list find path image dir -name png data image_list txt use PNG case waifu2x trained 3000 high-resolution-beautiful-PNG images Converting training data th convert_data lua Training Noise Reduction level1 model th train lua -method noise -noise_level 1 -test images miku_noisy png th cleanup_model lua -model models noise1_model t7 -oformat ascii check performance model models noise1_best png Training Noise Reduction level2 model th train lua -method noise -noise_level 2 -test images miku_noisy png th cleanup_model lua -model models noise2_model t7 -oformat ascii check performance model models noise2_best png Training 2x UpScaling model th train lua -method scale -scale 2 -test images miku_small png th cleanup_model lua -model models scale2 0x_model t7 -oformat ascii check performance model models scale2 0x_best png Status API Training Shop Blog 2015 GitHub Inc Terms Privacy Security Contact Something went wrong request Please try"),
('Estimating treatment effects and power analysis with R in ipython', "Estimating treatment effects power analysis Outline Sampling distributions standard errors Confidence intervals average treatment effects Power analysis Type errors Type II errors Type M errors difficulty invoking load_ext try pip install rpy2 --upgrade command line restarting Jupyter Python kernel 1 load_ext rpy2 ipython 2 R First time users install packages uncommenting following line install packages c 'dplyr' 'ggplot2' 'foreach' 'lmtest' 'sandwich' 'doMC' library 'plyr' library 'dplyr' library 'ggplot2' library 'foreach' library 'lmtest' library 'sandwich' library 'broom' options digits 3 setwd ' www-15-tutorial' Attaching package dplyr following objects masked package plyr arrange count desc failwith id mutate rename summarise summarize following object masked package stats filter following objects masked package base intersect setdiff setequal union foreach simple scalable parallel programming Revolution Analytics Use Revolution R scalability fault tolerance http www revolutionanalytics com Loading required package zoo Attaching package zoo following objects masked package base Date Date numeric Error setwd www-15-tutorial cannot change working directory Sampling distributions standard errors 3 Sampling distributions standard errors experiment we're interested min difference two averages certainty value average depends number samples dive variation see difference two means let's look variance single variable Consider mean bernoulli random variable flip coin lands 1 probability p 0 probability 1-p flip coin N times observe average number times lands 1 take measurements mean num replications times File ipython-input-3-66a200ba8d9f line 15 SyntaxError invalid syntax Sampling distribution average Hypothetical experiment average number heads biased coin p 0 2 100 coin flips observe simulation average distributed replicate experiment 2000 times called sampling distribution 4 R N - 100 p - 0 2 num replications - 2000 replications - replicate num replications mean rbinom N 1 p print quantile replications c 0 025 0 975 95 interval hist replications xlim c 0 0 4 histogram 2 5 97 5 0 13 0 28 Standard error standard error standard deviation sampling distribution 5 R Alternatively use standard deviation sampling distribution approximately normal sufficiently large N est se - sd replications mean est p - mean replications mean est p est se c mean est p - 1 96 est se mean est p 1 96 est se 1 0 123 0 277 Estimating standard error using math standard deviation k draws Bernoulli distribution approximated sigma sqrt frac p p-1 N standard error experiment 95 confidence interval z_ alpha 2 text SE 1 96 text SE 6 R single trial - rbinom N 1 p p single trial - mean single trial normal approximation SE binomial se - sqrt p single trial 1-p single trial N se 1 0 0384 7 R norm approx 95 CI c p single trial - 1 96 se p single trial 1 96 se 1 0 105 0 255 Relationship N CI width 8 R let's construct confidence intervals p values p N d - expand grid p c 0 05 0 25 0 5 n seq 100 2000 100 d - d mutate se sqrt p 1-p n qplot n p data d color factor p ylab 'p 95 confidence interval ' geom_errorbar aes ymin p-1 96 se ymax p 1 96 se Relationship N CI width 9 R see standard error rapidly diminishes N sqrt 1 n particular qplot n se data d color factor p geom 'line' ylab 'standard error' Sampling distribution average treatment effects Simple experiment Consider simulated experiment binary treatment D y mu D delta epsilon delta average treatment effect ATE 10 R run experiment - function n ate data frame y0 rnorm n mean 10 sd 3 mutate y1 y0 ate D rbinom n 1 0 5 y D y1 1-D y0 experiment - run experiment 2000 2 0 head experiment y0 y1 D y 1 9 28 11 3 1 11 3 2 11 34 13 3 0 11 3 3 8 98 11 0 1 11 0 4 8 41 10 4 1 10 4 5 10 68 12 7 1 12 7 6 12 95 15 0 0 13 0 Distribution outcomes simulated subjects 11 R qplot y geom 'histogram' data experiment facets D theme_bw stat_bin binwidth defaulted range 30 Use 'binwidth x' adjust stat_bin binwidth defaulted range 30 Use 'binwidth x' adjust Sampling distribution estimated ATE 12 R est ate - function exp mean exp exp D 1 y - mean exp exp D 0 y num replications - 2000 ates - replicate num replications est ate run experiment n 1e3 2 0 hist ates main 'sampling distribution ATE' Estimating SEs Using t-test 13 R 95 estimated treated effects lie within range quantile ates c 0 025 0 975 Large sample approximations let us make statements variance ATE different similar populations possible randomizations compute ate experiment experiment test y D 1 y D 0 Error withVisible could find function compute ate Estimating SEs Using linear model 14 R m - lm y D data experiment est ate - coeftest m 'D' 'Estimate' est se - coeftest m 'D' 'Std Error' c est ate-1 96 est se est ate 1 96 est se 1 1 86 2 40 Estimating SEs Using bootstrap 15 R source 'css_stats R' weighted est ate - function exp m - lm y D data exp weights weights coeftest m 'D' 'Estimate' replicates - iid bootstrap experiment weighted est ate print quantile replicates c 0 025 0 975 hist replicates main 'bootstrap distribution ATE single experiment' Loading required package grid Loading required package lattice Loading required package survival Loading required package Formula Attaching package Hmisc following objects masked package dplyr combine src summarize following objects masked package plyr discrete summarize following objects masked package base format pval round POSIXt trunc POSIXt units 2 5 97 5 1 85 2 43 16 R function returns confidence intervals dataframe outcome y binary treatment D use later get ci - function experiment - coeftest lm y D data experiment est ate - 'D' 'Estimate' est se - 'D' 'Std Error' p value - 'D' 'Pr ' return list est ate est ate est se est se p value p value get ci experiment est ate 1 2 13 est se 1 0 139 p value 1 5 47e-50 Type II M errors Type error Finding significant difference difference Type II error Failure find significant difference Type M error Measuring ATE large 17 smaller sample sizes heavily skewed outcomes non-iid data recommended use methods like bootstrap covered later randomization inferenceb robust cluster standard errors Type Type II Type M Errors investigate variation get repeating hypothetical experiment multiple times simulate multiple replications randomized trial Simulating variability replicating experiment 18 R function runs experiment multiple times num replications generates dataframe containing estimates treatment effects standard errors experiment sorted size effect replicate experiment - function sim func params num replications 100 Note parallelize process using dopar 'doMC' package installed exps - foreach replication 1 num replications combine rbind data frame get ci call sim func params exps - exps order exps est ate exps - mutate exps rank 1 nrow exps significant sign est ate-1 96 est se sign est ate 1 96 est se exps Type errors Type error detecting statistically significant effect effect 19 R run 300 trials experiment ATE null experiments - replicate experiment run experiment list n 1e3 ate 0 0 num replications 200 qplot est ate data null experiments main sampling distribution ATE stat_bin binwidth defaulted range 30 Use 'binwidth x' adjust Type errors 5 experiments yield statistically significant results Visualize rank ordering CIs 200 hypothetical experiments effect size 20 R plot results experiments rank-ordered effect size along confidence intervals Experiments significant confidence intervals cross zero highlighted blue print mean null experiments significant qplot rank est ate data null experiments color significant xlab 'experiment number ranked effect size ' ylab 'estimated ATE' main 'demonstration Type errors' geom_pointrange aes ymin est ate-1 96 est se ymax est ate 1 96 est se geom_point 1 0 05 Type II errors Consider experiment true effect 0 5 average 10 Experiments small often detect treatment effect 21 R sad experiments - replicate experiment run experiment list n 250 ate 0 5 num replications 200 qplot rank est ate data sad experiments color significant xlab 'experiment number ranked effect size ' ylab 'estimated ATE' main 'demonstration Type II errors' geom_pointrange aes ymin est ate-1 96 est se ymax est ate 1 96 est se geom_point geom_hline yintercept 0 5 Type M errors Type M error magnitude error experiments underpowered report significant experiments expected overestimate effects 22 R Fraction experiments statistically significant called statistical power experiment print mean sad experiments significant Type M errors experiments underpowered reporting significant experiments massively overstate effects subset sad experiments significant TRUE mean est ate 1 0 24 1 0 965 Power analysis Use analytical expressions simulations determine large experiment Depends Number subjects condition Effect size Variance outcome 23 R run experiment2 - function n ate prop treated 0 5 y sd 3 data frame y0 rnorm n mean 10 sd y sd mutate y1 y0 ate D rbinom n 1 prop treated y D y1 1-D y0 Relationship precision proportion subjects experiment Let's simulate replications varying proportions users assigned treatment control 24 R exps - foreach p seq 0 04 0 96 0 04 combine rbind varying prop list n 1e3 ate 1 prop treated p y sd 3 replications - replicate experiment sim func run experiment2 params varying prop num replications 250 data frame prop treated p sim se sd replications est ate sim ate mean replications est ate norm est se head replications est se 1 Relationship precision proportion subjects experiment Using sampling distribution simulations 25 R qplot prop treated 1 96 sim se data exps ylab '95 CI width' xlab 'proportion subjects treatment' ylim c 0 2 0 geom c 'line' 'point' main 'Precision estimated ATE simulation ' Relationship precision proportion subjects experiment Using estimated standard error single experiment 26 R qplot prop treated 1 96 norm est se data exps ylab '95 CI width' xlab 'proportion subjects treatment' ylim c 0 2 0 main 'Precision estimated ATE approximation ' variance affects required sample size 27 R use stripped version doesn't use replications demonstration purposes using get ci single trial parameterization n - 1e3 true ate - 0 1 p 0 5 exps - foreach n seq 100 1e3 1e2 combine rbind foreach sd c 1 2 3 combine rbind experiment - run experiment2 n n ate 1 prop treated 0 5 y sd sd ses - get ci experiment data frame n n sd sd est ate ses est ate est se ses est se qplot n 1 96 est se data exps color factor sd geom c 'line' 'point' geom_hline aes yintercept true ate linetype 'dashed' Exercise many samples need detect 10 percentage point change bernoulli outcome p 0 33"),
('A really easy way to discover Experts in Machine Learning. We can also discover the latest blogs/opinions/articles/videos shared by these experts & the conversations these experts are engaging in. A one stop shop to get all your content in 35000 deep topics and communities.', 'Articles Magazine Linear Influencers Conversations ARTICLES Magazine Layout Linear Layout INFLUENCERS CONVERSATIONS Search topical articles influencers conversations Search topical articles influencers conversations Tweets rightrelevance Right Relevance Inc 2014 FAQ Blog Team Terms Privacy Contact Create RightRelevance account Discover fresh relevant content interests save interesting articles follow influential experts first share soon-to-be viral content much Cancel account Login Sign RightRelevance Continue Twitter Continue Facebook Continue LinkedIn Discover fresh relevant content interests save interesting articles follow influential experts first share soon-to-be viral content much Sign'),
('Machined Learnings: ICLR 2015 Review', "Machined Learnings AI winter coming Monday May 11 2015 ICLR 2015 Review ambition quality small community ICLR combine make new favorite conference Recent successes speech vision along wave capital billionaire founder-emperors venture capitalists created sense optimism desire attack Artificial Intelligence enthusiasm contagious procedural note use Arxiv review process made easy dialogue reviewers everyone double blind myth nowadays anyway organizers insightful choosing conference name Although referred deep learning conference conference learning representations early days AI e 1960s representations identified critical time representations hand-constructed prohibitively laborious solutions highly specialized particular problems key idea motivating conference use data learning algorithms help us design representations hopefully making resulting representations easier develop broadly applicable Today deep learning e layered nonlinearities trained non-convex optimization techniques leading technology something better arise conference near-term future-proofed selection accepted papers invited talks extremely sensible given context deep learning papers definitely majority also interesting papers leveraging eigensystems spectral methods dictionary learning invited talks diverse entertaining Percy Liang's talk learning latent logical forms semantic parsing excellent example work clearly involves learning representations yet jokingly professed unfamiliarity deep learning talk many good papers check entire schedule caught eye Neural Machine Translation Jointly Learning Align Translate result paper interesting paper also excels example learned representation design process Deep learning merely application highly flexible model classes large amounts data simple Gaussian kernel would solved AI Instead deep learning like rest machine learning navigating delicate balance model complexity data resources subject computational constraints particular data faster GPU would create kinds improvements standard neural encoder decoder architecture mismatch latent vector representation sequence-to-sequence mapping approximated much better approach judiciously increase model complexity manner better matches target Furthermore art knowing alignments important per se inspiration clearly existing SMT systems figuring incorporate alignment-like operations architecture without destroying ability optimize using SGD Kudos authors Note representation learned data clearly human designers gifted system strong prior via specification architecture deep convolutional networks anticipate continue case near future always data impoverished relative complexity hypothesis classes we'd like consider Anybody says I'm using deep learning want learn raw data without making assumptions doesn't get also use phrase universal approximator exit conversation run away fast possible nothing dangerous incorrect intuition expressed high precision c f Minsky NICE Non-linear Independent Components Estimation authors define flexible nonlinearity volume preserving invertible resulting generative model inference training sampling inpainting straightforward It's one tricks that's cool want find use Qualitatively characterizing neural network optimization problems effectiveness SGD somewhat mysterious authors dig optimization landscapes encountered actual neural networks gain intuition talk poster additional cool visualizations paper Structured prediction several papers exploring advance deep neural networks beyond classification structured prediction Combining neural networks CRFs popular choice Chen et al nice poster along lines good results Pascal VOC 2012 Jaderberg et al utilized similar strategy tackle variadic extensible output problem recognizing words natural images Extreme classification several papers proposing methods speed learning classification models number output large Vijayanarasimhan et al attempt parsimoniously approximate dot products using hashing whereas Vincent provides exact expression gradient certain loss functions avoids computing outputs explicitly I'll digging papers next weeks understand better Also theory use label embedding technique avoid output layer entirely training extreme deep classifiers GPU haven't implemented yet YMMV Posted Paul Mineiro 10 07 Email ThisBlogThis Share TwitterShare FacebookShare Pinterest Labels Conference Deep Learning ICLR comments Post Comment Older Post Home Subscribe Post Comments Atom CISL hiring looking full-time employment interface distributed systems large-scale machine learning drop line mbox pmineiro mbox microsoft ldotp mbox com name Paul Mineiro I'm currently employed Microsoft Cloud Information Services Lab CISL mbox paul ldotp mbox mineiro mbox gmail ldotp mbox com Twitter mbox PaulMineiro Blog Archive 2015 6 May 1 ICLR 2015 Review April 2 February 2 January 1 2014 14 December 1 November 1 October 2 September 1 August 1 June 2 May 1 April 1 March 1 February 2 January 1 2013 15 December 2 November 1 October 2 September 1 August 1 July 1 June 2 May 1 April 1 March 1 February 1 January 1 2012 28 December 3 November 1 October 2 August 1 July 4 June 5 May 3 April 3 March 2 February 1 January 3 2011 51 December 4 November 5 October 5 September 2 August 2 July 6 June 8 May 5 April 3 March 5 February 2 January 4 2010 40 December 5 November 3 October 6 September 13 August 11 July 2 MathJax awesome Javascript intensive recommend Safari 5 Chrome IE 10 viewing Subscribe Posts Atom Posts Comments Atom Comments Insanely Great Visual Studio's default key bindings failed last time Hat Tip Thanks Dan Raies Dihedral Soup providing CSS tips tricks AI Winter Coming Powered Blogger"),
('Planning, implementation, and analysis of a Web application using Python, PlanOut, and R (WWW215, with notebooks)', 'Online Experiments Computational Social Science Eytan Bakshy Sean J Taylor 2015 International World Wide Web Conference WWW Abstract tutorial teaches attendees design plan implement analyze online experiments First review basic concepts causal inference motivate need experiments discuss basic statistical tools help plan experiments exploratory analysis power calculations use simulation R discuss statistical methods estimate causal quantities interest construct appropriate confidence intervals Particular attention given scalable methods suitable big data including working weighted data clustered bootstrapping discuss design implement online experiments using PlanOut open-source toolkit advanced online experimentation used Facebook show basic B tests within-subjects designs well sophisticated experiments implemented demonstrate experimental designs social computing literature implemented also review detail two large field experiments conducted Facebook using PlanOut Finally discuss issues logging common errors deployment analysis experiments Attendees given code examples participate planning implementation analysis Web application using Python PlanOut R Slides Notebooks 0 Estimation Power 1 Introduction PlanOut 2 Making Data 3 Analyzing Experiments Code Github Repository'),
('Weak Learning, Boosting, and the AdaBoost algorithm', "Math Programming Navigation Skip content HomeMain ContentPrimersResearchProgram GalleryProof GalleryAbout AuthorSupport Post navigation Math Programming Survey Weak Learning Boosting AdaBoost algorithm Posted May 18 2015 j2kun addressing question means algorithm learn one imagine many different models quite invariably raises question models different along precise description re comparing models ve seen one learning model far called Probably Approximately Correct PAC espouses following answer learning question algorithm solve classification task using labeled examples drawn distribution achieve accuracy arbitrarily close perfect distribution meet goal arbitrarily high probability runtime number examples needed scales efficiently parameters accuracy confidence size example Moreover algorithm needs succeed matter distribution generates examples think game algorithm designer adversary First learning problem fixed everyone involved knows task algorithm designer pick algorithm adversary knowing chosen algorithm chooses nasty distribution examples fed learning algorithm algorithm designer wins algorithm produces hypothesis low error given samples goal prove algorithm designer pick single algorithm extremely likely win matter adversary picks ll momentarily restate precise definition post compare slightly different model called weak PAC-learning model essentially PAC except requires algorithm accuracy slightly better random guessing algorithm output classification function correctly classify random label probability least small fixed quantity Greek eta called edge edge random guessing call algorithm produces hypothesis weak learner contrast ll call successful algorithm usual PAC model strong learner amazing fact strong learning weak learning equivalent course weak learner thing strong learner mean equivalent problem weak-learned strong-learned computationally One direction equivalence trivial strong learner classification task automatically weak learner task reverse much harder crux algorithm transforming weak learner strong learner Informally boost weak learning algorithm feeding examples carefully constructed distributions take majority vote reduction strong weak learning magic happens post ll get depths boosting technique ll review model PAC-learning define means weak learner organically come AdaBoost algorithm intuitive principles prove AdaBoost reduces error training data run data turns despite origin boosting purely theoretical question boosting algorithms wide impact practical machine learning well usual code data used post available blog Github page History multiplicative weights get details bit history context PAC learning introduced Leslie Valiant 1984 laying foundation flurry innovation 1988 Michael Kearns posed question whether one boost weak learner strong learner Two years later Rob Schapire published landmark paper Strength Weak Learnability closing theoretical question providing first boosting algorithm Schapire Yoav Freund worked together next years produce simpler versatile algorithm called AdaBoost won G del Prize one highest honors theoretical computer science AdaBoost also standard boosting algorithm used practice though enough variants warrant book subject m going define prove AdaBoost works post implement test data first want give high level discussion technique afterward goal make wispy intuition rigorous central technique AdaBoost discovered rediscovered computer science recently recognized abstractly right called Multiplicative Weights Update Algorithm MWUA applications everything learning theory combinatorial optimization game theory idea Maintain nonnegative weight elements set Draw random element proportionally weights something chosen element based outcome something Update weights repeat something usually black box algorithm like solve simple optimization problem output something interpreted reward penalty weights updated according severity penalty details done differ depending goal light one interpret MWUA minimizing regret respect best alternative element one could chosen hindsight fact precisely technique used attack adversarial bandit learning problem Exp3 algorithm multiplicative weight scheme See lengthy technical survey Arora Kale research-level discussion algorithm applications let remind formal definition PAC ve read previous post PAC model next section redundant Distributions hypotheses targets PAC-learning trying give labels data set distribution producing data used everything provide data algorithm uses learn measure accuracy every time might get samples algorithm designer know successful learning algorithm work matter unknown function called target concept assigns label data point target function re trying learn algorithm draws example allowed query label use labels seen come hypothesis used new examples algorithm may seen problem solved low error give concrete example let spam emails Say set emails distribution emails get sent personal inbox PAC-learning algorithm would take emails along classification spam spam plus minus 1 algorithm would produce hypothesis used label new emails algorithm truly PAC-learner guarantee high probability randomness emails receive algorithm produce low error entire distribution emails get sent relative personal spam labeling function course practical issues model consistent function calling things spam distribution emails get labeling function change time emails come according distribution independent random draws theoretical model hope algorithms devise model happen work well practice formal definition error hypothesis produced learning algorithm read error respect concept re trying learn distribution probability drawn hypothesis produces wrong label define PAC-learning formally introducing parameters probably approximately Let say informally first algorithm PAC-learns distribution probability least hypothesis produced algorithm error flush things hiding full definition Definition PAC algorithm said PAC-learn concept class set distribution target concept probability produces hypothesis error least symbols Moreover must run time polynomial size element reason need class concepts instead one target concept otherwise could constant algorithm outputs correct labeling function Indeed get problem ask whether exists algorithm solve e problem PAC-learnable algorithm learns described one target concept exist algorithm solve problem hard-coding description concept source code need class possible answers algorithm searching algorithm actually job call algorithm gets guarantee strong learner weak learner definition except replace weak error bound fixed error require algorithm achieve desired accuracy get accuracy slightly better random guessing get choose see value influences convergence boosting algorithm One important thing note constant independent size example number examples particular need avoid degenerate possibility learning problem scales quality weak learner degrades toward 1 2 want bounded away 1 2 clarify parameters floating around always probably part PAC error bound approximately part strong learners error bound weak learners could weak learner prove boost weak learner strong learner idea weak learner Informally rule thumb somehow guarantee little bit better random guessing practice however people sort make things work kind funny recently nobody really studied makes good weak learner use example like one re show long get good error rate care mathematical guarantees Likewise expect final boosted algorithm arbitrarily well want low error rates weak learner ll use post produces decision stumps know decision tree decision stump trivial decision tree whole tree one node know decision tree decision stump classification rule form Pick feature value feature output label input example value feature output label otherwise Concretely decision stump might mark email spam contains word viagra might deny loan applicant loan credit score less number weak learner produces decision stump simply looking features values features finds decision stump best error rate brute force baby Actually ll something little bit different ll make data numeric look threshold feature value split positive labels negative labels Python code ll use post boosting code part collaboration two colleagues Adam Lelkes Ben Fish usual code used post available Github First make class decision stump attributes represent feature threshold value feature choice labels two cases classify function shows simple hypothesis class Stump def __init__ self self gtLabel None self ltLabel None self splitThreshold None self splitFeature None def classify self point point self splitFeature self splitThreshold return self gtLabel else return self ltLabel def __call__ self point return self classify point fixed feature index ll define function computes best threshold value index def minLabelErrorOfHypothesisAndNegation data h posData negData x y x y data h x 1 x y x y data h x -1 posError sum y -1 x y posData sum y 1 x y negData negError sum y 1 x y posData sum y -1 x y negData return min posError negError len data def bestThreshold data index errorFunction '''Compute best threshold given feature Returns threshold error ''' thresholds point index point label data def makeThreshold return lambda x 1 x index else -1 errors threshold errorFunction data makeThreshold threshold threshold thresholds return min errors key lambda p p 1 allow user provide generic error function weak learner tries minimize case minLabelErrorOfHypothesisAndNegation words threshold function label example feature value greater threshold otherwise might want opposite labeling threshold bestThreshold function doesn care wants know threshold value best compute right hypothesis next function def buildDecisionStump drawExample errorFunction defaultError find index best feature split best threshold index labeled example pair example label drawExample accepts arguments returns labeled example data drawExample _ range 500 bestThresholds bestThreshold data errorFunction range len data 0 0 feature thresh _ min bestThresholds key lambda p p 2 stump Stump stump splitFeature feature stump splitThreshold thresh stump gtLabel majorityVote x x data x 0 feature thresh stump ltLabel majorityVote x x data x 0 feature thresh return stump little bit inefficient matter illustrate PAC framework emphasize weak learner needs nothing except ability draw distribution computes best threshold creates new stump reflecting majorityVote function picks common label examples list Note drawing 500 samples arbitrary general might increase increase success probability finding good hypothesis fact proving PAC-learning theorems number samples drawn often depends accuracy confidence parameters omit simplicity Strong learners weak learners suppose weak learner concept class concept produce probability least hypothesis error bound modify algorithm get strong learner idea maintain large number separate instances weak learner run dataset combine hypotheses majority vote code might look like following python snippet examples binary vectors labels sign real number label def boost learner data rounds 100 m len data learners learner random choice data m rounds _ range rounds def hypothesis example return sign sum 1 rounds h example h learners return hypothesis bit simplistic majority weak learners wrong fact overly naive mindset one might imagine scenario different instances high disagreement prediction going depend random subset learner happens get better instead taking majority vote take weighted majority vote give weak learner random subset data test hypothesis data get good estimate error use error say whether hypothesis good give good hypotheses high weight bad hypotheses low weight proportionally error boosted hypothesis would take weighted majority vote hypotheses example might look like following data list example label pairs def error hypothesis data return sum 1 x y data hypothesis x y len data def boost learner data rounds 100 m len data weights 0 rounds learners None rounds range rounds learners learner random choice data m rounds weights 1 - error learners data def hypothesis example return sign sum weight h example h weight zip learners weights return hypothesis might better something even cleverer Rather use estimated error say something hypothesis identify mislabeled examples round somehow encourage better classifying examples later rounds turns key insight algorithm called AdaBoost Ada stands adaptive re adaptively modifying distribution training data feed based data learns easily boosting algorithm runs distribution given probability weight examples misclassified key guarantee weak learn matter distribution data course error also measured relative adaptively chosen distribution crux argument relating error error original distribution re trying strong learn implement idea mathematics start fixed sample drawn assign weight Call true label example Initially set 1 Since dataset repetitions normalizing probability distribution gives estimate ll pick update parameter intentionally vague ll repeat following procedure number rounds Renormalize probability distribution Train weak learner provide simulated distribution draws examples according weights weak learner outputs hypothesis every example mislabeled update replacing every correctly labeled example replace end final hypothesis weighted majority vote weights depend amount error round Note weak learner misclassifies example increase weight example means re increasing likelihood drawn future rounds particular order maintain good accuracy weak learner eventually produce hypothesis fixes mistakes previous rounds Likewise examples correctly classified reduce weights examples easy learn given lower emphasis prize-winning idea elegant powerful easy understand rest working values parameters proving supposed details proof Let jump straight Python program performs boosting First pick data representation Examples pairs whose type tuple object int labels valued Since algorithm entirely black-box need assume anything examples represented dataset list labeled examples weights floats boosting function prototype looks like boost object int learner int - object - int boost given weak learner strong learner def boost examples weakLearner rounds weak learner saw decision stumps following function prototype weakLearner - list label - list - label accept input function draws labeled examples distribution output hypothesis list - label def weakLearner draw return hypothesis Assuming weak learner fill rest boosting algorithm mysterious details First helper function compute weighted error hypothesis exmaples also returns correctness hypothesis example ll use later compute weighted error given hypothesis distribution return hypothesis results error def weightedLabelError h examples weights hypothesisResults h x y x y examples 1 correct else -1 return hypothesisResults sum w z w zip hypothesisResults weights z 0 Next main boosting algorithm draw function accepts input list floats sum 1 picks index proportional weight entry index def boost examples weakLearner rounds distr normalize 1 len examples hypotheses None rounds alpha 0 rounds range rounds def drawExample return examples draw distr hypotheses weakLearner drawExample hypothesisResults error computeError hypotheses examples distr alpha 0 5 math log 1 - error 0001 error distr normalize d math exp -alpha h d h zip distr hypothesisResults print Round d error 3f error def finalHypothesis x return sign sum h x h zip alpha hypotheses return finalHypothesis code almost clear round run weak learner hand-crafted distribution compute error resulting hypothesis distribution update distribution mysterious way depending alphas logs exponentials particular use expression product true label predicted label computed weightedLabelError comment says either depending whether predicted label correct incorrect respectively choice strange logarithms exponentials result optimization allow us minimize training error quickly possible ll see proof follow rest section prove works weak learner correct One small caveat proof assume error hypothesis zero weak learner supposed return perfect hypothesis practice want avoid dividing zero add small 0 0001 avoid quick self-check wouldn stop middle output perfect hypothesis distribution perfect might original distribution wanted define algorithm pseudocode helps proof would write way Given rounds start uniform distribution labeled input examples label Say input examples Let weak learning algorithm run Let error Let Update entry rule chosen normalize distribution Output final hypothesis sign e let prove works ll prove error input dataset training set decreases exponentially quickly number rounds ll run example save generalization error next post many years algorithm tweaked proof straightforward Theorem AdaBoost given weak learner stopped round edge random choice satisfies training error AdaBoost Proof Let number examples given boosting algorithm First derive closed-form expression terms normalization constants Expanding recurrence relation gives starting distribution uniform combining products sum exponents simplifies Next show training error bounded product normalization terms part always seemed strange training error boosting depends factors need normalize distribution different perspective multiplicative weights scheme didn explicitly normalize distribution step d get nonnegative weights could convert distribution sampling step training error would depend product weight updates step Anyway let prove training error defined written indicator function follows sign determines prediction product negative incorrect strange thing re going upper bound indicator function either zero one works predicts correctly indicator function zero exponential greater zero hand incorrect exponential greater one get rearranging formula first part gives Since forms distribution sums 1 factor training error bounded last step bound product normalization factors enough show normalization constant defined sum numerator terms step D e split correct incorrect terms contribute exponent get definition sum incorrect part correct part get Finally since upper bound want pick minimize expression little calculus see chose algorithm pseudocode achieves minimum simplifies Plug get use calculus fact get desired fine dandy says true weak learner training error AdaBoost vanishes exponentially fast number boosting rounds generalization error really care whether hypothesis produced boosting low error original distribution whole training sample started One might expect run boosting rounds eventually overfit training data generalization accuracy degrade However practice case longer boost even get zero training error better generalization tends long time sort mystery ll resolve mystery sequel post ll close showing run AdaBoost real world data adult census dataset adult dataset standard dataset taken 1994 US census tracks number demographic employment features including gender age employment sector etc goal predict whether individual makes 50k per year first lines training set 39 State-gov 77516 Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States 50K 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 13 United-States 50K 38 Private 215646 HS-grad 9 Divorced Handlers-cleaners Not-in-family White Male 0 0 40 United-States 50K 53 Private 234721 11th 7 Married-civ-spouse Handlers-cleaners Husband Black Male 0 0 40 United-States 50K 28 Private 338409 Bachelors 13 Married-civ-spouse Prof-specialty Wife Black Female 0 0 40 Cuba 50K 37 Private 284582 Masters 14 Married-civ-spouse Exec-managerial Wife White Female 0 0 40 United-States 50K perform preprocessing data categorical examples turn binary features see full details github repository post first post-processed lines newlines added data import adult train test adult load train 3 39 1 0 0 0 0 0 1 0 0 13 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 2174 0 40 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -1 50 1 0 1 0 0 0 0 0 0 13 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 13 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -1 38 1 1 0 0 0 0 0 0 0 9 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 40 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -1 run boosting training data compute error test data boosting import boost data import adult decisionstump import buildDecisionStump train test adult load weakLearner buildDecisionStump rounds 20 h boost train weakLearner rounds Round 0 error 0 199 Round 1 error 0 231 Round 2 error 0 308 Round 3 error 0 380 Round 4 error 0 392 Round 5 error 0 451 Round 6 error 0 436 Round 7 error 0 459 Round 8 error 0 452 Round 9 error 0 432 Round 10 error 0 444 Round 11 error 0 447 Round 12 error 0 450 Round 13 error 0 454 Round 14 error 0 505 Round 15 error 0 476 Round 16 error 0 484 Round 17 error 0 500 Round 18 error 0 493 Round 19 error 0 473 error h train 0 153343 error h test 0 151711 isn shabby ve tried running boosting rounds hundred error doesn seem improve much implies finding best decision stump weak learner least fails dataset see indeed training errors across rounds roughly tend 1 2 Though compared results baseline AdaBoost seems work pretty well kind meta point theoretical computer science research One spends years trying devise algorithms work theory finding conditions get good algorithms theory comes practice anything hope algorithms work well kind amazing something like Boosting works practice clear weak learners exist even given real world problem results speak Next time Next time ll get bit deeper theory boosting ll derive notion margin quantifies confidence boosting prediction ll describe maybe prove theorem says minimum margin AdaBoost training data large generalization error AdaBoost entire distribution small notion margin actually quite deep one shows another famous machine learning technique called Support Vector Machine fact part recent research ve working well future next time Share Share FacebookClick share Google Click share RedditClick share TwitterMoreClick email friendClick share PinterestClick share LinkedInClick share TumblrLike Like Loading Related entry posted Algorithms Learning Theory Probability Theory Bookmark permalink Post navigation Math Programming Survey Leave Reply Cancel reply Enter comment Fill details click icon log Email required Address never made public Name required Website commenting using WordPress com account Log Change commenting using Twitter account Log Change commenting using Facebook account Log Change commenting using Google account Log Change Cancel Connecting Notify new comments via email Notify new posts via email Github Repository Medium blog Twitter Feed Facebook Group Google Community Friends Adventures Analysis Baking Math Logical Tightrope RSSRSS - Posts Create free website blog WordPress com Confit Theme Send Email Address Name Email Address Cancel Post sent - check email addresses Email check failed please try Sorry blog cannot share posts email d bloggers like"),
('Time-lapse Mining from Internet Photos', "Skip navigation UploadSign inSearch Loading Close Yeah keep Undo Close video unavailable Watch QueueTV QueueWatch QueueTV Queue Remove allDisconnect Loading Watch Queue TV Queue __count__ __total__ Time-lapse Mining Internet Photos SIGGRAPH 2015 Ricardo Martin Brualla youtube SubscribeSubscribedUnsubscribe287 Subscription preferences Loading Loading Working Add Want watch later Sign add video playlist Sign Share Report Need report video Sign report inappropriate content Sign 1 226 316 2 570 Like video Sign make opinion count Sign 2 571 106 Don't like video Sign make opinion count Sign 107 Loading Loading Loading Rating available video rented feature available right Please try later Published May 16 2015We introduce approach synthesizing time-lapse videos ofpopular landmarks large community photo collections Theapproach completely automated leverages vast quantityof photos available online First cluster 86 million photos intolandmarks popular viewpoints sort photos bydate warp photo onto common viewpoint Finally westabilize appearance sequence compensate lightingeffects minimize flicker resulting time-lapses show diversechanges world popular sites like glaciers shrinking skyscrapers constructed waterfalls changing course Project website http grail cs washington edu projec Category Science Technology License Standard YouTube License Show Show less Loading Autoplay autoplay enabled suggested video automatically play next Next Birth 13 years 3 min 30 sec Time Lapse Lotte Original - Duration 3 32 Hofmeester 2 641 963 views 3 32 Play nextPlay Three Years Time Lapse growing hair - Duration 4 25 AndreasSheiLT 421 441 views 4 25 Play nextPlay 5 18 14 Wright Newcastle WY Supercell Time-Lapse - Duration 2 02 BasehuntersChasing 18 210 756 views 2 02 Play nextPlay Minecraft Timelapse Four Pillar Survival - Duration 26 05 LazerLord10 268 381 views 26 05 Play nextPlay Gold Mining Documentary - Duration 1 07 56 Gold Documentary 301 views 1 07 56 Play nextPlay Tutorial Motion Timelapse Milky Way Dynamic Perception Stage One Stage R - Duration 20 24 Ian Norman 166 180 views 20 24 Play nextPlay Dyna Model Dynamic Human Shape Motion SIGGRAPH 2015 - Duration 7 08 Michael Black 6 349 views 7 08 Play nextPlay IMMERSIVE MOD EVER - Skyrim Mods - Week 171 - Duration 11 11 MMOxReview 507 596 views 11 11 Play nextPlay Liebherr - 282 C Ship Site Timelapse - Duration 4 45 Liebherr 23 026 views 4 45 Play nextPlay Chevy Small-Block Rebuild Time-lapse - Duration 3 49 Hagerty Classic Cars 1 521 771 views 3 49 Play nextPlay Amazing Time-Lapse Bees Hatch Eyes - Duration 1 09 National Geographic 254 594 views 1 09 Play nextPlay Pioneering Underground Mining - Duration 8 48 JoyMiningMachinery 283 736 views 8 48 Play nextPlay SIGGRAPH 2013 - Implicit Skinning Real-Time Skin Deformation Contact Modeling - Duration 5 00 Teturarsyra 155 291 views 5 00 Play nextPlay Drawing time lapse bag M M's - hyperrealistic art - Duration 3 22 Marcello Barenghi 2 455 124 views 3 22 Play nextPlay Manhattan Project HD1080P Time-lapse - Duration 4 58 Cameron Michael 514 657 views 4 58 Play nextPlay Skyrim Maximum Graphics Overhaul 2015 vs Vanilla Comparison WQHD 1440p - Duration 6 00 Candyland 58 570 views 6 00 Play nextPlay Portrait Lotte - 0 14 years 4 min Original - Duration 4 06 Hofmeester 28 196 783 views 4 06 Play nextPlay NEXT-GEN GRAPHICS - Skyrim Mods - Week 161 - Duration 8 46 MMOxReview 839 058 views 8 46 Play nextPlay Real-Time Hyperlapse Creation via Optimal Frame Selection - Duration 5 00 Neel Joshi 2 556 views 5 00 Play nextPlay NORWAY - Time-Lapse Adventure 4K - Duration 5 33 Rustad Media 1 350 658 views 5 33 Play nextPlay Loading suggestions Show Language English Country India Safety History Help Loading Loading Loading Press Copyright Creators Advertise Developers YouTube Terms Privacy Policy Safety Send feedback Try something new Loading Working Sign add Watch Later Add Loading playlists"),
('Automatic Inference of Search Patterns for Taint-Style Vulnerabilities by Fabian Yamaguchi, Alwin Maier, Hugo Gascon, and Konrad Rieck [PDF]', "IEEE Computer Society's Technical Committee Security Privacy apologizes reached page fault page you've requested exist 404"),
('Simple trick to speedup dropout: Efficient batchwise dropout training using submatrices', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1502 02478 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context cs NE prev next new recent 1502Change browse cs cs CV References CitationsNASA ADS DBLP - CS Bibliography listing bibtex Ben Graham Jeremy Reizenstein Leigh Robinson Bookmark Computer Science Neural Evolutionary Computing Title Efficient batchwise dropout training using submatrices Authors Ben Graham Jeremy Reizenstein Leigh Robinson Submitted 9 Feb 2015 Abstract Dropout popular technique regularizing artificial neural networks Dropout networks generally trained minibatch gradient descent dropout mask turning units---a different pattern dropout applied every sample minibatch explore simple alternative dropout mask Instead masking dropped units setting zero perform matrix multiplication using submatrix weight matrix---unneeded hidden units never calculated Performing dropout batchwise one pattern dropout used sample minibatch substantially reduce training times Batchwise dropout used fully-connected convolutional neural networks Subjects Neural Evolutionary Computing cs NE Computer Vision Pattern Recognition cs CV Cite arXiv 1502 02478 cs NE arXiv 1502 02478v1 cs NE version Submission history Benjamin Graham view email v1 Mon 9 Feb 2015 13 29 48 GMT 36kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('Deep Reinforcement Learning (ICLR2015, David Silver, Google DeepMind) [x-post /r/artificial]', "Skip navigation UploadSign inSearch Loading Close Yeah keep Undo Close video unavailable Watch QueueTV QueueWatch QueueTV Queue Remove allDisconnect Loading Watch Queue TV Queue __count__ __total__ ICLR2015-david-silver-part1 ICLR SubscribeSubscribedUnsubscribe662 Subscription preferences Loading Loading Working Add Want watch later Sign add video playlist Sign Share Report Need report video Sign report inappropriate content Sign Transcript Statistics 1 587 21 Like video Sign make opinion count Sign 22 1 Don't like video Sign make opinion count Sign 2 Loading Loading Transcript interactive transcript could loaded Loading Loading Rating available video rented feature available right Please try later Published May 18 2015ICLR 2015 Invited Talk David Silver Google DeepMind Deep Reinforcement Learning Category Science Technology License Standard YouTube License Show Show less Loading Autoplay autoplay enabled suggested video automatically play next Next ICLR2015-david-silver-part2 - Duration 9 52 ICLR 301 views 9 52 Play nextPlay ICLR2015-terry-sejnowski-part1 - Duration 33 26 ICLR 348 views 33 26 Play nextPlay ICLR2015-max-jaderberg - Duration 20 40 ICLR 75 views 20 40 Play nextPlay ICLR2015-karen-simonyan - Duration 19 29 ICLR 66 views 19 29 Play nextPlay 178 videos Play Play Ren stimpy spongebob production Musicby TVkingOmega ICLR2015-junhua-mao - Duration 16 34 ICLR 94 views 16 34 Play nextPlay ICLR2015-nicolas-vasilache - Duration 16 52 ICLR 38 views 16 52 Play nextPlay Software Inc Part 1 Easy Route - Duration 38 27 Lathland 207 993 views 38 27 Play nextPlay Xperia Z3 - Embrace power explore 2 days battery life - Duration 1 33 Sony Xperia 292 320 views 1 33 Play nextPlay iPhone 6S's Best New Feature - Duration 2 24 CollegeHumor 1 016 551 views 2 24 Play nextPlay WEIRDEST PICTURE INTERNET - Duration 4 22 PewDiePie 2 368 956 views 4 22 Play nextPlay Yuphoria Launch Live Webcast - Duration 1 33 03 YU PlayGod 60 863 views 1 33 03 Play nextPlay Language English Country India Safety History Help Loading Loading Loading Press Copyright Creators Advertise Developers YouTube Terms Privacy Policy Safety Send feedback Try something new Loading Working Sign add Watch Later Add Loading playlists"),
("'Is unsupervised pre-training still useful given recent advances? If so, when?' (ICLR 2015 workshop paper)", 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1412 6597 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context cs CV prev next new recent 1412Change browse cs cs LG cs NE References CitationsNASA ADS DBLP - CS Bibliography listing bibtex Tom Le Paine Pooya Khorrami Wei Han Thomas Huang Bookmark Computer Science Computer Vision Pattern Recognition Title Analysis Unsupervised Pre-training Light Recent Advances Authors Tom Le Paine Pooya Khorrami Wei Han Thomas Huang Submitted 20 Dec 2014 v1 last revised 10 Apr 2015 version v4 Abstract Convolutional neural networks perform well object recognition number recent advances rectified linear units ReLUs data augmentation dropout large labelled datasets Unsupervised data proposed another way improve performance Unfortunately unsupervised pre-training used state-of-the-art methods leading following question unsupervised pre-training still useful given recent advances answer three parts 1 develop unsupervised method incorporates ReLUs recent unsupervised regularization techniques 2 analyze benefits unsupervised pre-training compared data augmentation dropout CIFAR-10 varying ratio unsupervised supervised samples 3 verify findings STL-10 discover unsupervised pre-training expected helps ratio unsupervised supervised samples high surprisingly hurts ratio low also use unsupervised pre-training additional color augmentation achieve near state-of-the-art performance STL-10 Comments Accepted workshop contribution ICLR 2015 Subjects Computer Vision Pattern Recognition cs CV Learning cs LG Neural Evolutionary Computing cs NE Cite arXiv 1412 6597 cs CV arXiv 1412 6597v4 cs CV version Submission history Tom Paine view email v1 Sat 20 Dec 2014 04 20 55 GMT 1040kb D v2 Tue 27 Jan 2015 22 03 40 GMT 1039kb D v3 Mon 2 Mar 2015 21 05 34 GMT 1046kb D v4 Fri 10 Apr 2015 21 26 31 GMT 1046kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('Implementing the Logistical Regression Classifier', "Toggle navigation nbviewer FAQ IPython Jupyter View GitHub Download Notebook logistic-regression logistic-regression ipynb 1 matplotlib inline numpy import exp log linspace import matplotlib import matplotlib pyplot plt Let's Write Logistical Regression Classifier Introduction Logistical regression classifiers well-known machine learning technique classifying data notebook we'll go basics work write simple implementation Finally we'll test implementation using detect spam corpus email Throughout notebook we'll use spam classification convenient example help explain classifier works Resources notebook code uses found github Prerequisites get notebook you'll need following prerequisites Python numpy Basic calculus sums series Basic linear algebra vectors matrices dot product Basics numerical optimization finding minimum point functions Machine learning basics test training sets cost functions supervised learning precision recall Logistical Regression Classifier logistical regression classifier gets name function core logistical sigmoid function h x 1 1 e - bx b mathbb R function interesting shape 2 x linspace -15 15 100 0 b 1 plt plot x 1 1 exp -b x plt show function varies 0 1 close one either extreme domain suggests way using function classifier - find way representing problem points x axis use function put point one two classes either 0 1 point x h x 0 5 say x class 0 h x 0 5 say x class 1 call 0 5 decision boundary example consider classifying spam We'd like use sigmoid function h x classify message spam not-spam need find way mapping message onto X-axis sigmoid function predict class method falls One mapping could length longest capitalized word message Spammers sometimes use capitalized words get attention expect spam messages longer capitalized words take length longest capitalized word use message's position X-axis good start won't quite work decision boundary set 0 5 occurs x 0 Since can't capitalized word negative length strategy would classify almost messages spam solve modify parameters sigmoid funtion b shift function left right setting parameter 3 5 b 1 plt plot x 1 1 exp -b x plt show decision boundary 5 instead 0 message capitalized word length greater 5 classified spam changing parameters sigmoid function we've changed classify emails Generalizing Multiple Dimensions discussion chose length longest capitalized word determine message spam However many features email messages used example email address originate presence word free money presence symbols length message etc Later notebook we'll use logistical regression classifier classify messages 57 features need find way generalizing approach many dimensions Assume process email message extract vector real-valued features Call vector vec x use multidimensional feature vector sigmoid function need find way projecting onto one dimension Linear algebra provides us way performing projection operation dot product need another vector call vec theta order let vec x x_0 x_1 x_2 x_m vec theta p_0 p_1 p_2 p_m m mathbb N number features we've extracted dot product vec x cdot vec theta x_0p_0 x_1p_1 x_2p_2 x_mp_m fix x_0 1 looks similar exponential term sigmoid function rewrite h vec x vec theta 1 1 e vec x cdot vec theta h vec x vec theta 1 1 e p_0 x_1p_1 x_2p_2 x_mp_m Set m 1 becomes h vec x vec theta 1 1 e p_0 x_1p_1 identical original sigmoid function parameters p_0 b -p_1 given parameter vector vec theta email message classify email extracting feature vector vec x message performing dot product operation vec theta passing result sigmoid function checking side decision boundary output It's clear given new feature vector vec x way classify depends entirely values vec theta classification model completely defined values parameter vector vec theta Choosing Parameters Following conventions set call vec theta parameter vector problem becomes choosing values vec theta previous discussion chose parameters b guesswork However multiple dimensions large training sets need algorithmic way choose values vec theta Choosing parameters called training classifier logistical regression classifier supervised algorithm means order train algorithm must provide training set feature vectors vec x_i mid 1 n set labels vec y_i mid y_i 0 1 forall 1 n indicate correct class place vec x_i Training algorithm depends concept cost function following Given parameter vector vec theta feature vector vec x_i training set Make prediction sigmoid model h vec x_i vec theta Compare prediction correct label vec x_i prediction incorrect return high value cost prediction correct return low value cost train classifier use numerical methods iteratively find values vec theta minimize cost function cost function logistical regression classifier J vec theta X vec y -1 n sum_ 1 n y_i log h vec x_i vec theta 1 - y_i log 1 - h vec x_i vec theta lambda sum_ j 1 m theta_j 2 n mathbb N number training vectors training set X n x m matrix containing n vectors training set m features vec x_i th vector training set y_i class label x_i h vec x vec theta sigmoid function used model vec theta parameter vector lambda regularization parameter way derive function properties make useful cost function outside scope discussion However see need y_i 1 h vec x_i vec theta close 1 first second terms sum close 0 cost J low y_i 1 h vec x_i vec theta close 0 first term becomes large cost high y_i 0 h vec x_i vec theta close 1 second term becomes large cost high y_i 0 h vec x_i vec theta close 0 terms approach 0 cost low finding cost training example vec x_i summing get total cost vec theta entire training set final term cost function lambda sum_ j 1 m theta_j 2 used regularize cost Regularization prevents parameters vec theta becoming large penalizing large values strategy tends improve accuracy classifier test set prevents classifier overfitted training set parameter lambda mathbb R used control degree regularization called regularization parameter need find values vec theta minimize cost function J problem minimizing functions well-studied branch mathematics many libraries us chosen cost function J scipy optimize library automatically find best value vec theta thereby training classifier Vectorizing Cost Function There's one important thing need training classifier - need vectorize function J function J wrote performs operation sums entire training set various optimizations numpy scipy libraries depend using matrices vectors express cost function terms matrix vector operations take advantage optimizations write cost function J vec theta X vec y -1 n vec y ' cdot log h X vec theta vec ' - vec y ' cdot log vec - h X vec theta lambda vec theta cdot vec theta ' vec unit vector vector ones ' symbol represents transpose vector log h functions applied element vectors operate parameters code included notebook implements algorithm way Putting Together we're ready look code 3 components classifier algorithm prediction function cost function optimization function Implementations components found src logistic py predict function takes matrix feature vectors X parameter vector vec theta mulitples feature matrix parameter vector applies sigmoid function element resulting vector creates vector n predictions n feature vectors X cost function computes cost parameter vector vec theta training set X vec y train function finds parameter vector minimizes cost function train classifier pass training set X vec y initial guess model parameters vec theta train function optimization routines scipy optimize find parameter vector minimizes cost function training set X vec y Classifying Spam test classifier we'll use well-known UCI Machine Learning Repository Spambase Data Set data set constructed extracting feature vectors 57 dimension 4600 emails email manually labelled spam not-spam making perfect purposes download data set run download method src spambase py file 4 src import spambase spambase download downloading spam data set download complete unzipping data complete use read_data function src spambase py get read X vec y split train tests sets generate initial value vec theta 5 theta Xtrain Xtest ytrain ytest spambase read_data train classifier using train function src logistic py 6 src import logistic trained_theta logistic train theta Xtrain ytrain Optimization terminated successfully Current function value 0 615512 Iterations 5 Function evaluations 660 Gradient evaluations 11 trained_theta defines model use make predictions test accuracy classifier generate set predictions Xtest check accuracy precision ytest 7 numpy import matrix array predictions logistic predict matrix trained_theta transpose Xtest recall 0 5 decision boundary predictions 1 p 0 5 else 0 p predictions make list tuples tuple contains predicted class labeled class predictions zip predictions 0 ytest tolist print predictions 10 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 1 1 1 calculate precision recall classifier 8 tp len 1 x predictions x 0 1 x 0 x 1 fp len 1 x predictions x 0 1 x 0 x 1 precision float tp float tp float fp tn len 1 x predictions x 0 0 x 0 x 1 fn len 1 x predictions x 0 0 x 0 x 1 recall float tp float tp float fn print Precision 0 format precision print Recall 0 format recall Precision 0 916955017301 Recall 0 864600326264 messages marked spam 91 actually spam spam messages training set found 87 bad first try Next Steps many improvements could make algorithm many details ommitted discussion sake brevity Choosing Initial Values vec theta src spambase py initial values vec theta chosen normally distributed 0 done prevent cost function finding initial cost Inf -Inf possible way cost uses logarithm initializing vec theta around 0 ensure values passed logarithm manageable function run away ways dealing problem implement avoid complexity purpose exercise learn implement basics logistical regression classifier deal possible edge cases Gradient optimization routine used train classifier scipy optimize fmin_cg implementation gradient descent seeks optimal solution following gradient derivative multiple dimensions minimal point algorithm depends able calculate gradient point performance improved finding gradient function passing fprime parameter Since fmin_cg function forced numerically estimate gradient point hurt performance Better Implementations It's important note code included notebook meant basic implementation logistical regression classifier use code anything instruction you're looking classifier use production use one standard implementations sklearn linear_model LogisticRegression Comparisons Classifiers discussion say anything logistical regression classifier compares classifiers naive bayes support vector machine Different algorithms perform better different problems evaluate several choosing one solution problem working Conclusion notebook went basics logistical regression classifier works wrote simple implementation evaluated performance spam classification problem found performed fairly well many improvements could made hope useful basic introduction concepts machine learning classification Thanks reading Appendix Resources Machine Learning Coursera https www coursera org learn machine-learning Introduction Statistical Learning http www-bcf usc edu gareth ISL getbook html Scikit-learn's implementation logistic regression classifier http scikit-learn org stable modules generated sklearn linear_model LogisticRegression html Wikipedia http en wikipedia org wiki Logistic_regression Meta Author Matthew Fournier Website http mfournier ca First Published May 18 2015 Back top web site host notebooks renders notebooks available websites Delivered Fastly Rendered Rackspace nbviewer GitHub repository nbviewer version f399a06 IPython version 3 2 0-dev 5e57377 Rendered Thu 28 May 2015 17 50 51 UTC"),
('We are ignoring the new machine age at our peril | The Guardian - Comment is free', "Close Skip main content free become member sign subscribe search jobs dating guardian jobs dating masterclasses join us membership subscribe change edition switch US edition switch AU edition switch INT beta edition UK edition switch US edition switch Australia edition switch International beta Guardian Winner Pulitzer prize 2014 home opinion columnists home UK world politics sport football opinion selected culture business lifestyle fashion environment tech travel browse sections close Internet Comment free ignoring new machine age peril John Naughton Could sudden shifts technology soon coming taking jobs Driven study suggests huge numbers skilled jobs could threatened technological advances Photograph Justin Sullivan Getty Images Sunday 17 May 2015 09 15 BST Last modified Sunday 17 May 2015 10 07 BST Share Facebook Share Twitter Share via Email Share LinkedIn Share Google Share WhatsApp species seem good dealing nonlinearity cope moderately well situations environments changing gradually sudden major discontinuities people call tipping points leave us spooked perversely relaxed climate change example things changing slowly imperceptibly almost far hasn kind sharp catastrophic change would lead us seriously recalibrate behaviour attitudes information technology know indeed become cliche computing power doubling least every two years since records things began know amount data generated digital existence expanding annually astonishing rate know capacity store digital information increasing exponentially apparently sussed however various strands technological progress unconnected Quite contrary therein lies problem thinker done explain consequences connectedness Belfast man named W Brian Arthur economist youngest person ever occupy endowed chair Stanford University later years associated Santa Fe Institute one world leading interdisciplinary research institutes 2009 published remarkable book Nature Technology formulated coherent theory technology evolves spurs innovation industry Technology argued builds organically ways resemble chemistry even organic life implicit Arthur conception technology idea innovation linear mathematicians call combinatorial ie one driven whole bunch things significant point combinatorial innovation brings radical discontinuities nobody could anticipated recent years ve begun see results information technology dramatic case probably self-driving car development us failed predict made possible sudden conjunction whole lot different technologies include near-infinite computing power provided Moore law precise digital mapping GPS developments laser infrared sensor technology machine-learning algorithms plus availability massive data-sets train Put together using kind skilled engineering resources possessed company Google get self-driving car implications vehicle stretch far beyond future automobile industry even future transport signals vast swaths human activity employment hitherto regarded beyond reach intelligent machines may susceptible automation need revise assumptions future work light combinatorial innovation Last September Dr Carl Benedikt Frey Michael Osborne two researchers Martin School Oxford published results major study susceptibility jobs new kind automation report Future Employment Susceptible Jobs Computerisation makes pretty sobering read Frey Osborne used machine-learning techniques estimate probability computerisation 702 detailed occupations based US government classifications occupations conclusion 47 total US employment risk technologies operational laboratories field lots technical argument methodology algorithms used Oxford study little doubt main thrust research accurate lots non-routine cognitive white-collar well blue-collar jobs going eliminated next two decades need planning contingency won course two reasons first politicians pay attention anything time-horizon longer five-year electoral cycle second innate inability handle nonlinear change ve always able absorb mechanisation automation past response challenge technology Automation always created jobs destroyed true past innovation incremental society time absorb respond shock new Combinatorial innovation different kettle fish feeds grows exponentially Given re bound lose race machine isn time began thinking might harness improve quality lives rather merely enrich corporations comment Topics Internet Google Employment Share Facebook Share Twitter Share via Email Share LinkedIn Share Google Share WhatsApp View comments comments Sign create Guardian account join discussion discussion closed comments re maintenance right still read comments please come back later add Commenting disabled account Order newest oldest Show 25 25 50 100 Threads collapsed expanded unthreaded Loading comments Trouble loading View comments popular Guardian back top home UK world politics sport football opinion selected culture business lifestyle fashion environment tech travel sections close home UK education media society law scotland wales northern ireland politics world europe US americas asia australia africa middle east cities development sport football cricket rugby union F1 tennis golf cycling boxing racing rugby league US sports football live scores tables competitions results fixtures clubs opinion selected columnists culture film tv radio music games books art design stage classical business economics banking retail markets eurozone lifestyle food health fitness love sex family women home garden fashion environment climate change wildlife energy pollution tech travel UK europe US money property savings pensions borrowing careers science professional networks observer today's paper editorials letters obituaries g2 weekend guide saturday review membership crosswords video Opinion Internet membership jobs dating masterclasses subscribe topics contributors us contact us complaints corrections terms conditions privacy policy cookie policy securedrop 2015 Guardian News Media Limited affiliated companies rights reserved"),
("ICLR 2015's videos", "Skip navigation UploadSign inSearch Loading Close Yeah keep Undo Close video unavailable Watch QueueTV QueueWatch QueueTV Queue Remove allDisconnect Loading Watch Queue TV Queue __count__ __total__ Sign YouTube Sign ICLR 2015 ICLR 115 videos ICLR2015-luke-vilnis ICLR 2 ICLR2015-antoine-bordes-part2 ICLR 3 ICLR2015-junhua-mao ICLR 4 ICLR2015-antoine-bordes ICLR 5 ICLR2015-david-silver-part1 ICLR 6 ICLR2015-david-silver-part2 ICLR 7 ICLR2015-max-jaderberg ICLR 8 ICLR2015-karen-simonyan ICLR 9 ICLR2015-nicolas-vasilache ICLR 10 ICLR2015-terry-sejnowski-part1 ICLR 11 ICLR2015-terry-sejnowski-part2 ICLR 12 ICLR2015-joerg-bornschein ICLR 13 ICLR2015-olivier-henaff ICLR 14 ICLR2015-percy-liang-part1 ICLR 15 ICLR2015-percy-liang-part2 ICLR ICLR2015-luke-vilnis ICLR SubscribeSubscribedUnsubscribe662 Subscription preferences Loading Loading Working Add Want watch later Sign add video playlist Sign Share Report Need report video Sign report inappropriate content Sign Transcript Statistics 470 views 3 Like video Sign make opinion count Sign 4 0 Don't like video Sign make opinion count Sign 1 Loading Loading Transcript interactive transcript could loaded Loading Loading Rating available video rented feature available right Please try later Published May 18 2015ICLR 2015 Talk Word Representations via Gaussian Embedding Luke Vilnis Andrew McCallum Brown University Category Science Technology License Standard YouTube License Show Show less Loading ICLR2015-david-silver-part1 - Duration 33 26 ICLR 1 448 views 33 26 Play nextPlay ICLR2015-david-silver-part2 - Duration 9 52 ICLR 301 views 9 52 Play nextPlay ICLR2015-junhua-mao - Duration 16 34 ICLR 94 views 16 34 Play nextPlay ICLR2015-max-jaderberg - Duration 20 40 ICLR 75 views 20 40 Play nextPlay ICLR2015-karen-simonyan - Duration 19 29 ICLR 66 views 19 29 Play nextPlay ICLR2015-antoine-bordes - Duration 33 26 ICLR 163 views 33 26 Play nextPlay ICLR2015-percy-liang-part1 - Duration 33 26 ICLR 99 views 33 26 Play nextPlay ICLR2015-antoine-bordes-part2 - Duration 4 36 ICLR 133 views 4 36 Play nextPlay ICLR2015-percy-liang-part2 - Duration 4 31 ICLR 36 views 4 31 Play nextPlay Software Inc Part 1 Easy Route - Duration 38 27 Lathland 207 993 views 38 27 Play nextPlay Xperia Z3 - Embrace power explore 2 days battery life - Duration 1 33 Sony Xperia 292 320 views 1 33 Play nextPlay iPhone 6S's Best New Feature - Duration 2 24 CollegeHumor 1 016 551 views 2 24 Play nextPlay WEIRDEST PICTURE INTERNET - Duration 4 22 PewDiePie 2 368 956 views 4 22 Play nextPlay Yuphoria Launch Live Webcast - Duration 1 33 03 YU PlayGod 60 863 views 1 33 03 Play nextPlay Language English Country India Safety History Help Loading Loading Loading Press Copyright Creators Advertise Developers YouTube Terms Privacy Policy Safety Send feedback Try something new Loading Working Sign add Watch Later Add Loading playlists"),
('Nvidia says Pascal GPUs to arrive next year with 10x speedup for deep learning', 'USA - United States USA - United States ARG - Argentina BRA - BrasilCHL - ChileCHN - ChinaCLM - ColombiaDEU - GermanyESP - SpainFRA - FranceGBR - United KingdomIND - IndiaITA - ItalyJPN - JapanKOR - KoreaMEX - MexicoPOL - PolandRUS - RussiaTWN - TaiwanTHA - ThailandTUR - TurkeyUSA - United StatesVEN - VenezuelaChange default amp lt div id globalSelector amp gt amp lt div class currentTitle amp gt amp lt href http www nvidia com content global unset php amp gt USA - United States amp lt amp gt amp lt div amp gt amp lt div amp gt Drivers GeForce Drivers NVIDIA Drivers Products Processors GeForce Quadro Tegra Tesla NVIDIA GRID NVS Legacy Technologies Advanced Rendering CUDA Deep Learning G-SYNC Machine Learning Multi-GPU NVLink PhysX Optimus Optix SLI Technologies NVIDIA GRID Virtual Desktops Apps Cloud Gaming Quadro VCA NVIDIA DRIVE 3D Vision Platforms Desktops Notebooks Tablets Smartphones Workstations Servers High Performance Computing Automotive Embedded SHIELD Communities GeForce com TegraZone com GPU Technology Conference NVIDIA Partner Network PartnerForce NVIDIA Forums GRID Forums CUDA Zone Developer Zone NVIDIA Research 3D Vision Live GPU Venture Zone NVIDIA Blog Social Media Facebook Flickr Google Instagram LinkedIn Tumblr Twitter YouTube Support Shop NVIDIA Company Information Newsroom NVIDIA Blog Investors Citizenship Visual Computing Careers Blog Menu Home Auto Corporate Gaming Mobile Enterprise Cloud Explore Blogs 7 Corporate NVIDIA Next-Gen Pascal GPU Architecture Provide 10X Speedup Deep Learning Apps Ian Buck March 17 2015 NVIDIA Pascal GPU architecture set debut next year accelerate deep learning applications 10X beyond speed current-generation Maxwell processors NVIDIA CEO co-founder Jen-Hsun Huang revealed details Pascal company updated processor roadmap front crowd 4 000 keynote address GPU Technology Conference Silicon Valley benefit billion dollars worth refinement R D done last three years told audience rise deep learning process computers use neural networks teach led NVIDIA evolve design Pascal originally announced last year GTC Pascal GPUs three key design features result dramatically faster accurate training richer deep neural networks human cortex-like data structures serve foundation deep learning research Along 32GB memory 2 7X newly launched NVIDIA flagship GeForce GTX TITAN X Pascal feature mixed-precision computing 3D memory resulting 5X improvement deep learning applications feature NVLink NVIDIA high-speed interconnect links together two GPUs lead total 10X improvement deep learning Pascal offer better performance Maxwell key deep-learning tasks Mixed-Precision Computing Greater Accuracy Mixed-precision computing enables Pascal architecture-based GPUs compute 16-bit floating point accuracy twice rate 32-bit floating point accuracy Increased floating point performance particularly benefits classification convolution two key activities deep learning achieving needed accuracy 3D Memory Faster Communication Speed Power Efficiency Memory bandwidth constraints limit speed data delivered GPU introduction 3D memory provide 3X bandwidth nearly 3X frame buffer capacity Maxwell let developers build even larger neural networks accelerate bandwidth-intensive portions deep learning training Pascal memory chips stacked top placed adjacent GPU rather processor boards reduces inches millimeters distance bits need travel traverse memory GPU back result dramatically accelerated communication improved power efficiency NVLink Faster Data Movement addition NVLink Pascal let data move GPUs CPUs five 12 times faster today current standard PCI-Express greatly benefits applications deep learning high inter-GPU communication needs NVLink allows double number GPUs system work together deep learning computations addition CPUs GPUs connect new ways enable flexibility energy efficiency server design compared PCI-E Broadcast live streaming video Ustream Categories Corporate Gaming Speeds Feeds SupercomputingTags Deep Learning GPU NVLINK Pascal Similar Stories Seventh Heaven NVIDIA Extends Record-Breaking Award Streak Computex Watch Amazing Art Takes Shape SHIELD Tablet NYU Advance Deep Learning Research Multi-GPU Cluster Data Fast Lane NVLink Unleashes Application Performance Paul Bryant Yeah make dinner sandwich Huh right GPUs humans rule OSTEVAND wife Lemming Overlord render lifelike picture Jen Hsun eating dinner sandwich pay well enough http daow net DAOWAce basically bother Maxwell wait Pascal got boss Yeah want wait year bother Maxwell use longer running games well update Pascal gen 2 whatever newest card always waiting next best thing never card http daow net DAOWAce got burned buying 780 shortly Ti released wait extensive period time see NVIDIA pulls shyte jumping newer generation product point d ebay cheap 780 run SLI Pascal Titan X nice 300 1 000 could ve lower fab emily emily new Cheap Archeage Gold game community see many tired cause play years Beta play beta much RS3 Gold enough Cause interesting believe Yes may place need rest play maps since year bleeding new mod stuff nonsence bored maps played year Archeage Gold Next Post Previous Post Subscribe RSS Email Follow Us Find us Facebook Follow us Twitter Find us Flickr Watch us YouTube Find us LinkedIn Find us Google X Enter email address Subscribe Recent Posts Seventh Heaven NVIDIA Extends Record-Breaking Award Streak Computex posted May 27 2015 Gut Check EchoPixel Gives Surgeons Interactive 3D View Inside Human Body Video posted May 26 2015 GeForce eSports Studio Brings Gamers Around NVIDIA World posted May 22 2015 Great Data Center Migration Virtualized 3D Graphics Moving GPUs posted May 19 2015 Watch Amazing Art Takes Shape SHIELD Tablet posted May 18 2015 Witcher 3 Wild Hunt Available Today posted May 18 2015 GeForce DirectX 12 Drivers Windows 10 WHQL-Certified posted May 15 2015 First Full-HD 60 Frames per Second Game-Streaming Service Ruin Forever posted May 12 2015 Learn Virtual GPUs Talk Citrix Synergy posted May 8 2015 5 Wild Ways Startups Using GPUs posted May 4 2015 Solutions 3DTV Play 3D PCs Optimus Graphics Cards GRID High Performance Computing Visualization CUDA Tegra Cool Stuff Corporate Events Affiliate Program Developers Channel Partners Careers RSS Feeds Newsletters Contact Us Security Copyright 2015 NVIDIA Corporation Legal Info Privacy Policy'),
('Particle Swarm Optimization in F# part 2', "Daniel Slater's Blog thoughts rants ramblings programming machine learning development Monday May 18 2015 Particle Swarm Optimization F part 2 - Parallelizing last post gave example particle swarm optimization algorithm F F nice features main reason wanted use easy write multi-threaded applications Multi-threaded PSO version 1 want algorithm run multi-threaded take line update_particles function let updated_particles particles List map fun x - update_particle args loss_func x global_best_params change let updated_particles particles List map fun x - async return update_particle args loss_func x global_best_params Async Parallel Async RunSynchronously Array toList Like magic whole application runs parallel mutable types guarantee issues cross thread calls things don't like though 3 lines really one also creates array map back list annoying quirk F much easier run Arrays parallel Parallel map function Lists arrays mutable losing guaranteed thread safety bit help stack overflow yielded PSeq lib allowing run parallel operations F sequences rewritten let updated_particles particles PSeq map fun x - update_particle args loss_func x global_best_params PSeq toList Multi-threaded PSO version 2 running parallel speed improved still don't use cores efficiently possible iteration updating particles waits every particle complete reasonable take amount time lets say function something could execute 1 second 100 seconds always wait amount time longest complete 1 left running single thread better alternative run whole lifetime particle parallel piece data needs travel particles global_best parameters handled passing ref setter functions always take current global best start update whenever new value changes need make Remove update_particles run_until_stop_condition methods replace let rec private run_particle args Args particle Particle get_global_best unit - Particle check_particle_against_global_best loss_func iterations_to_run Particle let updated_particle update_particle args loss_func particle get_global_best Parameters check_particle_against_global_best updated_particle let new_iterations_to_run iterations_to_run - 1 stop_condition args iterations_to_run get_global_best Local_best_loss updated_particle else run_particle args updated_particle get_global_best check_particle_against_global_best loss_func new_iterations_to_run execute method needs modified run run_particle parallel let execute args Args loss_func list float - float initail_weights seq list float let particles initail_weights Seq take args particles PSeq map fun w - Particle w _ 1 w Length - 0 0 w loss_func w Seq toList let global_best ref particles List minBy fun x - x Local_best_loss let monitor new System Object let check_particle_against_global_best particle Particle lock monitor fun - particle Local_best_loss global_best Value Local_best_loss global_best contents - particle let final_particles particles PSeq map fun x - run_particle args x global_best check_particle_against_global_best loss_func args iterations PSeq toList global_best Value Local_best global_best Value Local_best_loss final_particles looks bit like traditional C multi-threaded code possibility screwing hopefully contained problem enough confident haven't keep ref particle global best need monitor lock updating final check_particle_against_global_best pass new particle create see improvement Speed tests method write execution speed action let speed_test action title let iterations 20 let sw System Diagnostics Stopwatch let results 0 iterations GC Collect sw Restart action sw Stop yield sw Elapsed List sort let median_result List nth results iterations 2 printfn title median_result GC Collect forces Net framework full garbage collection isn't done speed one test affected memory used previous test I'm taking median time think better measure mean ever run set speed tests always take ages spooky behavior OS's middle time ignores occasional outliers Results fairly old 4 core desktop Single threadedMulti-threaded 1Mutli-threaded 2 Fixed length function time seconds2 451 561 33 Variable length function time seconds4 451 440 71 looks like Multi-threaded 2 gives pretty decent improvement Full code github Posted Daniel Slater 11 40 Email ThisBlogThis Share TwitterShare FacebookShare Pinterest comments Post Comment Newer Post Older Post Home Subscribe Post Comments Atom Daniel Slater View complete profile Subscribe Posts Atom Posts Comments Atom Comments Blog Archive 2015 3 May 3 Programming programming computer game Net run Particle Swarm Optimization F part 2 - Paralle Particle Swarm Optimization F"),
('The Five Elements of Data Science Process', 'Request storySign Sign upAhmed El Deeb May 151 minNext storyNext storyThe author chose make story unlisted means people link see sure want share Yes show sharing optionsThe Five Elements Data Science Process Share Twitter Share FacebookThe Five Elements Data Science ProcessHere think five elements order sensible data science process Metrics Always start metrics Design set metrics one use assessing quality models Choosing right metric absolutely crucial Hold-out Set Construct sample least one reliable test set acquire ground truth Innovation Use intuition innovation construct different features models combine different ways step could benefit combining innovation multiple individuals either collaborative competitive setting Selection Use metrics statistical hypothesis testing pick best solution winning model could well combination several models Deployment Monitoring many cases optional step data science endeavors need sometimes output process model needs deployed production environment monitored anomalies malfunction Although 3 fun part time cleverness spent without 1 2 4 really get anywhere would doomed go circles chasing increasingly complicated algorithms means quantify gains wasting precious cycles even worse losing methodical competitor RecommendRecommendedBookmarkBookmarkedShareMoreBlockedUnblockFollowFollowingAhmed El DeebPublished May 15 rights reserved author'),
('Implementing Neural Turing Machines', ''),
('Petuum: a distributed machine learning framework', "Home Performance Research Contact Petuum distributed machine learning framework aims provide generic algorithmic systems interface large scale machine learning takes care difficult systems plumbing work algorithmic acceleration simplifying distributed implementation ML programs - allowing focus model perfection Big Data Analytics Petuum runs efficiently scale research clusters cloud compute like Amazon EC2 Google GCE View GitHub Petuum v1 0 available Download GitHub Petuum Petuum provides essential distributed programming tools tackle challenges running ML scale Big Data many data samples Big Models large parameter intermediate variable spaces Unlike general-purpose distributed programming platforms Petuum designed specifically ML algorithms means Petuum takes advantage data correlation staleness statistical properties maximize performance ML algorithms realized core features distributed Parameter Server distributed Scheduler STRADS Petuum comes ready-to-run ML algorithms addition distributed ML programming tools Petuum comes library distributed ML algorithms serve demo Petuum's capabilities also used massive scale real Big Data analytics programs implemented top Petuum framework speed scalability Currently library includes steadily enriched Convolutional Neural Network CNN Distance Metric Learning Multiclass Logistic Regression Nonnegative Matrix Factorization Sparse Coding K-means MedLDA advanced topic model LDA topic model Matrix Factorization collaborative filtering Fully-connected Deep Neural Networks Random Forest Classifier Petuum allows run bigger models less hardware without compromising accuracy highlights LDA topic model 20m unique words 10k topics 220b sparse parameters cluster total 256 cores 1TB memory Matrix Factorization collaborative filtering 20m-by-20k matrix rank 400 8b parameters cluster total 128 cores 1TB memory Convolutional Neural Network 1b parameters cluster total 1024 CPU cores 2TB memory GPUs required Sparse Coding 1b parameters cluster total 128 CPU cores 1TB memory Multiclass Logistic Regression 10m parameters 10 classes 1M dimensions cluster total 512 CPU cores 1TB memory economical Petuum Cloud compute instances 16 cores 128GB memory available less 2 hr providers like Amazon Google Petuum team committed regular releases 2015 expect important features YARN HDFS compatibility brand new programming interface greatly improved ease-of-use What's Petuum anyway Petuum comes perpetuum mobile musical style characterized continuous steady stream notes Paganini's Moto Perpetuo excellent example goal build system runs efficiently reliably -- perpetual motion Latest News 2015 5 16 paper Petuum New Platform Distributed Machine Learning Big Data accepted KDD'15 work ties together data- model-parallel aspects Petuum unified framework large-scale distributed ML 2015 2 1 Eric Xing gave WSDM 2015 Winter School talk Algorithm System Interface Distributed Machine Learning 2015 1 17 LightLDA paper accepted WWW'15 LightLDA world's biggest topic model running standard hardware costs 100x less competing systems Coming soon large-scale LDA server powered Petuum LightLDA try browser 2015 1 1 Petuum v1 0 officially released GitHub Highlights many new ML algorithms library 2x larger improved Parameter Server performance new interface STRADS 2014 12 14 Eric Xing gave keynote speech Big Data Technology Conference 2014 China New Platform Cloud-Based Distributed Machine Learning Big Data 2014 11 28 New research papers AAAI NIPS 2014 6 30 Petuum beta release v0 9 officially GitHub Highlights improved SSP parameter server performance parameter server snapshots fault recovery single-machine out-of-core parameter server new STRADS API Logistic Regression application 2014 5 23 Eric Xing gave keynote speech ParLearning 2014 Algorithmic System Interface Big Learning 2014 03 31 Petuum alpha release v0 2 officially GitHub Highlights new built-in applications better documentation improved Parameter Server STRADS variable scheduler 2013 12 25 Petuum alpha release v0 1 officially GitHub 2013 12 7 Qirong Ho gave oral presentation 2013 NIPS Effective Distributed ML via Stale Synchronous Parallel Parameter Server Petuum project Sailing Lab Carnegie Mellon University"),
('Data cleaning using multi-target decision trees', "Skip content Sign Sign repository Explore Features Enterprise Blog Watch 4 Star 18 Fork 2 AmmsA DTCleaner Code Issues Pull requests Pulse Graphs HTTPS clone URL Subversion checkout URL clone HTTPS Subversion Download ZIP DTCleaner data cleaning using multi-target decision trees 40 commits 1 branch 0 releases Fetching contributors Java 100 0 Java branch master Switch branches tags Branches Tags master Nothing show Nothing show DTCleaner Update README md latest commit 7008e71b86 AmmsA authored May 20 2015 Permalink Failed load latest commit information DTCleaner Added comments renamed variables Apr 20 2015 README md Update README md May 20 2015 README md Motivation recognized poor data quality multiple negative impact enterprises 1 Businesses operating dirty data risk causing large amount financial loses Maintaining data quality also increases operational cost business would need spend time resources detect erroneous data correct data grows bigger days data repairing became important problem important research area DTCleaner produces multi-target decision trees purpose data cleaning It's built detecting erroneous tuples dataset based given set conditional functional dependencies CFDs building classification model predict erroneous tuples cleaned dataset satisfies CFDs semantically correct Example Consider following schema hosp ProviderNum HospName Addr City State ZIP County Phone HospType HospOwner EmergencySerivce Condition MeasureName StateAvg data schema taken US Department Health Human Services website hosp tuple contains 14 values attributes describing provider- level data measures different care heart attack care heart failure care surgical care following conditional functional dependencies CFDs used detect erroneous tuples CFD1 hosp Zip 36545 - City Jackson CFD2 hosp Zip 94115 - City San Francisco CFD1 resp CFD2 asserts zip code 94115 resp 36545 city name must San Francisco resp Jackson HospName Addr City State Zip Jackson Medical Ctr 220 Hospital Drive Jackson AL 36545 Jackson Medical Ctr 220 Hospital Drive Jakson AL 36545 SanFran Hospital 1001 Potrero Ave San Francisco CA 94110 Cali Pacific Medical Ctr 3555 Cesar St San Fran CA 94110 Consider tuple 2 4 Tuple 2 resp 4 satisfies premise CFD1 resp CFD2 disagree right hand side RHS values city misspelled case tuple 2 shortened case tuple 4 point know entries t2 City t2 Zip t4 City t4 Zip dirty need cleaned problem complicated simply changing values city match RHS values CFD unsure attribute wrong one zip code city maybe first place DTCleaner system takes following inputs clean dataset assumed clean satisfies CFDs use dataset test accuracy predicted values prediction process dirty dataset matches clean dataset terms attributes dataset violates number CFDs given set CFDs dataset system would first perform CFD violating detection separates CFD-violating-tuples inserts test set end clean non-CFD-violating tuples set would use training model set violating tuples use making predictions It's worth noting DTCleaner focuses happens acquire set valid consistency set CFDs user could experts enough knowledge bussiness logic using CFD discovery algrothim repair violating tuples closer dataset consistent CFDs repairing part determining attribute change what's new value done machine learning Installation Make sure following libraries build path guava-18 0 jar weka-src jar weka jar Usage Usage DTCleaner input arff CFDinput Eample DTCleaner data hospitalFewerAttr20PercentNoiseOn4 arff arff data CFDs Contributing Fork Create feature branch git checkout -b my-new-feature Commit changes git commit -am 'Add feature' Push branch git push origin my-new-feature Submit pull request D Bibtex misc mustafa2015dtcleaner title DTCleaner data quality author Abualsaud Mustafa url https github com AmmsA DTCleaner year 2015 License Permission hereby granted free charge person obtaining copy software associated documentation files Software deal Software without restriction including without limitation rights use copy modify merge publish distribute sublicense sell copies Software permit persons Software furnished subject following conditions copyright notice permission notice shall included copies substantial portions Software SOFTWARE PROVIDED WITHOUT WARRANTY KIND EXPRESS IMPLIED INCLUDING LIMITED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE NONINFRINGEMENT EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM DAMAGES LIABILITY WHETHER ACTION CONTRACT TORT OTHERWISE ARISING CONNECTION SOFTWARE USE DEALINGS SOFTWARE Bibliography 1 Thomas C Redman impact poor data quality typical enterprise Commun ACM 41 2 79 82 February 1998 Status API Training Shop Blog 2015 GitHub Inc Terms Privacy Security Contact Something went wrong request Please try"),
('Time-lapse Mining from Internet Photos', "Time-lapse Mining Internet Photos Ricardo Martin-Brualla1 David Gallup2 Steven M Seitz1 2 1University Washington 2Google Inc Overview introduce approach synthesizing time-lapse videos popular landmarks large community photo collections approach completely automated leverages vast quantity photos available online First cluster 86 million photos landmarks popular viewpoints sort photos date warp photo onto common viewpoint Finally stabilize appearance sequence compensate lighting effects minimize flicker resulting time-lapses show diverse changes world's popular sites like glaciers shrinking skyscrapers constructed waterfalls changing course Popular Press Interview WIRED Live TV interview BBC article whole segment PBS Gizmodo Quartz Engagdet Washington Post Verge CNET Non English interview La Voz Spiegel Le Monde Paper Ricardo Martin-Brualla David Gallup Steve M Seitz Time-lapse Mining Internet Photos appear ACM SIGGRAPH 2015 PDF Upcoming Code results published soon Photo Credits Reproduced photos Creative Commons License Attribution 2 0 Generic Flickr users Aliento M jirihnidek mcxurxo elka_cz Daikrieg Free Image Reproduced photos permission Flickr user dration Nadav Tobias Klaus Wi kirchen Juan Jes o"),
('[1412.6071] Fractional Max-Pooling', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1412 6071 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context cs CV prev next new recent 1412Change browse cs References CitationsNASA ADS DBLP - CS Bibliography listing bibtex Benjamin Graham Bookmark Computer Science Computer Vision Pattern Recognition Title Fractional Max-Pooling Authors Benjamin Graham Submitted 18 Dec 2014 v1 last revised 12 May 2015 version v4 Abstract Convolutional networks almost always incorporate form spatial pooling often alpha times alpha max-pooling alpha 2 Max-pooling act hidden layers network reducing size integer multiplicative factor alpha amazing by-product discarding 75 data build network degree invariance respect translations elastic distortions However simply alternate convolutional layers max-pooling layers performance limited due rapid reduction spatial size disjoint nature pooling regions formulated fractional version max-pooling alpha allowed take non-integer values version max-pooling stochastic lots different ways constructing suitable pooling regions find form fractional max-pooling reduces overfitting variety datasets instance improve state-of-the art CIFAR-100 without even using dropout Subjects Computer Vision Pattern Recognition cs CV Cite arXiv 1412 6071 cs CV arXiv 1412 6071v4 cs CV version Submission history Benjamin Graham view email v1 Thu 18 Dec 2014 20 45 11 GMT 215kb D v2 Mon 22 Dec 2014 11 06 35 GMT 230kb D v3 Mon 2 Mar 2015 20 06 22 GMT 233kb D v4 Tue 12 May 2015 06 36 11 GMT 230kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('Artificial Neural Networks, to the point', 'Skip content Maker Maker Approach Artificial Intelligence Primary Menu Home GitHub RSS Contact Artificial Neural Networks point May 17 2015May 17 2015 atrilla Artificial Neural Networks powerful models Artificial Intelligence Machine Learning suitable many scenarios elementary working form direct simple However devil details models particularly need much empirical expertise get tuned adequately succeed solving problems hand post intends unravel adaptation tricks plain words concisely pragmatic style practitioner focused value-added aspects business need clear picture overall behaviour neural nets keep reading Note neural network plausibly renown universal learning system Without loss generality text makes decisions regarding model shape topology training method like design choices though easily tweaked implementation may suitable solve kinds problems accomplished first breaking complexity depicting procedure tackle problems systematically order quickly detect model flaws fix soon possible Let say gist process achieve lean adaptation procedure neural networks Theory Operation Artificial Neural Networks ANNs interesting models Artificial Intelligence Machine Learning powerful enough succeed solving many different problems Historical evidence importance found leading technical books dedicate many pages cover comprehensibly Overall ANNs general-purpose universal learners driven data conform connectionist learning approach based interconnected network simple units simple units aka neurons compute nonlinear function weighted sum inputs see clearly equations Neural networks expressive enough fit dataset hand yet flexible enough generalise performance new unseen data true though neural networks fraught experimental details experience makes difference successful model skewed one following sections cover essentials modus operandi without getting bogged small details Framework say 9 10 people use neural networks apply multilayer perceptron MLP MLP basically feed-forward network 3 layers least input layer output layer hidden layer see Figure 1 Thus MLP structural loops information always flows left input right output lack inherent feedback saves lot headaches analysis totally straightforward given output network always function input depend former state model previous input Figure 1 Framework multilayer perceptron behaviour defined weight connections given values parameters e thetas Framework multilayer perceptron Regarding topology MLP normally assumed densely-meshed one-to-many link model layers mathematically represented two matrices parameters named thetas case certain connection little relevance respect observable training data network automatically pay little attention contribution assign low weight close zero Prediction evaluation output neural network e prediction given input vector data matter matrix multiplication end following variables described convenience dimension input layer dimension hidden layer dimension output layer dimension corpus number examples Given variables parameters network e thetas matrices defined follows following sections describe ordered steps need followed order evaluate network prediction Input Feature Expansion first step attain successful operation neural network add bias term input feature space mapped input layer feature expansion input space bias term increases learning effectiveness model adds degree freedom adaptation process Note directly represents activation values input layer Thus input layer linear input vector defined linear activation function Transit Hidden Layer activations outputs input layer determined values flow hidden layer weights defined Similarly dimensionality hidden layer expanded bias term increase learning effectiveness new function introduced generic activation function neuron generally non-linear see application yields output values hidden layer provides true learning power neural model Output Prediction Finally activation values output layer e network prediction calculated follows Activation Function activation function neuron non-linear function provides expressive power neural network Typically sigmoid function hyperbolic tangent function used recommended function smooth differentiable monotonically non-decreasing learning purposes Note range functions varies respectively Therefore output values neurons always bounded upper lower limits ranges entails considering scaling process broader range predicted values needed Training Training neural network essentially means fitting parameters set example data considering objective function aka cost function process also known supervised learning usually implemented iterative procedure Cost Function cost function somehow encodes objective goal attained network usually defined classification regression evaluation function However actual form cost function effectively error fitting function cost function quantifies amount error misfitting network displays respect set data Thus order achieve successfully working model cost function must minimised adequate set parameter values several solutions valid long cost function convex function e bowl-like shape well known example quadratic function trains neural network considering minimum squared error criterion whole dataset training examples Note term cost function represents target value network e ideal desired network output given input data value cost function expressed convex optimisation procedure e g gradient-based method must conducted order minimise value Note essentially least-squares regression One last remark made amount examples training procedure considers several instances per cost computation e approach called batch learning Batch learning slow cost computation accounts available training instances contrast usual consider one training instance time e order speed iterative learning process procedure called online learning Online learning steps faster compute single-instance approximation cost function makes little inaccurate around optimum However online learning rather convenient cases Gradient Descent Given convex shape cost function minimisation objective boils finding extremum function end may use analytic form derivative cost function nightmare numerical finite difference automatic differentiation Gradient descent first-order optimisation algorithm first starts arbitrarily chosen parameters computes derivative cost function respect model parameters updated moving distance determined called learning rate former initial point direction steepest descent e along negative gradient steps iterated loop stopping criterion met e g determined number epochs e single presentation patterns training example set reached Gradient descent effectively algorithm gradient ascent seeking minimum objective function instead maximum order reuse already developed code recall DRY Repeat principle tip number 11 Pragmatic Programmer m going take negative former cost function like order conduct gradient descent approach gradient ascent algorithm Note algorithm needs learning rate parameter sets step size used update neural network model parameters set small convergence needlessly slow whereas large update correction process may overshoot even diverge Parameter Initialisation initial weights thetas assigned training process critical respect success learning strategy determine starting point optimisation procedure depending value adjusted parameter values may end different places cost function multiple local minima parameter initialisation process based uniform distribution two small numbers take account amount input output units adjacent layers order ensure proper learning procedure weights parameters need randomly assigned order prevent symmetry topology network model would likely incur convergence problems Regularisation mean squared-error cost function described incorporate knowledge constraint characteristics parameters adjusted gradient descent optimisation strategy may develop generalisation problem space solutions large solutions may turn model unstable new unseen data Therefore need smooth performance model wide range input data Neural networks usually generalise well long weights kept small tip also concordance parameter initialisation Thus Tikhonov regularisation process aka ridge regression introduced means control complexity model favour increased general performance regularisation approach favours small weight values typical trade-off Machine Learning known bias-variance trade-off direct relationship complexity model nature data amount available training data adjust ability model learn less complex scenarios raises issue respect fitting data simple explain complex model said overfit data causing overall performance drop high variance model Similarly complex data tackled simple model model said underfit data also causing overall performance drop high bias model usual engineering compromise must reached adequate value Practical Issues success Artificial Intelligence Machine Learning applications plausibly conceived matter controlling several key variables following series good practices works literature identify bits pieces taken account designing successful model described follows Focus model generalisation keep separate self-validation set data used train model test estimate actual performance model Incorporate much knowledge possible Expertise key indicator success Data driven models magic information available greater performance model Feature Engineering utmost importance relates former point useful information extracted input data better performance expected Salient indicators keys success may lead selecting informative features mutual information chi-square change feature space used represent instance data Principal Component Analysis always standardise data exclude outliers Get data model good enough Related curse dimensionality principle good data lacking successful model obtained must coherent relation parameters model e complexity amount available data train Ensemble models integrate criteria Bearing mind optimum model structure known advance one reasonable approaches obtain fairly good guess apply different models different learning features problem combine weight outputs Related techniques also known boosting following sections delve topics practical strategies Target Values designing learning system suitable take account nature problem hand e g whether classification problem regression problem determine number output units case classification amount different classes target output binary vector Given instance output unit corresponds instance class set decision rule classification driven maximum output unit Figure 2 shows digital XOR gate TTL technology voltage thresholds taken account 2 inputs 2 outputs 2 categories true class false class Figure 2 Digital XOR gate TTL technology Digital XOR gate TTL technology case regression problem equal number dependent variables Figure 3 displays regression capacitor discharge function independent variable time dependent variable voltage Figure 3 Capacitor discharge function Capacitor discharge function Hidden Units number hidden units determines expressive power network thus complexity transfer function complex model complicated data structures learn Nevertheless argument cannot extended ad infinitum shortage training data respect amount parameters learnt may lead model overfit data aforementioned regularisation function used avoid situation Thus common skew toward suggesting slightly complex model strictly necessary regularisation compensate extra complexity necessary heuristic guidelines guess optimum number hidden units indicate amount somewhat related number input output units experimental issue though rule thumb Apply configuration works problem re done end Presently hype deep learning e like rebranding multilayer neural networks next big thing Machine Learning Artificial Intelligence However recently hard publish something scientific community neural networks fashion back neural nets seem fanciest technique ever existed problems seem solvable especially size network grows huge numbers era big data right Figure 4 Artificial Neural Network -- Artificial neural network class multilayer perceptron model -- -- Activation function assumed sigmoid -- Tikhonov regularisation set 1 ann -- PRE -- - size input layer number -- HID - size hidden layer number -- - size output layer number -- -- POST -- Returns instance ANN table function ann new HID local newann Lin Lhid HID Lout self __index self setmetatable newann self newann initw return newann end -- POST -- Initialises model thetas function ann initw local epsilonIN math sqrt 6 math sqrt self Lin self Lhid local epsilonOUT math sqrt 6 math sqrt self Lhid self Lout -- local function initmat din dout value math randomseed os time local mat 1 dout local aux j 1 din table insert aux math random - 0 5 0 5 value end table insert mat aux end return mat end -- self thetain initmat self Lin 1 self Lhid epsilonIN self thetaout initmat self Lhid 1 self Lout epsilonOUT end -- PRE -- input - feat 1 N vector table -- -- POST -- Returns output 1 K vector table function ann predict input local function matprod m1 m2 local result -- init 1 m1 local row j 1 m2 1 table insert row 0 end table insert result row end -- multiply 1 m1 j 1 m2 1 local prod 0 k 1 m1 1 prod prod m1 k m2 k j end result j prod end end return result end -- local function sigmoid x local y 1 1 math exp -x return y end -- input must column N 1 vector table -- step 1 local aIN 1 1 input table insert aIN input end -- step 2 local zHID matprod self thetain aIN local aHID 1 1 zHID table insert aHID sigmoid zHID 1 end -- step 3 local azOUT matprod self thetaout aHID 1 azOUT azOUT 1 sigmoid azOUT 1 end local flatOUT 1 azOUT table insert flatOUT azOUT 1 end return flatOUT end -- PRE -- feat - list example feature vectors table -- targ - list target value vectors table -- -- POST -- Fits neural network params given data -- Returns training error number function ann train feat targ require gradient_ascent local function saveThetas local thetas -- theta 1 self Lhid j 1 self Lin 1 table insert thetas self thetain j end end -- theta 1 self Lout j 1 self Lhid 1 table insert thetas self thetaout j end end return thetas end local function loadThetas thetas -- theta local index 1 1 self Lhid j 1 self Lin 1 self thetain j thetas index index index 1 end end -- theta 1 self Lout j 1 self Lhid 1 self thetaout j thetas index index index 1 end end end local function cost thetas local sqerr pr local J 0 loadThetas thetas m 1 feat pr self predict feat m sqerr 0 k 1 pr sqerr sqerr math pow targ m k - pr k 2 end J J sqerr end J J feat -- Regularisation local R 0 1 self thetain j 2 self thetain 1 R R math pow self thetain j 2 end end 1 self thetaout j 2 self thetaout 1 R R math pow self thetaout j 2 end end R R 0 01 self Lin self Lhid self Lhid self Lout return - J R end -- flatten thetas local flatTheta saveThetas gradient_ascent cost flatTheta 1 -- deflat theta restore model trerr cost flatTheta loadThetas flatTheta -- return training err return trerr end Neural networks inherently hard per se ve always Getting succeed wide range problems challenging task indeed However basic form operation simple fact simple plain least squares regression sophisticated fitting function complete code listing shown Figure 4 Actually exists myriad techniques deal many details example renown backpropagation learning algorithm without loss generality left scope aimed post big picture neural nets ll reading future post due time Stay tuned Posted Optimisation Post navigation Gradient-based methods 7 thoughts Artificial Neural Networks point Michael says May 19 2015 06 01 Great article next post please explain recurrent neural networks work including LSTM Also programming language re using Reply atrilla says May 19 2015 21 46 Thanks Michael growing interest NN fact ve got copy venerable Hertz Krogh Palmer book delve details insightful physics perspective m thinking implementing C intensive computing purposes order effectively tackle tough learning issues deep learning facing LSTM scene sooner later regarding programming language use called Lua lightweight powerful well designed language surprising Let point former post Reply atrilla says May 21 2015 21 41 piece published today Demistifying LSTM Neural Networks Reply Michael says May 21 2015 22 20 Thanks actually saw another post RNNs today http karpathy github io 2015 05 21 rnn-effectiveness Reply atrilla says May 21 2015 22 39 Awesome Pushed back toread list Thanks Reply Michael says May 21 2015 22 31 Regarding languages m currently using Python Numpy advantage going Lua especially say still use C performance Reply atrilla says May 21 2015 22 56 use Python Numpy bundle extensively well especially work fantastic Lua like means push learn new coding techniques deliberate practice home Performance close C attainable just-in-time compiler LuaJIT Lua said easier Python port embedded platforms low memory footprint present Internet Things hype might advantage perspective still keeping C sight romantic guess got inspired work developed Facebook AI Research lab FAIR Torch project stuff related deep learning Lots things learn Reply Leave Reply Cancel reply email address published Required fields marked Name Email Website Comment may use HTML tags attributes href title abbr title acronym title b blockquote cite cite code del datetime em q cite strike strong Search Recent Posts Artificial Neural Networks point Gradient-based methods closer look problem state encoding genetic algorithms electronics genetic algorithms simulated annealing algorithm Follow ai_maker Facebook AI-Maker LinkedIn SubscribeLeave Blank Change email Categories Electronics Optimisation Search Uncategorized Archives May 2015 April 2015 March 2015 February 2015 January 2015 December 2014 Proudly powered WordPress Theme Plainly Chris Rudzki'),
('Best results on MNIST, CIFAR-10, CIFAR-100, STL-10, SVHN', ''),
('Free Ebooks for Machine Learning', 'Toggle navigation Home Coding Freebies Inspiration Tips Tools Tutorials 12 Best Free Ebooks Machine Learning May 14 2015 Coding 3 Comments Machine learning scientific discipline works construction study algorithms operate building model example inputs using make predictions decisions number people joining nerdy geeks machine learning seen quite lot development course years one recently started planning start career machine learning answer yes won scare words like quite difficult job hard nut crack take motivation understand trying say Many things world difficult appear beginning impossible Machine learning everyone cup tea determined contribute help though directly like writing book blog something like list useful resources compiled list best free machine learning ebooks written aim help targeted nerdy people machine learning easily possible take look best free machine learning ebooks listed make pick one would want read first go along ones 1 Bayesian Reasoning Machine Learning David Barber Bayesian Reasoning Machine Learning David Barber ebook designed final-year undergraduates master students limited background linear algebra calculus basic reasoning advanced techniques within framework graphical models readers get learn developing skills easily could wish 2 Inductive Logic Programming Techniques Applications Nada Lavrac Saso Dzeroski book inductive logic programming ILP research field intersection machine learning logic programming aims formal framework besides providing practical algorithms inductively learning relational descriptions form logic programs 3 Gaussian Processes Machine Learning Carl E Rasmussen Christopher K Williams book makes readers learn principled practical probabilistic approach learning kernel machines quite different easy way comprises supervised-learning problem regression classification includes detailed algorithms 4 Machine Learning Neural Statistical Classification D Michie D J Spiegelhalter book D Michie D J Spiegelhalter written aim provide up-to-date review different approaches classification compare performance wide range challenging data-sets also draw conclusions applicability realistic industrial problems 5 Information Theory Inference Learning Algorithms David J C MacKay book Information theory topic explained well talked detail help readers attain good knowledge practical communication systems like arithmetic coding data compression sparse-graph codes error-correction 6 Elements Statistical Learning Data Mining Inference Prediction Hastie R Tibshirani J Friedman book readers get learn conceptual underpinnings rather obtaining theorical knowledge comprises statistical framework best pick statisticians researchers practitioners 7 LION Way Roberto Battiti Mauro Brunato LION Way ebook written aim help readers machine learning Intelligent Optimization LION combination learning data optimization applied solve complex dynamic problems 8 Introduction Machine Learning Amnon Shashua Introduction Machine learning ebook comprises Statistical Inference Bayes EM ML MaxEnt duality algebraic spectral methods PCA LDA CCA Clustering PAC learning Formal model VC dimension Double Sampling theorem 9 Course Machine Learning Hal Daume III Course Machine Learning ebook comprises set introductory material covering various aspects modern machine learning 10 Reinforcement Learning C Weber M Elshaw N M Mayer book comprises know reinforcement learning 22 chapters first 11 chapters focus description extended scope reinforcement learning remaining 11 chapters show already wide usage numerous fields 11 Introduction Machine Learning Nils J Nilsson name says introduction machine learning book Nils J Nilsson surveys topics machine learning circa 1996 aim pursue middle ground theory practice 12 Reinforcement Learning Introduction Richard Sutton Andrew G Barto Reinforcement learning lets users learn machine easy way like agent tries maximize total amount reward receives interacting complex uncertain environment already read one ebooks listed one would suggest others go first go share reviews gone free machine learning ebooks Subscribe Newsletter Get useful resources tips freebies inspiration dosage subscribing free email newsletter Please check mail confirm subscription Something went wrong Ebooks Share story Rajni Setia Rajni Mass Communication Post Graduate passion blogging biking Besides reading loves freelancing spreading information related latest development field technology design Leave Reply Cancel Reply Notify follow-up comments email Notify new posts email 3 Comments Carl Mullins - 1 Thank Reply Yaroslav Halchenko - 0 Thanks collating ease access books made git-annex repository available https github com datalad mlbooks Cheers Reply Rajni Setia - 0 Thanks making us aware easy access machine learning ebooks Reply Next ArticleLatest Tools Week 9th May 15th May Subscribe Newsletter Get useful resources tips freebies inspiration dosage subscribing free email newsletter Subscribe Connect Us Privacy Contact Copyright 2015 TheNeoDesign com Right Reserved'),
('New York R Conference 2015 Talks & Videos', "NEW YORK R CONFERENCE Speakers Agenda Sponsors 2015 Talks BROUGHT LANDER ANALYTICS WORK-BENCH Let Know Tickets Go Sale 2015 Talks Videos Andrew Gelman Call Bayesian Know m One Andrew Gelman call Bayesian know m one Mike Dewar Data Perspective Chris Wiggins Engagement Reality Hilary Parker Reproducible Analysis Production Lessons Etsy Bryan Lewis htmlwidgets Jared Lander Making R Go Faster Bigger Vivian Peng Storytelling Data Visualizations Kaz Sakamoto Kristina Pecorelli Visualizing Livability NYC Wes McKinney DataFrames Good Bad Ugly Joseph Rickert Reproducible Data Analysis Revolution R Open Joseph Rickert Reproducibility Checkpoint RRO Michael Kane Practical Principles Scalable Statistical Analysis Winston Chang Dashboarding Shiny Karen Moon David Goliath Saar Golde Leveraging RHadoop Analyzing Multiple Myeloma Patient Timelines Max Richman R Every Survey Analysis Daniel Chen Interactive Ebola Plots Shiny Max Kuhn Zachary Deane-Mayer One Knows It's Like Bad Man Development Process Caret Package Max Kuhn Zachary Deane Mayer Nobody Knows Like Bad Man Development Process Caret Package Julie Yoo Hiring Human Machine Learing Harlan Harris Software Architecture Predictive Models R Stefan Karpinski Data Analysis Julia R Gene Ekster Alternative Data Institutional Investing Common Problems Solved R R ENTHUSIASTS DATA SCIENTISTS GATHER EXPLORE SHARE INSPIRE IDEAS Buy Tickets APRIL 24 - 25 2015 9 - 5 PM WORK-BENCH 110 5TH AVE 5TH FLOORNEW YORK NY 10011 Featured Speakers Chris Wiggins Chief Data Scientist New York Times Hilary Parker Senior Data Analyst Etsy Inc Andrew Gelman Statistics Professor Columbia University Saar Golde Chief Data Scientist Knowledgent See Full List Speakers April 24 8 00am - 8 50am Breakfast Open Registration 8 50am - 9 00am Opening RemarksJared Lander Lander Analytics Jon Lehr Work-Bench 9 00am - 9 20am Data PerspectiveMike Dewar Data Scientist New York Times R D Lab 9 25am - 9 45am Dynamic Market Structure Discovery Automated Detection Machine LearningAbhi Patel Context Relevant 9 50am - 10 10am Beyond Information NoiseMichael Dobrovolsky Lead Architect Enterprise Big Data Solutions Advanced Analytics Services Morgan Stanley Wealth Management 10 10am - 10 40am Coffee Networking 10 40am - 11 00am Dashboarding ShinyWinston Chang Software Engineer RStudio 11 05am - 11 25am Making R Go Faster BiggerJared Lander CEO Lander Analytics 11 30am - 11 50am Talk TBDShawn Simpson Senior Data Scientist Dow Jones 11 50am - 1 00pm Lunch Networking 1 00pm - 1 20pm htmlwidgetsBryan Lewis Applied Mathematician Paradigm4 1 25pm - 2 05pm Banking Panel 2 10pm - 2 30pm Creating Consuming Web Services R-landMike Malecki Neal Richardson Crunch io 2 35pm - 2 55pm Engagement RealityChris Wiggins Chief Data Scientist New York Times 3 00pm - 3 20pm Talk TBDMichael Kane Associate Research Scientist Public Health Biostatistics Yale 3 20pm - 3 50pm Afternoon Break Networking 3 50pm - 4 10pm Reproducible Analysis Production Lessons EtsyHilary Parker Senior Data Analyst Etsy 4 15pm - 4 35pm Integration VernacularJames Powell Organizer NYC Python 4 40pm - 5 00pm Integrating R Python Enterprise Software Darren Kaplan CEO Co-Founder hiQ Labs April 25 8 30am - 9 00am Breakfast 9 00am - 9 20am Leveraging RHadoop Analyzing Multiple Myeloma Patient TimelinesSaar Golde Chief Data Scientist Knowledgent 9 25am - 9 45am David Goliath Small Start Providing Data Tools Influence Way Big Retailers WorkKaren Moon Co-Founder CEO Trendalytics 9 50am - 10 10am Storytelling Data VisualizationsVivian Peng Transmedia Designer 10 10am - 10 40am Coffee Networking 10 40am - 11 00am Reproducible Data Analysis Revolution R OpenJoe Rickert Data Scientist Community Manager Revolution Analytics 11 05am - 11 25am One Knows It's Like Bad Man Development Process Caret PackageMax Kuhn Pfizer R D Zachary Deane-Mayer Cognius 11 30am - 11 50am DataFrames Good Bad UglyWes McKinney Software Engineer Cloudera author pandas python data analysis library 11 50am - 1 00pm Lunch Networking 1 00pm - 1 20pm R Every Survey AnalysisMax Richman Data Scientist 1 25pm - 2 05pm Call Bayesian Know m OneAndrew Gelman Director Applied Statistics Center Columbia University 2 10pm - 2 30pm Interactive Ebola Plots ShinyDaniel Chen Bioinformatics Researcher 2 35pm - 2 55pm Talk TBDKaz Sakamoto Kristina Pecorelli 3 00pm - 3 20pm Talk TBDJulie Yoo Chief Scientific Officer Co-Founder Pymetrics 3 20pm - 3 50pm Afternoon Break Networking 3 50pm - 4 10pm Software Architecture Predictive ModelsHarlan Harris Director Data Science EAB 4 15pm - 4 35pm Data Analysis Julia RStefan Karpinski Co-Creater Julia 4 40pm - 5 00pm Alternative Data Institutional Investing Common Problems Solved RGene Ekster Director Data Product Development 1010Data 5 00pm - 5 10pm Closing RemarksJared Lander 5 30pm - 7 30pm Happy Hour Rye House11 West 17th Street 5th 6th Ave Sponsored Sponsor Inquiry Accomodations Jade Hotel Greenwich Village NYC Mention Work-Bench Discount 52 W 13th St New York NY www thejadenyc com 212 375-1300 Logistics Please plan arrive Work-Bench Friday April 24th 8 30am NY R Meetup Copyright 2015 New York R Conference rights reserved"),
('Deep Recurrent Neural Networks for Time Series Prediction', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1407 5949 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF Current browse context cs NE prev next new recent 1407Change browse cs References CitationsNASA ADS DBLP - CS Bibliography listing bibtex Sharat C Prasad Piyush Prasad Bookmark Computer Science Neural Evolutionary Computing Title Deep Recurrent Neural Networks Time Series Prediction Authors Sharat C Prasad Piyush Prasad Submitted 22 Jul 2014 v1 last revised 18 Dec 2014 version v2 Abstract Ability deep networks extract high level features recurrent networks perform time-series inference studied view universality one hidden layer network approximating functions weak constraints benefit multiple layers enlarge space dynamical systems approximated given space reduce number units required certain error Traditionally shallow networks manually engineered features used back-propagation extent limited one attempt choose large number hidden units satisfy Markov condition made case Markov models shown many systems need modeled higher order present work present deep recurrent networks longer backpropagation time extent solution modeling systems high order predicting ahead study epileptic seizure suppression electro-stimulator Extraction manually engineered complex features prediction employing allowed small low-power implementations avoid possibility surgery extraction features may required included solution recurrent neural network performs feature extraction prediction prove analytically adding hidden layers increasing backpropagation extent increases rate decrease approximation error Dynamic Programming DP training procedure employing matrix operations derived DP use matrix operations makes procedure efficient particularly using data-parallel computing simulation studies show geometry parameter space network learns temporal structure parameters converge model output displays dynamic behavior system greater 99 Average Detection Rate real seizure data tried Comments Preliminary submitted IEEE TNNLS Subjects Neural Evolutionary Computing cs NE MSC classes 62M45 82C32 92B20 ACM classes C 1 3 F 1 1 2 6 5 1 Cite arXiv 1407 5949 cs NE arXiv 1407 5949v2 cs NE version Submission history Sharat Prasad view email v1 Tue 22 Jul 2014 17 25 50 GMT 1948kb v2 Thu 18 Dec 2014 17 04 23 GMT 14133kb authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('My first experience with machine learning using Amazon Machine Learning', 'Skip content Lars Holdgaard Entrepreneur wants change world Menu widgets Blog Goals Contact Subpages Danish Transhumanism Danish Bitcoin Digital nomad Categories Digital nomad Economics Entrepreneurship Health Life philosophy Society Technology Uncategorized Recent Posts different countries affect emotionally mentally self-improvement followup Review Amazon Machine learning first experience machine learning Today startups seek change physical world Personal reflections Avoid decisions default mastery Review co-working spaces HUBBA Bangkok Punspace Chiang Mai Review Amazon Machine learning first experience machine learning Bachelor Degree Software Engineering best technical university Denmark Master Degree Business see quite technical person job make sure developers company make great code pick right architectural decisions good job customers seem love us well keep buying stuff business guy business perspective totally hyped machine learning artificial intelligence reason mind well let us say believe future totally want build next machine-learning -applied product conquer world experience machine learning artificial intelligence deep learning fancy words ve read Amazon launched Amazon Machine Learning hyped could finally become AI Machine learning learning hacker ready hack world saw introduction video around 30 minutes long looked really cool presenter also seemed really simple mind decided something try review experience Amazon Machine Learning One customers big webshop Denmark took last 5 5 years revenue data approximately 2000 rows meant file Date created Revenue naive thesis let us import tool wait 5 minutes near perfect prediction future mean max 5-10 error rate got tool Created new data source Ready rock world Failed Fuck Failed reviewed data source Oh could white space columns Fair enough Try Failed didn like date format Apparently Danish time format popular gives errors decided find Python skills import data play around finally created new file data columns ID correct dates importantly nicely formatted revenues created data source Success ready finally beaten system could get work selected revenue target data date categorial Click Click Predict shit please said pending 10 minutes assumed 10 minutes meant data would awesome could tell customers fire whole business intelligence team use machine learning skills instead said data READY output file two columns score numbers 0 631 0 521 tried play around interface simply couldn find predict future value button kept clicking around nothing weird graphs weird numbers weird files Amazon storage could given might naive stupid judge least know limits went Upwork earlier Odesk called help couple useless applicants found savior Mario Filho Brazil made 20 USD hr offer profile experience using wide range machine learning algorithms supervised classification regression unsupervised clustering learning solve real-world problems data hire steps take make data project success Understand goals expectations Clean prepare data Develop models respecting technical standards Report performance test environment Deploy model production sounded like guy needed Maybe never worked Amazon Machine Learning sure 0 631 0 535 results meant recommendation need help machine learning contact LinkedIn https www linkedin com mariofilho extremely helpful friendly knows stuff think keep current hourly price long even much higher worth sent order revenue told something show could screencast 2 hours Mario ready Holy cow Amazon Machine Learning complex first thing saw screencast small 49-line-long Python script wrote make data nice shocked aren supposed click upload button type file truth next 30 minutes amazing Mario made multiple data sources apparently need data sources data test evaluation made batch predictions made everything needed get actual data shocked thought chance watching initial introduction video Boy wrong absolutely chance would done right end tried upload file data source could use generate prediction May 7th 2015 revenue data May 6th 2015 got number compared real life data saw 14 error margin tell Mario awesome Mario written small Python function well Brazil ran def mape y_true y_pred print MAPE np abs y_true y_pred y_true mean turned 34 error margin 34 useless shocked 34 couldn tell customers fire business intelligence team 34 error margin Mario calmed told couple things consider clean dataset outliers data points stick could benefit normal machine learning tricks normalizing Amazon uses simple linear regression technique mentioned something Gradient Boosted Decision-something could help Creating models specific subsets data like model SKU region Feature engineering creating relevant features exploring interactions still absolutely clue means probably means important told machine learning hard work work lot data expect better results Apparently turns machine learning hard 30 Minutes talking Mario wrote Skype One last thing promise ran Gradient Boosted Decision Trees model see happens MAPE 25 good signal complex model tuned parameters give better error re getting closer extra work knows maybe could get 15-20 matter quite sad result Let say applied lot work got error rate 15 good predict future made big mistake understand machine learning Amazon Machine Learning boring mathematics evil AI conquer world Amazon Machine Learning quite simple simply lot mathematics linear regression simply lot matrix calculations finding optional values model Machine learning understand data Machine learning simply combination statistics computer science Looking back extremely naive course could expect much better 25 error rate Using normal thinking make sense impossible predict Maybe actually understood context behind numbers brain data maybe could better Amazon Machine Learning simply mathematics Amazon Machine Learning average developer yet Agreed read probably read articles videos even read article Mario sent ahem user perspective far ready tool average developer sit use require understand basic foundations machine learning extremely interesting see person Mario never worked tool able understand time never worked chance sure great tool right help sure useful solve world hunger predict much money pocket tomorrow however fun experience Posted 15 05 2015Author LarsHoldgaardCategories Entrepreneurship Technology 5 thoughts Review Amazon Machine learning first experience machine learning David G says 15 05 2015 19 14 Great write-up really appreciate time document share experience using AWS machine learning service post link article Mario sent Best wishes future startup endeavors Reply LarsHoldgaard says 18 05 2015 04 47 David Sure http cloudacademy com blog aws-machine-learning Reply Pingback Amazon - - Pingback Amazon - - Pingback Vidora - Amazon Machine Learning average developer yet Leave Reply Cancel reply email address published Required fields marked Name Email Website Comment may use HTML tags attributes href title abbr title acronym title b blockquote cite cite code del datetime em q cite strike strong Post navigation Previous Previous post Today startups seek change physical worldNext Next post different countries affect emotionally mentally self-improvement followup Proudly powered WordPress'),
('[1406.3896] Freeze-Thaw Bayesian Optimization', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org stat arXiv 1406 3896 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context stat ML prev next new recent 1406Change browse cs cs LG stat References CitationsNASA ADS Bookmark Statistics Machine Learning Title Freeze-Thaw Bayesian Optimization Authors Kevin Swersky Jasper Snoek Ryan Prescott Adams Submitted 16 Jun 2014 Abstract paper develop dynamic form Bayesian optimization machine learning models goal rapidly finding good hyperparameter settings method uses partial information gained training machine learning model order decide whether pause training start new model resume training previously-considered model specifically tailor method machine learning problems developing novel positive-definite covariance kernel capture variety training curves Furthermore develop Gaussian process prior scales gracefully additional temporal observations Finally provide information-theoretic framework automate decision process Experiments several common machine learning models show approach extremely effective practice Subjects Machine Learning stat ML Learning cs LG Cite arXiv 1406 3896 stat ML arXiv 1406 3896v1 stat ML version Submission history Jasper Snoek view email v1 Mon 16 Jun 2014 03 43 20 GMT 996kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('ICLR 2015 highlights', "Engineering Blog Posts Twitter Github Jobs RSS ICLR 2015 Maciej Kula Eddie Bell Fri 08 May 2015 ICLR relatively new conference primarily concerned deep learning learned representations conference third year 300 attendees two Lyst post we'll discuss interesting papers themes presented year Simplifying network topology One difficulties employing deep convolutional networks complicated topology often hand-tuned new application Choosing number convolutional pooling layers stride sizes non-linearities used initialisation method suit particular task resembles burden traditional feature engineering Two papers presented ICLR suggest might entirely necessary first Striving Simplicity Convolutional Net authors argue pooling subsampling layers network replaced convolution layers without loss accuracy argument goes roughly follows purpose pooling layers perform dimensionality reduction widen subsequent convolutional layers' receptive fields example instead detecting feature top left corner pooling layer allows feature detected across entire top part image However effect achieved using convolutional layer using stride 2 also reduces dimensionality output widens receptive field higher layers resulting operation differs max-pooling layer 1 cannot perform true max operation 2 allows pooling across input channels authors argue structure cross-channel connections treated something learned rather imposed absence cross-channel interactions indeed beneficial network able discover structure approach seems work practice achieving competitive results number tasks second Deep Convolutional Networks Large-Scale Image Recognition core idea hand-tuning layer kernel sizes achieve optimal receptive fields say 5 times 5 7 times 7 replaced simply stacking homogenous 3 times 3 layers effect widening receptive field achieved layer composition rather increasing kernel size three stacked 3 times 3 7 times 7 receptive field time number parameters reduced 7 times 7 layer 81 parameters three stacked 3 times 3 layers authors report resulting models perform well compared state-of-the-art architectures Gaussian word embeddings Word Representations via Gaussian Embedding extends word embedding approach representing words points rather Gaussian distributions allows model express uncertainty one word's meaning may either applicable many contexts uncertain specific given context also allows asymmetry word relationships One interesting application picking words specific given context rather used across many contexts example according Gaussian embedding model 'sense' 'joviality' neighbourhood 'feeling' However variance 'joviality' smaller variance 'sense' reflecting fact 'joviality' specific 'sense' applied across many contexts gives rise idea entailment since mass probability distribution 'joviality' lies inside 'sense' 'joviality' entails 'sense' Detecting different senses word could interesting extension model example word 'bank' could used denote financial institution 'investment bank' well natural feature 'river bank' Currently highly polysemic words represented diffuse distributions would interesting able treat mixtures specific meanings example might possible treat token high variance mixture two Gaussians use expectation-maximisation algorithm discriminate different meanings incoming tokens model training Applying mixture splitting approach recursively might allow exhaustive identification individual word meanings Adverserial training examples Goodfellow et al presented work explaning harnessing adverserial training examples Adversarial examples misclassifications slightly different correctly classified examples Szegedy et al found tiny changes image average distortion 0 006508 resulted consistant misclassification following images right column misclassified 'ostrich' systematic perturbations despite appearing almost identical human Another concept related adversarial examples called rubbish examples rubish example one human would reject belonging given class model would assign class high confidence Nguyen et al trained convolutional neural network CNN ImageNet used evolutionary algorithm generate synthetic images cost function evolution confidence CNN synthetic image belonging particular ImageNet class images show result evolutionary process selection classes image considered prototypical representation CNN uses predict given class ICLR paper Goodfellow et al continued work found adversarial examples exploit linear behaviour activation functions fact high-dimensional input make many infinitesimal perturbations along dimensions results big perturbation final output authors provide method constructing adversarial examples used training provide regularisation beyond techniques dropout Synthetic data Synthetic data rich history machine learning Two prominent include Minsky's use XOR example circle spiral moon style datasets clustering Facebook's research lab presented set synthetic generative tests aim developing AI systems capable general natural language reasoning Leon Bottou defines machine reasoning contrast machine learning manipulating previously acquired knowledge order answer new question authors advocate move away simple models trained lot data order promote development new methodologies harder datasets proposed reasoning tests veryspecific goal somewhat like unit tests code example following tests systems ability understand coreference Question Daniel kitchen went studio Sandra office Daniel Answer studio Whilst following complex test assess systems ability reason time Question afternoon Julie went park Yesterday Julie school Julie went cinema evening Julie go park Answer Cinema power artifical tests system fails test know exactly form reasoning cannot perform contrasts standard 'aggregation' evaluation methods accuracy Jadberg et al also use synthetic data different manner built artifical dataset noisy images containing distorted text trained model artifical data set whereby labels original text used generate images evaluated model real-word images text found artifical training data works well cheap proxy real-world data Conclusion conference good mix interesting papers stimulating conversations also liked call submissions format submissions posted arxiv reviewed allow early discussion dissemination results also makes papers easily accessible circumventing issue ridiculous academic publishing paywalls interested deep learning recommend draft deep learning Bengio et al Please enable JavaScript view comments powered Disqus comments powered Disqus"),
('Multi-Layer Perceptron OCR in JavaScript', "Skip content Sign Sign repository Explore Features Enterprise Blog Watch 18 Star 228 Fork 15 mateogianolio mlp-character-recognition Code Issues Pull requests Pulse Graphs HTTPS clone URL Subversion checkout URL clone HTTPS Subversion Download ZIP Trains multi-layer perceptron MLP neural network perform optical character recognition OCR 53 commits 1 branch 0 releases Fetching contributors JavaScript 100 0 JavaScript branch master Switch branches tags Branches Tags master Nothing show Nothing show mlp-character-recognition fix typo latest commit 4bbbb13b45 mateogianolio authored Mar 23 2015 Permalink Failed load latest commit information examples fix typo Mar 23 2015 mnist gitignore push network js update readme Mar 23 2015 LICENSE md add license add readme remove underscore dependency Mar 12 2015 README md fix typo Mar 23 2015 captcha js refactor increase readability add 'use strict' Mar 23 2015 config json fix typo Mar 23 2015 main js fix errors add freshly generated examples Mar 23 2015 network js fix errors add freshly generated examples Mar 23 2015 package json fixed syntax error Mar 15 2015 tools js refactor increase readability add 'use strict' Mar 23 2015 README md MLP character recognition Trains multi-layer perceptron MLP neural network perform optical character recognition OCR training set automatically generated using heavily modified version captcha-generator node-captcha Support MNIST handwritten digit database added recently see performance section network takes one-dimensional binary array default 20 20 400-bit input outputs 8-bit array converted character code Initial performance measurements show promising success rates training network saved standalone module ocr js used project var ocr require ' ocr js' var output ocr activate input map function bit return bit 0 5 1 0 output binary array converting character easy var character String fromCharCode parseInt output join '' 2 stuff Performance MNIST 0-9 config json mnist true network hidden 160 learning_rate 0 03 Neurons 400 input 160 hidden 4 output Learning rate 0 03 Training set 60000 digits Testing set 10000 digits Training time 36 min 22 50 ms Success rate 90 14 A-Za-z0-9 config json mnist false text abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ012356789 fonts sans-serif serif training_set 2000 testing_set 1000 image_size 16 threshold 400 network hidden 40 learning_rate 0 03 Neurons 256 input 40 hidden 8 output Learning rate 0 03 Training set Size 124000 characters Sample Testing set 62000 characters Training time 4 min 8 2 ms Success rate 78 60322580645162 a-z config json mnist false text abcdefghijklmnopqrstuvwxyz fonts sans-serif serif training_set 2000 testing_set 1000 image_size 16 threshold 400 network hidden 40 learning_rate 0 1 Neurons 256 input 40 hidden 8 output Learning rate 0 1 Training set Size 52000 characters Sample Testing set 26000 characters Training time 2 min 10 752 ms Success rate 91 77692307692308 0-9 config json mnist false text 0123456789 fonts sans-serif serif training_set 2000 testing_set 1000 image_size 16 threshold 400 network hidden 40 learning_rate 0 1 Neurons 256 input 40 hidden 8 output Learning rate 0 1 Training set Size 20000 digits Sample Testing set 10000 digits Training time 1 min 6 620 ms Success rate 99 22 Configuration Tweak network needs editing config json file located main folder Pasted default config file config json mnist false text abcdefghijklmnopqrstuvwxyz fonts sans-serif serif training_set 2000 testing_set 500 image_size 20 threshold 400 network hidden 40 learning_rate 0 1 mnist set true MNIST handwritten digit dataset used training testing network setting overwrite configured set sizes ignore image_size threshold fonts text settings text string containing glyphs train test network fonts array fonts used generating images training_set Number images generated used network training set testing_set images used testing network image_size size square chunk pixels containing glyph resulting network input size image_size 2 threshold analyzing pixels glyph algorithm reduces pixel r g b r g b everything threshold marked 1 resulting binary array used network input network hidden size number neurons hidden layer network learning_rate learning rate network Usage Clone repository script using canvas you'll need install Cairo rendering engine OSX done following one-liner copied canvas README wget https raw githubusercontent com LearnBoost node-canvas master install -O - sh install npm dependencies test npm install node main js Status API Training Shop Blog 2015 GitHub Inc Terms Privacy Security Contact Something went wrong request Please try"),
('On Machine Learning Startups, Ostrich Mania, and the Uncanny Valley', "KDnuggets Data Mining Analytics Big Data Data Science Subscribe KDnuggets News Follow KDnuggets Contact Data Mining Software News Jobs Academic Companies Courses Datasets Data Mining Course Education Meetings Polls Webcasts KDnuggets Home News 2015 May Opinions Interviews Reports Cloud Machine Learning Ostrich Mania Uncanny Valley 15 n16 Latest News 150 Influential People Big Data Hadoop Miner3D Data Visualization System Version 8 Discovery Communications Manager - Data Science KDnuggets News 15 n17 May 27 R wins Annual Poll Top 10 Algorithms Interview Spark Creator Top KDnuggets tweets May 19-25 KDnuggets Poll R leads RapidMiner Python catches Spark ignites Choosing Learning Algorithm Azure ML Cloud Machine Learning Ostrich Mania Uncanny Valley Tweet Previous post Next post Tags Amazon Azure ML Clarifai Cloud Analytics Economics IBM Watson MetaMind Startup Zachary Lipton Cloud machine learning services popping tens providing automated data science solutions anticipated customers want may follow peculiar distribution reminiscent uncanny valley comments Zachary Chase Lipton title post may seem preposterous could cloud machine learning services possibly common market ostrich meat could possibly common uncanny valley Hopefully article's end connections seem less tenuous Ostrich Mania Overview recent decades many investors come see ostrich farming coming panacea meat red like beef lower cholesterol thick skin desired production ostrich leather wallets shoes handbags luggage birds grow 300lbs feed-to-meat ratio far favorable cattle farming result incredible bubbles ostrich farming industry prices birds driven breeders consumers Massive farms set purpose growing ostrich stock anticipation future consumers demand birds Unlike tulip mania consumers drove speculative bubble ostrich mania closely resembles dot-com bubble investors drove price technology companies anticipating would justify incredible valuations future It's worth noting case tech bubble investors generally correct sector high-tech come justify many earlier valuations matching earnings companies like Amazon Apple greatly exceeded pre-crash valuations However specific cases investors clearly often cot details wrong e g America Online Back point hand ostrich mania distinct phenomenon tech bubble specificity say ostrich mania pertains specific industry specific product specific business plan anticipation coming market tech bubble worst saw companies funded neither technical expertise sound business plan simply fervor Ostrich mania may well pan Many farmers betting fact many visionary ideas follow ostrich mania paradigm aggressive bid Tesla several automakers roll electric cars similarly anticipated future consumer demand yet materialized point article suggest ostrich mania paradigm recipe failure instead suggest product's customers live future especially important consider critically preferences hypothetical consumers might reasonably example compatibility ostrich meat could American palette could assessed aggressive push American ostrich ranching absent diligence would unfortunate mistake Connection Machine Learning where's connection cloud machine learning services slew recently launched companies plan offer machine learning predictive analytics service Many bankrolled Series financing Metamind Clarifai Dato ForecastThis among others joined fray Large established companies also lined offerings including IBM Watson Analytics Microsoft Azure ML Amazon Machine Learning Mostly projects anticipate rising mainstream demand data analysis beyond current small group cognoscenti clarify believe folly Clearly machine learning services already providing value every single day every single smartphone computer user email ecosystem could function without high quality spam filtering Voice recognition baked every smart phone depends upon state art deep learning systems transform phonemes text Indirectly people already paying billions dollars machine learning services value real course don't know systems work automagically smartphone user doesn't know data phone accessing F1 score spam filter achieves end users fully automated system end spectrum it's clear expertly built highly specialized state art systems valuable consumers systems large institutions hiring top-tier computer scientists Hedge funds like Renaissance hire machine learning experts build systems predicting securities prices course want custom systems achieve better performance others world They're market out-of-the-box good-enough solutions contrast latest crop offerings provide service lies somewhere middle companies offer various forms flexible machine learning services common variety user supplies new dataset chooses variable predict service chooses trains classifier accessed web interface API make future predictions know users want machine learning magically happens know market machine learning experts build highly customized even novel systems What's less clear much desire point middle fully automated expertly engineered data science Uncanny Valley may heard uncanny valley Roboticists animators often speak Owing Japanese robotics professor Masahiro Mori idea suggests robot becomes human-like feelings towards grow positive point Pages 1 2 Previous post Next post popular last 30 days viewed last 30 days Poll Predictive Analytics Data Mining Data Science software tools used past 12 months - May 7 2015 7 Steps Learning Data Mining Data Science - Oct 10 2013 Grammar Data Science Python vs R - Mar 28 2015 Become Data Scientist Get Hired - May 1 2015 Awesome Public Datasets GitHub - Apr 6 2015 Data Scientists Automated Unemployed 2025 - May 5 2015 Top 10 Data Analysis Tools Business - Jun 13 2014 Myth Model Interpretability - Apr 27 2015 Top 10 R Packages Kaggle Champion - Apr 21 2015 9 Must-Have Skills Need Become Data Scientist - Nov 22 2014 shared last 30 days Poll Predictive Analytics Data Mining Data Science software tools used past 12 months - May 7 2015 Viewed Big Data Videos YouTube - May 9 2015 Inconvenient Truth Data Science - May 5 2015 Data Scientists Automated Unemployed 2025 - May 5 2015 Myth Model Interpretability - Apr 27 2015 3 Things Data Science Wont Find Books - May 11 2015 Real Data Scientists Please Stand - May 18 2015 R vs Python better - May 19 2015 Top 10 Data Mining Algorithms Explained - May 21 2015 5 Not-to-be-Missed Ideas Big Data - May 21 2015 KDnuggets Home News 2015 May Opinions Interviews Reports Cloud Machine Learning Ostrich Mania Uncanny Valley 15 n16 2015 KDnuggets KDnuggets"),
("Identifying music genres using Clarifai's deep learning API", "HomeAboutContactResearchSpeaking Skip content Alexandre Passant Entrepreneur Hacker HomeAboutContactResearchSpeaking Identifying music genres Spotify album covers using Clarifai deep learning API Google Prediction recent news image recognition start-up Clarifai raising 10M decided experiment Web API Deep Learning core approach used music streaming recommendations Spotify image recognition side ll describe experiment combines Spotify Web API Clarifai Image Recognition API Google Prediction order identify artist music genre based album covers clarifai deep learning image recognition API Clarifai deep learning API Machine-Learning-as-a-Service Clarifai one new services Machine-Learning-as-a-Service MLaaS area Azure ML Wise etc think Web development evolved past years completely make sense additional step towards No-stack start-up Many start-ups need Machine-Learning approaches scale product classifying customers recommending music identifying offensive user-generated content etc Yet better outsource platforms core ML expertise rather spending time resource utility unless becomes core business way buy fleet servers hire dev-ops use Google App Engine set-up telephony system rely Twilio Clarifai API uses deep learning image recognition easy use simple API Key token system bindings multiple languages Python instance following tags Green Day Dookie album cover clarifai client import ClarifaiApi clarifai_api ClarifaiApi clarifai_api tag_images 'http assets rollingstone com assets images list 0e099b2214b1673fc76c6c60257b88aefe571def jpg' API also lets provide feedback extraction results feedback used feed algorithm additional data SpotiTag Tagging artists album covers first step experiment tag artists using album covers wrote simple Python class queries Spotify Web API get artist top-10 albums pass Clarifai API filtering broad ones like graphic art etc way find relevant tags artist spotitags import SpotiTags SOCIAL_DISTORTION '16nn7kCHPWIB6uK09GQCNI' sp Spotitags print sp tag SOCIAL_DISTORTION 10 u'text' 5 78628945350647 u'silhouette' 4 906140387058258 u'people' 4 833337247371674 u'background' 4 743582487106323 u'vintage' 3 9088551998138428 u'banner' 3 8920465111732483 u'men' 3 76175057888031 u'card' 3 67703515291214 u'party' 3 6343321204185486 u'old' 2 952597975730896 SpotiTags class available GitHub artists genres bridge gap artist-tags genre-tags used top x songs playlists Spotify starting two very-unrelated genres Doom-metal K-pop Gathering small dataset 140 Doom-metal artist 102 K-pop ones passing previous tagging process top-tags genres K-pop Doom-metal people nobody female dark women people nobody abstract men light isolated night fashion old adult vintage group death business nature genre-tagging approach way similar Spotify published words associated different genres music Except m analyzing song titles album covers see tags italic overlap quite large genres ll come back later let look used data build artist classifier Cloud-based classification Google Prediction name suggest Google Prediction Google cloud-based Machine-Learning API predicting output value set features works whether value class case music genre continuous value e g expected number streams per month artist e classification regression ML-terms Example training data Google Prediction predict set tags belong Doom-metal K-pop category ve built simple training set follows note Google prediction splits string internal model genre list tags Separating previous dataset training testing lead 180 examples like following ones doom nobody vintage christmas light people nature architecture girl woman paper west history celebration islam traditional astrology event round party background reflection antique dark old shadow conceptual death man postcard back festival east years music fireworks banner couple night female abstract women model adult travel greeting religion card gold street church fine art silhouette style map sepia venice rain broken government country lantern castle color love one nude arms sexy shiny texture pin velvet wall pride kpop people adult one men north america vintage two vehicle motor vehicle transportation street cap group three police classic car humor hat cartoon outerwear xmas rapper fedora banner christmas jacket musician invitation man celebration background clothing decoration splash necktie text vest facial expression fast wedding automobile audio tape cassette stereo sound mono analogue obsolete record nobody tree radio broadcast compact reel fish eighties tuner unique nostalgia invertebrate conifer noise moon fine art black white outfit sculpture winter singer season outdoors law nature monochrome greeting fashion menswear women serious dark change boy stroke merry travel garden female face war still life forest music teenager looking leader youth addition try different models limited list tags feature-set either tags artist top-n results follows different approaches Model Success rate artist tags 88 71 Top 100 artist tags 93 55 Top 75 artist tags 87 10 Top 50 artist tags 91 93 Top 25 artist tags 87 101 Top 10 artist tags 82 25 fun part comes next small script combines three APIs together return expected class e music genre Spotify artist Using SpotiTags query Spotify API get artist top-10 albums pass Clarifai build artist tag-set Passing tag-set Google Prediction predict class using former models env marvin-7 clarifai alex python predict py -a7zDtfSB0AOZWhpuAHZIOw5 Candlemass doom Guessing artist genre based album covers genres models good fun ideal system would able classify genres e g 10 genres popular music haven far added Punk-rock equation order try additional models Adding 75 Punk-rock artists let look top-tags K-pop Doom-metal Punk-rock people nobody people female dark nobody women people vintage nobody abstract north america men light men isolated night text fashion old old adult vintage adult group death street business nature business earlier also lots overlap new genres added overlap expected growth Thus using former top-n tags approach results unexpectedly worse previously Model Success rate genre tags 82 50 Top 100 genre tags 82 50 Top 75 genre tags 82 50 Top 50 genre tags 80 00 Top 25 genre tags 71 25 Top 10 genre tags 71 25 Finding distinctive tags instead top-tags let focus ones specific genre e tags extracted album covers via Clarifai deep learning API appear top-100 genre top-100 others still limited three ones K-pop Doom-metal Punk-rock girl horror graffiti young fantasy european model water festival style smoke collection creativity black white urban beautiful history message celebration fire city sexy pattern performance shape sky cartoon person scary two definitely gives better feeling genre Sexy beautiful k-pop scary horrific Doom-metal K-Pop album covers via Google Images Using distinct-tags approach ve updated previous models accordingly training test sets take account top-n tags top-n distinct ones results new classifiers deciding artist playing Doom-metal K-pop Punk-rock based album cover tags Model Success rate Top 100 distincts genre tags 97 50 Top 75 distincts genre tags 98 75 Top 50 distincts genre tags 97 50 Top 25 distincts genre tags 96 25 Top 10 distincts genre tags 95 00 Much better previous approach Yet number genre growth probably need find tune models accurately identify artist genre means using examples training sets also probably additional data going images album titles maybe MIR techniques MLaaS opportunity developers Clarifai limited experiment post showcases different elements cloud-based machine-learning approach help identify artist playing based solely album cover look like globally using APIs definitely simplifies life developers entrepreneurs still need grasp underlying concepts need army Ph D fleet GPU-based servers running scikit Caffe understand make sense data Clarifai believe classification clustering could two additional features former audience would enjoy classification instead running pipeline another API ve done Google Prediction managing everything Clarifai would reduce integration time costs think API ve send examples automatically decide picture offensive clustering since API already accepts images bulk returning clusters additional parameter number cluster would also helpful premium offering already provides similar images feature would follow similar trend m even thinking features outside image domain music ve already started video analyzis case 10M bank growing team experts area doubt see new features API sooner later showcasing deep learning bring Web engineering Loved post Share Click share Twitter Opens new window Share Facebook Opens new window Click share Google Opens new window Click share LinkedIn Opens new window Click share Tumblr Opens new window Click share StumbleUpon Opens new window Click share Pocket Opens new window Click email friend Opens new window Related PUBLISHED May 14 2015May 20 2015 apassantin Music Technology clarifai Data Science google prediction machine learning Spotify Post navigation Testing ski apps French Alps Leave Reply Cancel reply email address published Required fields marked Name Email Website Comment may use HTML tags attributes href title abbr title acronym title b blockquote cite cite code del datetime em q cite strike strong Notify follow-up comments email Notify new posts email 6 thoughts Identifying music genres Spotify album covers using Clarifai deep learning API Google Prediction korbonits says May 14 2015 9 18 pm Thanks post came headline intrigued genre classification something d like figure genre classification audio data Would interesting try unsupervised setting probably far easier attempt supervised setting think Reply apassant says May 18 2015 8 29 Yes definitely makes sense lots work area see instance http ismir2001 ismir net pdf tzanetakis pdf related work https scholar google com scholar ion 1 espv 2 bav 2 r_cp biw 1356 bih 572 dpr 1 um 1 ie UTF-8 lr q related X1QHzTlcJJtEXM scholar google com Reply korbonits says May 18 2015 4 15 pm True thinking re deep learning without hand-coding features papers pre-deep learning Thanks links super helpful Reply apassant says May 18 2015 4 19 pm see case checked work Spotify http benanne github io 2014 08 05 spotify-cnns html Reply korbonits says May 18 2015 4 31 pm AWESOME Thanks Pingback 2015 Proudly powered WordPress Theme Konmi Pantip Treerattanapitak Send Email Address Name Email Address Cancel Post sent - check email addresses Email check failed please try Sorry blog cannot share posts email"),
("This week's top news, announcements and resources in Machine Learning and Artificial Intelligence", "Latest Issue Archives Search Search Search Toggle Menu Subscribe weekly collection best news resources Artificial Intelligence Machine Learning free Subscribe Email Email Subscribe spam ever email address ever used Artificial Intelligence Weekly Issue 3 May 28th 2015 Welcome 3rd issue AI Weekly week introducing new section Topic week since interesting posts discussions around Recurrent Neural Networks RNNs week I'll start new section deep-dive RNNs enjoy Thanks subscribing David News Ethics artificial intelligence 4 leading researchers express opinions development AI related challenges Stuart Russell Take stand AI weapons Sabine Hauert Shape debate don't shy Russ Altman Distribute AI benefits fairly Manuela Veloso Embrace robot human world nature com Google step closer developing machines human-like intelligence Google said work algorithms would encode thoughts bring common sense computers devices theguardian com Computational aesthetics algorithm spots beauty humans overlook team University Turin developed algorithm evaluate beauty image help us find best content platforms Flickr technologyreview com Also news week Stuart Russell expresses concerns AI calling researchers look beyond goal merely making artificial intelligence powerful team Queen Mary University UK come deep neural network framework beats humans sketch recognition Keen software house makes AI learn create hierarchy goals Jason Koebler Vice wonders super-intelligent agents would ignore us humans rather destroying us thoughts personalization services developed Nara Logics new app lets depressed teens chat celebrities dead friends Learning Key lessons machine learning Paper written Machine Learning researchers practitioners 12 key lessons datasets feature selection overfitting model construction washington edu Free ebook - Introduction Genetic Algorithms Genetic algorithms designed solve practical problems mimicking natural selection process free ebook good introduction field mpg de Learning see moving Complex neural networks often trained using millions hand-labelled images real-life though brains don't get trained way rather looking moving objects scenes paper explores possibility performing advanced training using egomotion supervisory signal feature learning arxiv org Topic week Recurrent Neural Networks seem rage week Enjoy little deep-dive topic unreasonable effectiveness Recurrent Neural Networks Great post recurrent neural networks bunch fun examples RNNs used generate Shakespearean essays Linux code github io unreasonable effectiveness Character-level Language Models Yoav Goldberg replies post introduces unsmoothed maximum-likelihood character level language models demonstrates effectiveness generating natural language outputs ipython org RNN - Handwriting generation demo Great online demonstration built Alex Graves uses Long Short-term Memory recurrent neural networks synthesize handwritten words result realistic test different handwriting styles toronto edu Training Recurrent Neural Networks thesis Thorough overview RNNs spare time willing dive specifics PhD thesis Ilya Sutskever utoronto ca Software tools code eight bits enough Deep Neural Networks Interesting article levels precision needed deep neural networks work appropriately petewarden com Brains Neurons brain makes decisions Researchers EPFL developed first biologically plausible model handle non-Markovian decision-making deepstuff org Major milestone reached Brain Preservation Challenge team Heidelberg come technique preserve mouse brain ultrastructure level preservation make possible perform electron microscopic imaging entire connectome Huge milestone brainpreservation org inspiration Hacking epic NHL goal celebration Using real-time machine learning detect goals occur live NHL games Fran ois Maillet developed nice hack create hue light show Philips hue lights Awesome idea great post blog implementation details francoismaillet com newsletter weekly collection best news resources Artificial Intelligence Machine Learning find worthwhile please forward friends colleagues share favorite network Share Twitter Share Linkedin Share Google Suggestions comments welcome reply email Thanks Previous issue May 21st 2015 3 3 Subscribe Email Email Subscribe spam ever email address ever used Artificial Intelligence Weekly 2015 Artificial Intelligence Weekly Terms Use Privacy Policy"),
('The Wolfram Language Image Identification Project', 'Wolfram Language Image Identification Project Announcement Blog Post works FAQS Wolfram Language Powered Wolfram Cloud Wolfram com WolframAlpha com Wolfram Sites Wolfram Terms use Privacy policy Contact us'),
('difference between Latent and Explicit Semantic Analysis', "current community chat blog Stack Overflow Meta Stack Overflow Stack Overflow Careers communities Sign log customize list stack exchange communities Stack Exchange sign log tour help Tour Start quick overview site Help Center Detailed answers questions might Meta Discuss workings policies site stack overflow careers Stack Overflow Questions Tags Users Badges Unanswered Ask Question Take 2-minute tour Stack Overflow question answer site professional enthusiast programmers It's 100 free registration required difference Latent Explicit Semantic Analysis vote 4 vote favorite 1 I'm trying analyse paper ''Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis'' One component system described therein I'm currently grappling difference Latent Explicit Semantic Analysis I've writing document encapsulate understanding it's somewhat cobbled together sources don't 100 understand I'd like know I've come accurate implementing process like singular value decomposition SVD Markov chain Monte Carlo machines corpus documents partitioned basis inherent characteristics assigned categories applying different weights features constitute singular data index highdimensional space often difficult determine combination factors leading outcome result variables interest hidden latent defining set humanly intelligible categories e Wikipedia article pages basis comparison Gabrilovich et al 2007 devised system whereby criteria used distinguish datum readily comprehensible text note semantic analysis explicit sense manipulate manifest concepts grounded human cognition rather latent concepts used Latent Semantic Analysis established Explicit Semantic Analysis opposition Latent Semantic Analysis accurate Info topic somewhat sparse question ostensibly deals similar issue though really machine-learning nlp share improve question edited May 14 9 22 asked May 14 9 04 Matthew_English 779215 really programming may want migrate question stats cel May 14 9 08 1 certainly programming it's algorithm programming algorithms encompassed moreover precedent asking questions one linked 600 views demonstrating communty interest inquireis Matthew_English May 14 9 11 Note many machine learning algorithms require deep understanding statistical background - Yes right it's completely off-topic believe - good answer question don't need good programmer need someone good statistics cel May 14 9 17 that's true they're disjoint sets site mind Matthew_English May 14 9 18 add comment 2 Answers 2 active oldest votes vote 1 vote difference Latent Semantic Analysis so-called Explicit Semantic Analysis lies corpus used dimensions vectors model word meaning Latent Semantic Analysis starts document-based word vectors capture association word documents appears typically weighting function tf-idf reduces dimensionality word vectors generally 300 using Singular Value Decomposition contrast original dimensions corresponded documents 300 new dimensions straightforward interpretation therefore called latent LSA used classify texts combining vectors words text paper mention understand Explicit Semantic Analysis document-based model well models words terms Wikipedia articles appear differs Latent Semantic Analysis however corpus Wikipedia cannot chosen freely b dimensionality reduction involved vectors words text combined classify otherwise interpret text share improve answer answered May 14 11 40 yvespeirsman 700212 according understanding traditional bag word model would kind global dictionary texts would evaluated basis ability populate global dictionary way many concepts it's global dictionary sorts texts evaluated determine Wikipedia concept resemble understanding _________________________________________________________ anything said paragraph grey one OP innacurate Matthew_English May 14 11 58 1 comment paragraph grey correct add LSA concepts 300 dimensions reduced matrix dictionary concepts less easily interpretable Wikipedia concepts yvespeirsman May 14 12 12 add comment find question interesting Try newsletter Sign newsletter get top new questions delivered inbox see example Subscribed Success Please click link confirmation email activate subscription vote 1 vote simple explanation would ESA - uses knowledge-base like wikipedia create inverted index maps words contents e title wikipedia page word occurs operates vector representation words word vector titles 0 1 LSA - uses Singular Value Decomposition principle project word-doc matrix lower ranked space dot product word-doc vector representation words co-occur document co-occur similar set words e Imagine Cat Car never co-occur document occurs Man document D_1 Car co-occurs Man document D_2 higher share improve answer edited May 16 3 18 answered May 15 9 13 user3639557 456110 add comment Answer draft saved draft discarded Sign log Sign using Google Sign using Facebook Sign using Stack Exchange Post guest Name Email Post guest Name Email discard posting answer agree privacy policy terms service answer you're looking Browse questions tagged machine-learning nlp ask question asked 14 days ago viewed 42 times active 12 days ago Blog Stack Overflow Andela partner provide education beyond borders Linked 4 Explicit Semantic Analysis Related 2 pre-built matrices latent semantic analysis 9 Latent Semantic Analysis concepts 6 Latent Semantic Analysis Python discrepancy 2 WordNet semantic relationship different parts object -2 difference evaluation metrics features relation binary classification 1 meaning implication matrices generated Singular Value Decomposition SVD Latent Semantic Analysis LSA 0 Scalability Latent Semantic Analysis WEKA 0 difference Named Entity Recognition Named Entity Extraction 1 Latent Semantic Analysis Handle Semantics -1 Regularized Latent Semantic Indexing R Hot Network Questions good testers competing see opens bugs correct word 'learnful' Typocaptcha - alternative CAPTCHA don't know whether staff member PhD address Dr emails terrible idea aim lower journal paper Single word describes boat stop drop point Rock Paper Scissors - Alter Egos DS cartridge batteries run dry Jorah know sound like pedant seemingly simple complicated questions huge symbol door Truth realm Neon Genesis Evangelion Opening Handling arrow keys console menu single word encompasses person's social media Internet presence affected area buff question glycolysis Deleting files maintenance plan attackers get anything DoS attacks except crashing service way encoding cryptographic hashes safe Firefox accusing distributing malware site Can't use inline array c Phrase mean fully prepared Decomposition partial fractions compute integral Tetris puzzle solvable worth - chest nickels half chest dimes hot questions question feed tour help blog chat data legal privacy policy work advertising info mobile contact us feedback Technology Life Arts Culture Recreation Science Stack Overflow Server Fault Super User Web Applications Ask Ubuntu Webmasters Game Development TeX - LaTeX Programmers Unix Linux Ask Different Apple WordPress Development Geographic Information Systems Electrical Engineering Android Enthusiasts Information Security Database Administrators Drupal Answers SharePoint User Experience Mathematica Salesforce 14 Photography Science Fiction Fantasy Graphic Design Seasoned Advice cooking Home Improvement Personal Finance Money Academia 10 English Language Usage Skeptics Mi Yodeya Judaism Travel Christianity Arqade gaming Bicycles Role-playing Games 21 Mathematics Cross Validated stats Theoretical Computer Science Physics MathOverflow 7 Stack Apps Meta Stack Exchange Area 51 Stack Overflow Careers site design logo 2015 stack exchange inc user contributions licensed cc by-sa 3 0 attribution required rev 2015 5 28 2614 Stack Overflow works best JavaScript enabled"),
('using SVM and SVD based feature selection to predict search results relevance @CrowdFlower @kaggle', 'Host Competitions Scripts Jobs Community User Rankings Forum Blog Wiki Sign Login Log Remember Forgot Username Password 20 000 497 teams Search Results Relevance Enter Merge 29 Jun 32 days Deadline new entry team mergers Mon 11 May 2015 Mon 6 Jul 2015 39 days go Dashboard Home Data Make submission Information Description Evaluation Rules Prizes Timeline Forum Scripts Leaderboard Scripts Feedback New Script votes Beating Benchmark 0 57 Language Latest Run Fork script released Apache 2 0 open source license Queue Time Running Time Cancel Run Output Log Info Versions Run Succeeded Exit Code Failure Message Timeout Exceeded Run Time Seconds Used Space Docker Image Name Dockerfile Image Id Forked version saved 2015 Kaggle Inc Team Careers Terms Privacy Contact Support'),
('Google Prediction API: a Machine Learning black box for developers', "CloudAcademy BlogCloud AcademyHow worksPlans PricingVideo CoursesAWS CertificationsCommunityFAQs Cloud Academy Blog Google Prediction API Machine Learning black box developersGoogle Prediction API Machine Learning black box developersMay 12 2015 Alex CasalboniGoogle Prediction API provides RESTful interface build Machine Learning modelsThis third article build Machine Learning models Cloud previously explored Amazon Machine Learning Azure Machine Learning relative newcomers cloud data market Google Prediction API hand released way back 2011 offers stable simple way train Machine Learning models via RESTful interface although might seem less friendly generally prefer browser interfaces going explore wide range services offered Google Cloud Platform easily check Developers Console free sign Free Trial offered Google 300 credit use 2 months check Cloud Academy courses Google Cloud Platform Google Prediction API Machine Learning Black BoxWe define Google approach black box since get control happens hood model configuration restricted specifying Classification vs Regression providing preprocessing PMML Predictive Model Markup Language file set weighting parameters case categorical models Let clarify basic concepts help specifically Google Prediction API need Regression whenever target output numerical continuous variable may may span specific range e price car age person etc Classification need whenever target output assume limited set values either numbers strings based application context Binary Classification special case target output assume two values let say True False simpler accurate models built cases building set binary models combining output might perform better single multi-class model hand input features columns contain type data although certain types easier work e text analysis clearly complex numerical regression good news Google doesn impose arbitrary constraints input data types require configuration process need format dataset right way Think big table row input vector first column target value need upload single CSV file Google Prediction API take care types detection values normalization features selection etc Google Prediction API first step uploading datasetThe Google Cloud service need order use Google Prediction API Cloud Storage store dataset need enable Console since automatically enabled every Google Cloud project First create new Project choose name ID optionally datacenter location previous articles AmazonML AzureML going train model HAR Human Activity Recognition using open dataset built UCI freely available dataset composed 10 000 records one defined 560 input features one target column take one following values 1 2 3 4 5 6 walking walking upstairs walking downstairs sitting standing laying Every record generated smartphone using accelerometer gyroscope data labelled manually based performer activity going build multi-class model understand whether given record sensor data generated real time definitively associated walking standing sitting laying etc model might useful things like activity tracking healthcare monitoring already gone process manipulating original dataset create one single CSV file since original dataset split smaller datasets training testing input features separated target values find Python script next step involves uploading dataset file Google Cloud Storage Developers Console clicking Storage Cloud Storage Storage Browser side menu want create new bucket e folder select upload file take file contains 90MB data ve got slow network connection might try upload smaller portion dataset model accuracy pretty good 20 Ground Truth use Google Prediction APIUnfortunately Google Prediction API doesn provide user-friendly Web interface almost every step beyond point performed using Python scripts via API call really stand coding might use official APIs Explorer want build products right making real API calls need enable Google Prediction API project find clicking APIs auth APIs last one item Google Cloud APIs list Enabling API quite straightforward need projects single click Enable API job One last step need create new oAuth2 Client ID Google APIs use oAuth2 authentication either create Service account key server server applications Web Application Client case since need work users data going use server server key Eventually might use WebApp Client ID even server server application code end slightly complicated need go typical oAuth flow either using browser copying pasting oAuth codes terminal Let proceed Click APIs auth Credentials Create new Client ID Select Service account popup confirm creation automatically download JSON file containing new Client ID data including client_email private_key' open file use two fields code Google Prediction API Model Training PhaseWe finally ready use Google Prediction API going work excellent official Python client even documentation might sometimes misleading Also going show small code segments focus sub-task find whole script see official documentation either use Hosted Model train ones order train new model going use insert API method Every Google Prediction API method takes project ID first parameter Trainedmodels Insert method expects body parameter containing model ID choose model type classification regression dataset either Cloud Storage location set instances Google Prediction API Model Training PythonPython train new classification model api trainedmodels insert project project_id body 'id' model_id 'storageDataLocation' 'machine-learning-dataset dataset csv' 'modelType' 'CLASSIFICATION' execute 123456 train new classification modelapi trainedmodels insert project project_id body 'id' model_id 'storageDataLocation' 'machine-learning-dataset dataset csv' 'modelType' 'CLASSIFICATION' execute Optionally specify following parameters sourceModel ID existing model case want clone storagePMMLLocation preprocessing file PMML format utility weighting function categorical models course training phase asynchronous need check model status using Trainedmodels get method long model trainingStatus property DONE won able use model good enough start generating new predictions might want first analyze see kind accuracy expect call Trainedmodels analyze method given lot useful information model Google Prediction API Model analysisPython retrieve new model's analysis analysis api trainedmodels analyze project project_id id model_id execute 12 retrieve new model's analysisanalysis api trainedmodels analyze project project_id id model_id execute API call returns insights dataset dataDescription providing three numerical statistics input feature count mean variance might useful aren anything special could computed without creating new model really need modelDescription field Indeed contains confusionMatrix structure Although easy read JSON format structure tell model behaves order process Google split dataset two smaller sets first one used train model second one evaluate math based values dataset size notice Google applied 90 10 split Google Prediction API Model Analysis JSONJavaScript 'confusionMatrix' '1' '1' '166 00' '2' '0 00' '3' '1 00' '4' '0 00' '5' '0 00' '6' '0 00' '2' '1' '0 00' '2' '164 00' '3' '0 00' '4' '0 00' '5' '0 00' '6' '0 00' '3' '1' '0 00' '2' '2 00' '3' '158 00' '4' '0 00' '5' '0 00' '6' '0 00' '4' '1' '0 00' '2' '0 00' '3' '0 00' '4' '161 00' '5' '5 00' '6' '1 00' '5' '1' '0 00' '2' '0 00' '3' '0 00' '4' '9 00' '5' '180 00' '6' '0 00' '6' '1' '0 00' '2' '0 00' '3' '0 00' '4' '0 00' '5' '0 00' '6' '196 00' 'confusionMatrixRowTotals' '1' '167 00' '2' '164 00' '3' '160 00' '4' '167 00' '5' '189 00' '6' '196 00' 'modelinfo' 'kind' 'prediction training' 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263 'confusionMatrix' '1' '1' '166 00' '2' '0 00' '3' '1 00' '4' '0 00' '5' '0 00' '6' '0 00' '2' '1' '0 00' '2' '164 00' '3' '0 00' '4' '0 00' '5' '0 00' '6' '0 00' '3' '1' '0 00' '2' '2 00' '3' '158 00' '4' '0 00' '5' '0 00' '6' '0 00' '4' '1' '0 00' '2' '0 00' '3' '0 00' '4' '161 00' '5' '5 00' '6' '1 00' '5' '1' '0 00' '2' '0 00' '3' '0 00' '4' '9 00' '5' '180 00' '6' '0 00' '6' '1' '0 00' '2' '0 00' '3' '0 00' '4' '0 00' '5' '0 00' '6' '196 00' 'confusionMatrixRowTotals' '1' '167 00' '2' '164 00' '3' '160 00' '4' '167 00' '5' '189 00' '6' '196 00' 'modelinfo' 'kind' 'prediction training' admit percentage values would easier read maybe precision recall statistics would also nice always compute let say intuitively see lot zeros around higher numbers main diagonal Every non-zero value outside main diagonal means model wrongly classified N records dataset applied data split Confusion Matrix looks C MatrixClass 1Class 2Class 3Class 4Class 5Class 6Class 199 40 00 60 000Class 20100 00 0000Class 301 25 98 75 000Class 400096 40 3 00 0 60 Class 50004 80 95 20 0Class 600000100 00 bad considering required effort total absence configuration data normalization Google successfully created reliable model generate new predictions based new unlabelled data generate new PredictionsIn order simplify demo assuming already computed every input feature smartphone sent server stored local CSV file Therefore reading file calling Trainedmodels predict takes csvInstance input form simple list values Google Prediction API realtime PredictionsPython activities dict labels '1' 'walking' '2' 'walking upstairs' '3' 'walking downstairs' '4' 'sitting' '5' 'standing' '6' 'laying' read new record local file open 'record csv' f record_str f readline obtain new prediction prediction api trainedmodels predict project project_id id model_id body 'input' 'csvInstance' record_str split ' ' execute retrieve classified label reliability measures label prediction get 'outputLabel' stats prediction get 'outputMulti' show results print currently class labels label label print stats 123456789101112131415161718192021222324 activities dictlabels '1' 'walking' '2' 'walking upstairs' '3' 'walking downstairs' '4' 'sitting' '5' 'standing' '6' 'laying' read new record local filewith open 'record csv' f record_str f readline obtain new predictionprediction api trainedmodels predict project project_id id model_id body 'input' 'csvInstance' record_str split ' ' execute retrieve classified label reliability measureslabel prediction get 'outputLabel' stats prediction get 'outputMulti' show resultsprint currently class labels label label print stats API call pretty fast respect ML services return following outputLabel predicted class case classified activity outputMulti list reliability measure class model average accuracy high enough take outputLabel prediction result case blindly trust model make advanced decisions based application context may want inspect outputMulti take final decision based class reliability measure Google Prediction API next believe Google black box reached pretty high level abstraction developers although flexible dataset configuration better analysis visualization would make product easier use everyone especially non-coders One nice feature always keep model updated adding new data fly without going whole training phase especially nice systems span long periods time easily adapt model new data conditions without need new modelling phase far speed performance Google Prediction API seems like great candidate real-time predictions respect ML services open dataset achieved highest accuracy taking couple minutes training average response time 1 3 seconds real-time predictions Become Cloud expert Annual MembershipNew courses every week 100 May 31st Start 7 day Free Trial Alex CasalboniI Software Engineer great passion music web technologies I'm experienced web development software design particular focus frontend UX sound music engineering background allows deal multimedia signal processing machine learning AI lot interesting tools even powerful merge Cloud Posts - WebsiteFollow Latest E-booksSign Cloud Academy Start Freeor social sign Learn AWS Cloud Academy Labs Learn Cloud Computing Cloud Computing Fundamentals Become AWS Certified AWS Certifications Guide Amazon Web Services AWS Microsoft Azure Docker Open-Source TechSubscribe newsletterStay updated Cloud Computing news weekly learning material Follow Cloud Cloud Academy Blog Google Prediction API Machine Learning black box developersAbout Sitemap Cloud FundamentalsCopyright 2013-2015 Cloud Academy rights reserved Share Facebook Twitter Google LinkedIn"),
('A Private Machine Learning Model for Your App Company (ask us anything, using scikit)', 'Blog apptheta com Private Machine Learning Model App Company Wednesday May 13 App Theta creates competitor revenue models app companies dual business offering - free open vs paid private couple notes Building App Theta Made Private Models Server Stack Private Models Framework Building App Theta co-founder Dan Felix specialist machine learning used work Google building anti-fraud systems machine learning built back-end systems including data collection modeling framework private models many companies Also working back-end another ex-Googler wife Anna Hentzel Johnson far focused data collection systems engineering also far beyond ken comes algorithms strong engineer around work amazing enjoy putting user interface top Made Private Models Let first set stage wanted build private models - custom data algorithms predict app company competitors sales paid plan data kept public model get one private models d understand biggest competitor AppAnnie com app company uses AppAnnie track sales data bundled sold tradeoff many companies happy make get dashboard exchange data side buying app revenue data AppAnnie separate product AppAnnie profits approach different app developer App Theta model accommodates 1 small early companies want best free service 2 large companies also want data privacy best sales dashboard app company Gaia GPS like 1 even see real sales dashboard demo App Theta Large game studios hand want analysis in-house Server Stack Today back-end stack consists Django Python Postgres running Amazon using Celery task scheduling couple jobs going time gather rankings data signals collate process report data users Rankings info plus sales data main basis models Dan built mini-framework around Python SciKit multiple models - one larger public model private models various companies use open data private data setup lets us customize algorithms data anyone expect infrastructure evolve greatly time already lets us really neat analysis bunch cool companies'),
('Deep Learning for Image Understanding in Planetary Science', 'Cuda zone menu Getting Started Downloads Training Ecosystem Search CUDA Zone Register Login Main menu Skip primary content Skip secondary content Features CUDACasts CUDA Pro Tips CUDA Spotlights Next Previous Deep Learning Image Understanding Planetary Science Share Posted May 12 2015 Brad Nemire Comments Tagged CUDA cuDNN Deep Learning Image Processing Machine Learning Went training 700 img MNIST 1500 img using CUDA 4000 img using cuDNN freaking amazing GPUComputing Leon Palafox leonpalafox March 27 2015 stumbled upon tweet Leon Palafox Postdoctoral Fellow University Arizona Lunar Planetary Laboratory reached discuss success GPUs share developers interested using deep learning image processing Tell us research University Arizona Leon Palafox working developing tool automatically identify various geological processes surface Mars Examples geological processes include impact cratering volcanic activity however processes generate landforms look similar even though form via vastly different mechanisms example small impact craters volcanic craters easily confused exhibit prominent rim surrounding central topographic depression particular interest research group automated mapping volcanic rootless cones Figure 2 shows landforms generated explosive interactions lava ground ice therefore mapping global distribution rootless cones Mars would contribute better understanding distribution near-surface water planet However must first develop algorithms correctly distinguish landforms similar appearance difficult task planetary geologists already great success applying state-of-the-art artificial neural networks data acquired High Resolution Imaging Science Experiment HiRISE camera onboard Mars Reconnaissance Orbiter MRO satellite Figure 1 view Mars centered Elysium Planitia includes youngest volcanic terrains planet Performing systematic regional survey sub-kilometer-scale landforms volcanic rootless cones would prohibitively time consuming using manual methods ideal Machine Learning algorithms Convolutional Neural Networks CNNs status project project development phase expect completed one two years depending number features wish train spent much processing time processing images CNN thanks NVIDIA cuDNN library substantially reduced time focusing identification volcanic rootless cones impact craters plan extend search include landforms like sand dunes recurring slope lineae thought formed seasonal seeps surface water cloud formations particular interest dynamic phenomena developed robust identification algorithm apply time series satellite observations investigate Martian environment changes time Mars provides ideal place develop test approaches ultimate aim apply similar techniques study Earth Figure 2 example CNN classification subsection HiRISE image PSP_002292_1875 shows input layer B shows output CNN classification volcanic rootless cones within scene output layer volcanic rootless cones outlined red algorithm demonstrates 96 classification accuracy Applying classification method regional distribution volcanic rootless cones within Elysium Planitia made possible GPU computing Without GPU computing classification took 1 hour 20 minutes GPU computing took 90 seconds applying CUDA platform project used MATLAB approach access CUDA library since present implementation MATLAB use MatConvNet framework like Theano Caffe provides great set tools build deploy Convolutional Neural Network CNN architectures also provides great CUDA interfaces cuDNN library still fine tuning libraries essence use CNN similar LeNet albeit modified work particular regime also running five CNNs parallel using different pixel sizes search differently scaled features image GPUs using five machines two NVIDIA Quadro K5000s share visual results Figures 1 3 show output region Elysium Planitia use CNN map location volcanic rootless cones process done larger scale incredible tool understanding geologic history Mars processed five CNNs looking features different scales finally pooled generate contour map Figure 3 example volcanic rootless cones Mars landforms generated explosive lava water interactions provide evidence former distribution ground-ice image composite showing orthorectified grayscale HiRISE image fused stereo-derived digital terrain model Identifying global distribution rootless cones using machine learning techniques would great advance toward understanding locations near-surface water Mars time data use training CNN far trained 800 examples Martian landforms selected full-resolution HiRISE images HiRISE image typically resolution 0 25 m pixel covers swath 6-km-wide resulting file sizes 3 4 GB Individual HiRISE images spectacular impressive data products digital terrain models generated using stereo-photogrammety Figure 3 data enables us visualize surface Mars three dimensions generate simulated illumination images used expand natural training sets Additionally using trained Convolutional Neural Networks CNNs examine thousands HiRISE images within Elysium Planitia volcanic region includes youngest lava flows Mars covering total millions square kilometers Figure 1 Performing manual search every rootless cone region would prohibitively time-consuming Fortunately automated approaches enable us map landforms vast area use results systematic survey infer regional distribution former ground-ice deposits Elysium Planitia first time biggest challenge project project many challenges algorithm implementation analysis results think biggest challenge readily available databases use train different features surface Mars databases consistent unlike computer vision community MNIST CIFAR standards good bad good sense allows tackle real-world problem state-of-the-art tools since databases consistent lot skepticism community whether approach work features surface However planetary science situation different data collected instruments like HiRISE made freely available public standardized form NASA Planetary Data System PDS GPU Computing experience GPUs impacted research ve used CUDA intensively previous research focused processing kinds data images couple classes projects used CUDA first time really became critical optimize efficiency image analyses approach investigate problem large scale short time access powerful GPU greatly reduces amount time need process images need analyze images HiRISE database consists 35 000 grayscale color images total database size 25 TB need apply different types CNNs classify correctly choose suitable architecture particular problem Without using GPUs would take days finish processing single image recent results shown hour two enough process single image using GPU diverse research background taken advantage access different research groups topics allowed use experience gained Machine Learning topics one area apply different way different area example work Bayesian networks applied gene networks huge applicability previous work UCLA EEG data Brain Machine Interfaces surprisingly enough work time series analysis also find use planetary sciences using things like Hidden Markov Models model various terrain profiles machine learning changed last years Machine learning gone niche research-oriented area 1990s boom industry past decade Part explosion readily available data due information revolution last several years Many companies investing large amount resources data science divisions like Facebook created machine learning laboratory year ago gotten people excited using machine learning tools since increases value job market without downside since many people apply Machine Learning software without knowing nuts bolts process could result disappointment companies future realizing classifiers tools tuned particular datasets ve seen fair share implementations preanalysis data whatsoever used tuning parameters textbook example think next years sophisticated Machine Learning tools become available work oriented toward large datasets effort put training people algorithms work rather using terms technology advances looking forward next five years think pervasive use unmanned aerial vehicles UAVs would amazing readily available running algorithms like CNNs feature recognition would allow us real-time information natural disasters riots local events imagine UAV large setting like Coachella Valley Music Arts Festival football stadium would allow organizers better control flow people real time prevent accidents UAV using CNN track wildfires would allow us information spread cases stop prevent privacy implications still concern think much gain technology mounting NVIDIA cards would even better since could real-time image processing without need transmit video Wi-Fi success story like Leon involves GPUs comment tell us research love reading sharing developer stories cuDNN Deep Learning GPUs learn deep learning GPUs visit NVIDIA Deep Learning developer portal Check related posts especially intro cuDNN Accelerate Machine Learning cuDNN Deep Neural Network Library sure check cuDNN Webinar Recording GPU-Accelerated Deep Learning cuDNN interested embedded applications deep learning check post Embedded Machine Learning cuDNN Deep Neural Network Library Jetson TK1 Related PostsAccelerate Machine Learning cuDNN Deep Neural Network LibraryDeep Learning Computer Vision Caffe cuDNNUnderstanding Natural Language Deep Neural Networks Using TorchDeep Speech Accurate Speech Recognition GPU-Accelerated Deep Learning parallel Share Brad Nemire Brad Nemire recently joined NVIDIA ARM Accelerating Computing developer marketing team Brad holds BA San Diego State University currently lives San Jose CA Follow BradNemire Twitter View posts Brad Nemire Subscribe RSS Email Connect Follow gpucomputing X Enter email address Subscribe ResourcesAbout Parallel Forall NVIDIA Developer Forums CUDA Newsletter Recent Posts Introduction Neural Machine Translation GPUs part 1 Accelerate NET Applications Alea GPU Deep Learning Image Understanding Planetary Science GPU Pro Tip Track MPI Calls NVIDIA Visual Profiler Parallel Direct Solvers cuSOLVER Batched QR TweetsRecent CommentsJimmy Ren Introduction Neural Machine Translation GPUs part 1 Alex Rothberg Jetson TK1 Mobile Embedded Supercomputer Takes CUDA EverywhereMark Harris GPU Pro Tip CUDA 7 Streams Simplify Concurrencytoni GPU Pro Tip CUDA 7 Streams Simplify ConcurrencyMark Harris GPU Pro Tip CUDA 7 Streams Simplify Concurrency CUDA ZONE GAMEWORKS EMBEDDED COMPUTING PROFESSIONAL GRAPHICS GET STARTED CUDA Parallel Computing CUDA Toolkit CUDACast LEARN Training Courseware Tools Ecosystem Academic Collaboration Documentation GET INVOLVED Forums Parallel Forall Blog Developer Program Contact Us Copyright 2015 NVIDIA Corporation Legal Information Privacy Policy'),
('Deep Learning Machine Solves the Cocktail Party Problem | MIT Technology Review', "Global Edition Insider Magazine Business Reports Lists Events Newsletters Support Contact Us Emtech MIT MIT Enterprise Forum Connect Select localized edition Close English Espa ol Deutsch Italiano Portugu Filter News Analysis Magazine List Innovators 35 Conferences Events Argentina Brasil Colombia Deutschland Italia M xico Portugal Espa United States Uruguay Pan Arab Ways Connect Discover one 28 local entrepreneurial communities first know launch new countries markets around globe Interested bringing MIT Technology Review local market Search Log Join New Popular Login Join MIT Technology Review Home News Analysis Features Views Multimedia Discussions Topics Popular Bladeless Wind Turbines Cleaning China Coal Recovering Robots Firefox Struggles Asimov Exclusive HP Machine Dreams Coding Culture Subscribe Buy June 1-2 2015St Regis HotelSan Francisco Register Home Home Biomedicine Business Computing Energy Materials Mobile Top Stories Business Report Food Robots Tom Simonite 0 Robots Walk Injuries Researchers Say Caleb Garling Bladeless Wind Turbines Future Phil McKenna Microsoft HoloLens Headset Make See People Aren Tom Simonite See Top Stories Menu News Analysis Magazine Lists Events Features Views Multimedia Discussions Topics Popular Bladeless Wind Turbines Cleaning China Coal Recovering Robots Firefox Struggles Asimov Exclusive HP Machine Dreams Coding Culture Current Issue Past Issues MIT News Magazine International Editions Business Reports Special Publications Subscriptions Smartest Companies Innovators 35 Breakthrough Technologies Nominations Upcoming Events Partner Events Emtech MIT MIT Enterprise Forum Team Join Us Press Room Awards Advertise Permissions Contact Us Customer Support Support Contact Us Emtech MIT MIT Enterprise Forum Insider Connect Latest Popular Shared Profile Unsupported browser browser meet modern web standards See scores action text Comments Email Service Service Share Print Deep Learning Machine Solves Cocktail Party Problem View 6 comments Emerging Technology arXiv April 29 2015 Deep Learning Machine Solves Cocktail Party Problem Separating singer voice background music always uniquely human ability anymore cocktail party effect ability focus specific human voice filtering voices background noise ease humans perform trick belies challenge scientists engineers faced reproducing synthetically large humans easily outperform best automated methods singling voices particularly challenging cocktail party problem field music humans easily concentrate singing voice superimposed musical background includes wide range instruments comparison machines poor task Today looks changing thanks work Andrew Simpson pals University Surrey U K guys used recent advances associated deep neural networks separate human voices background wide range songs approach showcases huge advances made recent years machine learning neural networks paves way general solution famous cocktail party problem allow among things vocals easily separated music accompany method guys use relatively straightforward start database 63 songs available set individual tracks contain different instrument voice well fully mixed version song Simpson co divide track 20-second segments create spectrogram shows frequencies sound vary time result kind unique fingerprint identifies instrument voice also create spectrogram fully mixed version song essentially component spectrograms added together task picking voice mixture essentially task separating voice unique spectrogram spectrograms present Simpson co trained deep convolutional neural network exactly used 50 songs train network keeping remaining 13 test total generated 20 000 spectrograms training purposes task neural network simple input gave fully mixed spectrogram expected produce essentially vocal spectrogram output task kind machine learning one parameter optimization deep neural network billion parameters need tuned way produces desired output process optimization learning occurs iteration network begins parameters set randomly gradually improves settings time scans database hundred iterations found good setup network Simpson co gave 13 songs seen test well could separate vocals mix outputs turned impressive results demonstrate convolutional deep neural network approach capable generalizing voice separation learned musical context new musical contexts say team Simpson co even compared results conventional cocktail party algorithm applied data main advantage deep neural network appears general learning vocal sounds say words learned voice sounds like deep neural network use information pick voices mix good approach compared human performance say One immediate application producing music tracks minus vocals karaoke machines clearly errr important goal broader implications well Deep neural networks revolutionizing machine learning wide range areas recently humans clear dominance pattern recognition tasks facial recognition object recognition lead considerably reduced cases lost altogether machines playing catch area cocktail party problems fool would bet triumphing distant future Ref arxiv org abs 1504 04658 Deep Karaoke Extracting Vocals Musical Mixtures Using Convolutional Deep Neural Network 6 comments Share thoughts 0 comments story Start discussion Tagged Computing Reprints Permissions Send feedback editor Related stories may missed author First Computational Imagination ability read description scene picture always uniquely human anymore Continue 0 Computational Aesthetics Algorithm Spots Beauty Humans Overlook 2 Recommended Around Web Week Ending May 23 2015 0 Quantum Life Spreads Entanglement Across Generations 2 Recommended Around Web Week Ending May 16 2015 0 Machine Vision Algorithm Beating Art Historians Game 5 Recommended Around Web Week Ending May 9 2015 1 Recommended Around Web Week Ending May 2 2015 0 Data Fusion Heralds City Attractiveness Ranking 1 Fixing China Coal Problem China rapidly cleaned coal plants comes hard part Continue 3 Food Technology may heading toward new food economy competitive innovative Continue 17 Continuous Productivity Aaron Levie CEO Box building online file storage system designed reshape industries Continue 9 Mobile Call Quality Gets Long-Overdue Upgrade Wireless companies ambitious startups racing make cell-phone calls better Continue 15 Firefox Maker Battles Save Internet Mozilla helped open Web flourish 2000s struggling play meaningful role mobile devices Continue 19 Owns Friends Social-networking sites fighting control users personal information Continue 0 Emerging Technology arXiv Contributor View Profile Follow arxivblog RSS First Computational Imagination 0 Interesting arXiv Papers Week ending May 23 2015 0 Computational Aesthetics Algorithm Spots Beauty Humans Overlook 2 Machine-Learning Algorithm Mines Rap Lyrics Writes 11 Quantum Life Spreads Entanglement Across Generations 2 Interesting arXiv Papers Week ending May 16 2015 1 Machine-Learning Algorithm Calculates Fair Distance Race Usain Bolt Long-Distance Runner Mo Farah 0 History's Greatest Chess Players Reveal Secret Formula Behind Fame 7 See author Latest Popular Shared White Paper Securing Big Data Life Cycle Produced partnership Oracle View Marketplace Diversity Big Data Sources Creates Big Security Challenges Produced partnership Oracle 3 hours ago Robots Start Grasp Food Processing 5 hours ago First Computational Imagination 1 day ago Research Shows Algorithm Help Robots Heal Risk Management Today Trends Tools Thought Leadership Series Provided 1 day ago Bladeless Wind Turbine Skeptics 1 day ago Managed Right China Coal Habit Could Less Damaging Fear 2 days ago Technology Reshaping Food Chain 3 days ago Hear Mobile Calls Still Subpar Changing 5 days ago Interesting arXiv Papers Week ending May 23 2015 Internet Everything Everyone Brought 6 days ago Computational Aesthetics Algorithm Spots Beauty Humans Overlook 6 days ago Seven Must-Read Stories Week Ending May 23 2015 Listen new podcast preview ll experience EmTech Digital Innovations Ideas Insights Provided 6 days ago Mozilla Halt Firefox Slide Break Mobile Internet Duopoly 6 days ago Recommended Around Web Week Ending May 23 2015 1 week ago Machine-Learning Algorithm Mines Rap Lyrics Writes 1 week ago Microsoft HoloLens Augmented Reality Headset Put Virtual Acrobats Golfers Living Room 1 week ago Teleport Facial Expressions Virtual Reality See full archive View Marketplace Diversity Big Data Sources Creates Big Security Challenges Produced partnership Oracle 3 hours ago Robots Start Grasp Food Processing 5 hours ago First Computational Imagination 1 day ago Research Shows Algorithm Help Robots Heal 1 day ago Bladeless Wind Turbine Skeptics 1 day ago Managed Right China Coal Habit Could Less Damaging Fear 2 days ago Technology Reshaping Food Chain 3 days ago Hear Mobile Calls Still Subpar Changing 5 days ago Interesting arXiv Papers Week ending May 23 2015 6 days ago Computational Aesthetics Algorithm Spots Beauty Humans Overlook 6 days ago Seven Must-Read Stories Week Ending May 23 2015 6 days ago Mozilla Halt Firefox Slide Break Mobile Internet Duopoly 6 days ago Recommended Around Web Week Ending May 23 2015 1 week ago Machine-Learning Algorithm Mines Rap Lyrics Writes 1 week ago Microsoft HoloLens Augmented Reality Headset Put Virtual Acrobats Golfers Living Room 1 week ago Teleport Facial Expressions Virtual Reality See full archive View Marketplace Diversity Big Data Sources Creates Big Security Challenges Produced partnership Oracle 3 hours ago Robots Start Grasp Food Processing 5 hours ago First Computational Imagination 1 day ago Research Shows Algorithm Help Robots Heal 1 day ago Bladeless Wind Turbine Skeptics 1 day ago Managed Right China Coal Habit Could Less Damaging Fear 2 days ago Technology Reshaping Food Chain 3 days ago Hear Mobile Calls Still Subpar Changing 5 days ago Interesting arXiv Papers Week ending May 23 2015 6 days ago Computational Aesthetics Algorithm Spots Beauty Humans Overlook 6 days ago Seven Must-Read Stories Week Ending May 23 2015 6 days ago Mozilla Halt Firefox Slide Break Mobile Internet Duopoly 6 days ago Recommended Around Web Week Ending May 23 2015 1 week ago Machine-Learning Algorithm Mines Rap Lyrics Writes 1 week ago Microsoft HoloLens Augmented Reality Headset Put Virtual Acrobats Golfers Living Room 1 week ago Teleport Facial Expressions Virtual Reality See full archive Show comments Conversation powered Livefyre New Trending Bladeless Wind Turbines Cleaning China Coal Recovering Robots Firefox Struggles Asimov Exclusive HP Machine Dreams Coding Culture Robots Start Grasp Food Processing First Computational Imagination Algorithm Help Robots Walk Injuries Bladeless Wind Turbines May Offer Form Function Archives Q Paul Otellini 5 years ago Lying Pixels 14 years ago Q Quixotic 8 years ago Direct Dell 13 years ago 2012 2011 2010 2009 2008 1899 Close Introducing MIT Technology Review Insider Already Magazine subscriber You're automatically Insider It's easy activate upgrade account Activate Account Become Insider It's new way subscribe Get even tech news research discoveries crave Sign Learn Find MIT Technology Review Insider explore options Show Place Inspiration Innovations Ideas FuelingOur Connected World June 1-2 2015 Register News Analysis Home Popular Today's News Blogs Photo Galleries Videos Back Top mission MIT Technology Review equip audiences intelligence understand world shaped technology Editions Find preferred version Choose six languages 13 regions worldwide Archives Explore 116 years innovation respected technology publication Lists Discover important people companies technologies shaping future Events Attend one 400 thought-provoking live events worldwide Subscribers MIT Technology Review delivered doorstep desktop tablet Newsletters MIT Enterprise Forum MIT News Magazine EmTech Company Us Work Us Advertise Us Reprints Permissions Account Join View Profile Manage Account Manage Subscription Customer Support Help Support Contact us Feedback Sitemap Connect Twitter LinkedIn YouTube Google StumbleUpon Facebook RSS Mobile MIT Technology Review Ethics Statement Terms Service Privacy Commenting Guidelines 2015 v1 13 05 10"),
('Keras: Fast Deep Learning Prototyping for Python', "Skip content Sign Sign repository Explore Features Enterprise Blog Watch 139 Star 1 910 Fork 242 fchollet keras Code Issues Pull requests Pulse Graphs HTTPS clone URL Subversion checkout URL clone HTTPS Subversion Download ZIP Theano-based Deep Learning library convnets recurrent neural networks http keras io 262 commits 1 branch 0 releases 33 contributors Python 100 0 Python branch master Switch branches tags Branches Tags master Nothing show Nothing show keras Merge pull request 158 jdwittenauer master Add wrapper scikit-learn classifier API latest commit df4e3a2d8d fchollet authored May 27 2015 Permalink Failed load latest commit information docs Merge branch 'master' https github com fchollet keras May 26 2015 examples Improve Otto example May 12 2015 keras Merge pull request 158 jdwittenauer master May 27 2015 test Merge pull request 158 jdwittenauer master May 27 2015 gitignore Update gitignore Mar 29 2015 LICENSE Initial commit Mar 27 2015 README md Update setup py installation instructions May 26 2015 setup py Update setup py installation instructions May 27 2015 README md Keras Theano-based Deep Learning library found Keras Keras minimalist highly modular neural network library spirit Torch written Python Theano deal dearth ecosystem Lua developed focus enabling fast experimentation able go idea result least possible delay key good research Use Keras need deep learning library allows easy fast prototyping total modularity minimalism extensibility supports convolutional networks vision recurrent networks sequence data well combinations two runs seamlessly CPU GPU Read documentation Keras io Keras compatible Python 2 7-3 4 Guiding principles Modularity model understood sequence standalone fully-configurable modules plugged together little restrictions possible particular neural layers cost functions optimizers initialization schemes activation functions dropout standalone modules combine create new models Minimalism module kept short simple 100 lines code Every piece code transparent upon first reading black magic hurts iteration speed ability innovate Easy extensibility New features new module per definition new way combine modules together dead simple add new classes functions existing modules provide ample examples Work Python separate models configuration files declarative format like Caffe PyLearn2 Models described Python code compact easier debug benefits syntax highlighting allows ease extensibility See examples Examples Multilayer Perceptron MLP keras models import Sequential keras layers core import Dense Dropout Activation keras optimizers import SGD model Sequential model add Dense 20 64 init 'uniform' model add Activation 'tanh' model add Dropout 0 5 model add Dense 64 64 init 'uniform' model add Activation 'tanh' model add Dropout 0 5 model add Dense 64 2 init 'uniform' model add Activation 'softmax' sgd SGD lr 0 1 decay 1e-6 momentum 0 9 nesterov True model compile loss 'mean_squared_error' optimizer sgd model fit X_train y_train nb_epoch 20 batch_size 16 score model evaluate X_test y_test batch_size 16 Alternative implementation MLP model Sequential model add Dense 20 64 init 'uniform' activation 'tanh' model add Dropout 0 5 model add Dense 64 64 init 'uniform' activation 'tanh' model add Dropout 0 5 model add Dense 64 2 init 'uniform' activation 'softmax' sgd SGD lr 0 1 decay 1e-6 momentum 0 9 nesterov True model compile loss 'mean_squared_error' optimizer sgd VGG-like convnet keras models import Sequential keras layers core import Dense Dropout Activation Flatten keras layers convolutional import Convolution2D MaxPooling2D keras optimizers import SGD model Sequential model add Convolution2D 32 3 3 3 border_mode 'full' model add Activation 'relu' model add Convolution2D 32 32 3 3 model add Activation 'relu' model add MaxPooling2D poolsize 2 2 model add Dropout 0 25 model add Convolution2D 64 32 3 3 border_mode 'full' model add Activation 'relu' model add Convolution2D 64 64 3 3 model add Activation 'relu' model add MaxPooling2D poolsize 2 2 model add Dropout 0 25 model add Flatten model add Dense 64 8 8 256 model add Activation 'relu' model add Dropout 0 5 model add Dense 256 10 model add Activation 'softmax' sgd SGD lr 0 1 decay 1e-6 momentum 0 9 nesterov True model compile loss 'categorical_crossentropy' optimizer sgd model fit X_train Y_train batch_size 32 nb_epoch 1 Sequence classification LSTM keras models import Sequential keras layers core import Dense Dropout Activation keras layers embeddings import Embedding keras layers recurrent import LSTM model Sequential model add Embedding max_features 256 model add LSTM 256 128 activation 'sigmoid' inner_activation 'hard_sigmoid' model add Dropout 0 5 model add Dense 128 1 model add Activation 'sigmoid' model compile loss 'binary_crossentropy' optimizer 'rmsprop' model fit X_train Y_train batch_size 16 nb_epoch 10 score model evaluate X_test Y_test batch_size 16 Architecture learning image captions convnet Gated Recurrent Unit word-level embedding caption maximum length 16 words Note getting actually work require using bigger convnet initialized pre-trained weights Displaying readable results also require embedding decoder max_caption_len 16 model Sequential model add Convolution2D 32 3 3 3 border_mode 'full' model add Activation 'relu' model add Convolution2D 32 32 3 3 model add Activation 'relu' model add MaxPooling2D poolsize 2 2 model add Convolution2D 64 32 3 3 border_mode 'full' model add Activation 'relu' model add Convolution2D 64 64 3 3 model add Activation 'relu' model add MaxPooling2D poolsize 2 2 model add Convolution2D 128 64 3 3 border_mode 'full' model add Activation 'relu' model add Convolution2D 128 128 3 3 model add Activation 'relu' model add MaxPooling2D poolsize 2 2 model add Flatten model add Dense 128 4 4 256 model add Activation 'relu' model add Dropout 0 5 model add Repeat max_caption_len GRU returns sequences max_caption_len vectors size 256 word embedding size model add GRU 256 256 return_sequences True model compile loss 'mean_squared_error' optimizer 'rmsprop' images numpy array shape nb_samples nb_channels 3 width height captions numpy array shape nb_samples max_caption_len 16 embedding_dim 256 captions supposed already embedded dense vectors model fit images captions batch_size 16 nb_epoch 100 examples folder find example models real datasets CIFAR10 small images classification Convnet realtime data augmentation IMDB movie review sentiment classification LSTM sequences words Reuters newswires topic classification Multilayer Perceptron MNIST handwritten digits classification Multilayer Perceptron Current capabilities complete coverage API check Keras documentation highlights convnets LSTM GRU word2vec-style embeddings PReLU batch normalization Installation Keras uses following dependencies numpy scipy Theano See installation instructions http deeplearning net software theano install html install HDF5 h5py optional required use model saving loading functions Optional recommended use CNNs cuDNN dependencies installed cd Keras folder run install command sudo python setup py install name Keras Keras means horn Greek reference literary image ancient Greek Latin literature first found Odyssey dream spirits Oneiroi singular Oneiros divided deceive men false visions arrive Earth gate ivory announce future come pass arrive gate horn It's play words horn fulfill ivory deceive Keras developed part research effort project ONEIROS Open-ended Neuro-Electronic Intelligent Robot Operating System Oneiroi beyond unravelling --who sure tale tell men look comes pass Two gates give passage fleeting Oneiroi one made horn one ivory Oneiroi pass sawn ivory deceitful bearing message fulfilled come polished horn truth behind accomplished men see Homer Odyssey 19 562 ff Shewring translation Status API Training Shop Blog 2015 GitHub Inc Terms Privacy Security Contact Something went wrong request Please try"),
('Slides Paris Machine Learning Meetup #9, Season 2: ML @Quora and @Airbus and in HFT, Tax, APIs war (Info on streaming but will probaby be in spoken French. Slides in English)', "Nuit Blanche name Igor Carron homepage Page Views Nuit Blanche since July 2010 Follow IgorCarron Cite Nuit Blanche related pages recent Compressive Sensing article Scientific Reports Attendant Project Page Please join comment Google Community 1502 CompressiveSensing subreddit 811 Facebook page LinkedIn Compressive Sensing group 3293 Advanced Matrix Factorization Group 1017 Reference pages include Big Picture Compressive Sensing Learning Compressive Sensing Advanced Matrix Factorization Jungle Page Highly Technical Reference Pages - Aggregators Technologies Exist CAI Cable Igor's Adventures Matrix Factorization search Reproducible Research page Paris Machine Learning Meetup Archives Meetup com register 2222 members LinkedIn post jobs 721 Google 233 Facebook follow-on discussions Twitter Wednesday May 13 2015 Tonight Paris Machine Learning Meetup 9 Season 2 ML Quora Airbus HFT Tax APIs war Email ThisBlogThis Share TwitterShare FacebookShare Pinterest Paris Machine Learning meetup streamed two different platforms two different cameras first one brought us Gerard Pazuelo result traditional google hangout air presentations available meetup French AXA Data Innovation Lab host sponsor us 9th Paris Machine Learning meetup Season 2 program slides slides time meetup starts 6 45pm Paris time Igor Carron Franck Bardol Paris Machine Learning Alberto Bietti Machine learning applications growing world knowledge Quora Abstract understanding quality user generated content providing personalized reading experiences users Quora faces many important questions addressed machine learning talk describe problems machine learning solutions we've built solve Christophe Bourgignat AXA remise les prix du dernier concours DataScience net AXA Joaquin Fernandez-Tapia High-Frequency Trading On-Line Learning Abstract propose optimization framework market-making limit-order book based theory stochastic approximation consider discrete-time variant Avellaneda-Stoikov model 1 similar developent article Laruelle Lehalle Pag 9 context optimal liquidation tactics idea take advantage iterative nature process updating bid ask quotes order make algorithm optimize strategy trial-and-error basis e on-line learning advantage approach exploration system algorithm performed run-time explicit specifications price dynamics necessary case stochastic control approach 7 discussed rationale method extended wider class algorithmic-trading tactical problems market-making http papers ssrn com sol3 papers cfm abstract_id 2594477 Gerard Dupont Airbus Defense Space Traitements avanc de flux documentaires multim dia chez AIRBUS DS - pourquoi et comment Unstructured data processing Practical machine learning intelligence applications machine learning coming tremendous results recent days application unstructured data processing still struggle google enough need specialized web intelligence system arises numerous constraints real world web data hitting hard talk go overview domain challenges ML - currently - put use foreseen next steps Michael Benesty ML use case French tax audit Pr paration d'un contr le fiscal en France par l'utilisation du gradient boosting sur une comptabilit Louis Dorard Machine Learning APIs War Amazon vs Google vs BigML vs PredicSis Related blog entry Amazon ML made lot noise came last month Shortly afterwards someone posted link Google Prediction API HackerNews quickly became one popular posts although Google's API released back 2010 services provide similar APIs gave idea comparing used Kaggle give credit challenge didn stop also included startups provide competing APIs comparison namely PredicSis BigML wave new ML services giant tech companies getting headlines bigger companies necessarily better products Join CompressiveSensing subreddit Google Community post Liked entry subscribe Nuit Blanche's feed there's came also subscribe Nuit Blanche Email explore Big Picture Compressive Sensing Matrix Factorization Jungle join conversations compressive sensing advanced matrix factorization calibration issues Linkedin labels meetup ML MLParis ParisMachineLearning Igor 5 13 2015 12 00 00 comments Post Comment Newer Post Older Post Home Subscribe Post Comments Atom Printfriendly Nuit Blanche Stemming Data Tsunami One Algorithm Time Tweet Igor Search Nuit Blanche Loading Subscribe E-MAIL Nuit Blanche Get new entries directly mailbox enter email address Nuit Blanche Nuit Blanche blog focuses Compressive Sensing Advanced Matrix Factorization Techniques Machine Learning well many engaging ideas techniques needed handle make sense high dimensional data also known Big Data Nuit Blanche french expression translates nighter restless night Contact Cassini MSL Oppy HiRISE SOHO SDO Rosetta-Philae Rosetta Contact igorcarron gmail com Webon LinkedIn Twitter Pages Home Reproducible Research implementations Randomized Numerical Linear Algebra RandNLA Advanced Matrix Factorization Learning Compressed Sensing It's CAI Cable Igor's Adventures Matrix Factorization Machine Learning Meetups Around World Compressed Sensing Pages Focused Interest Pages Datasets Challenges Nuit Blanche Conversations Linking Nuit Blanche blogs CS Meetings Real Time Experiments Highly Technical Reference Pages - Aggregators Recent Nuit Blanche entries Paris Machine Learning Meetup Archives Pinterest Boards Imaging Nature Technologies Exist Wondering Star Computational Photography Subscribe LinkedIn Matrix Factorization Group 1001 members right one Link stats Subscribe Nuit Blanche RSS Feed Posts Atom Posts Comments Atom Comments Subscribe LinkedIn Compressive Sensing group 3145 members right one Link stats Google Badge updated profile LinkedIn reflect activities Nuit Blanche means provide recommendations based experience reading blog Nuit Blanche QR code Search Nuit Blanche LoadingLatest news Compressive Compressed Sensing Arxiv Full Text Search Arxiv Google Compressive Sensing Compressed Sensing 24 hours week month Rice University Compressive Sensing repositoryLatest news Matrix Factorization Arxiv old Arxiv new Google 24 hours week month Readership Statistics another set watching blog feedreaders 740 readers receive every entries mailboxes 600 people come site directly everyday detailed information following blog entries far site seen 3 500 000 pageviews since counter installed 2007 Nuit Blanche Referenced Dead Tree World Big Picture Compressive Sensing mentioned article La Recherche french speaking equivalent competitor Science October 2010 issue page 20-21 Wired Magazine piece Compressed Sensing featuring links blog Big Picture March 1 2010 Emmanuel Candes Terry Tao wrote Nuit Blanche Dec '08 issue IEEE Information Theory Society Newsletter Xiaochuan Pan Emil Sidky Michael Vannier wrote Nuit Blanche commercial CT scanners still employ traditional filtered back-projection image reconstruction Check also acknowledgments Ghost Imaging paper one Like Link Xi'an's Og Toscana 3 - Filed Mountains pictures Running Travel Wines Tagged Chianti farmhouse Italia ruins sunset Tuscany 33 minutes ago Hack Day Eye-Controlled Wheelchair Advances Talented Teenage Hackers - Myrijam Stoetzer friend Paul Foltin 14 15 years old kids Duisburg Germany working eye movement controller wheel chair Th 50 minutes ago Terahertz Technology Abstract-Terahertz response patterned epitaxial graphene - Christian Sorger Sascha Preu1 2 Johannes Schmidt3 Stephan Winnerl3 Yuliy V Bludov4 Nuno M R Peres4 Mikhail Vasilevskiy4 Heiko B Weber1 http 1 hour ago Endeavour Data code regulation - Data code code data distinction software code input data blurry best arbitrary worst distinction 6 hours ago Another Word Cybersecurity Authoritative Reports Resources Topic Need Librarians - Cybersecurity Authoritative Reports Resources Topic Rita Tehan Information Specialist Congressional Research Service summary 18 hours ago Image Sensors World Sony 2015 IR Day - Sony held 2015 Investor Relations Day today Device Segment presentation Tomoyuki Suzuki Executive Deputy President Corporate Executive Offic 21 hours ago free hunch Interactive R Tutorial Machine Learning Titanic Competition - Always wanted compete Kaggle competition sure right skill set DataCamp created free interactive tutorial help 21 hours ago High Noon GMT Oh torn 'twixt love an' tenure Behold molten children Recurrent Neural Networks Generating Even Word God - Please note post taken seriously show artificial system learned enough lot text generate new text 2 days ago What's new Cancellation multilinear Hilbert transform - ve uploaded arXiv paper Cancellation multilinear Hilbert transform submitted Collectanea Mathematica paper uses method 2 days ago G del's Lost Letter P NP John Alicia Nash 1928 1933 2015 - condolences Awesome Stories source John Nash wife Alicia killed taxi accident New Jersey Turnpike Saturday afternoon wer 2 days ago Timothy Lottes OS Project 7 - PS 2 Misc - started back long break Finished PS 2 driver Simplified keyboard interface one 64-bit bit array memory simple p 3 days ago Ergodic Walk AISTATS 2015 talks one day - attended AISTATS day change year unfortunately due teaching missed poster Shuang Song presented work 3 days ago Herve La cuisine cr erait-elle son objet - La chimie cr e son objet la phrase est paradoxale dit qu'elle est du chimiste Marcellin Berthelot mais est-elle vraiment de lui Voir Marcellin 3 days ago ChapterZero quick thought Supernatural tv shows - finished season 9 Supernatural ve got give show credit one demands deus ex machina ending Anything less 3 days ago Mr Vacuum Tube Phased Array Radar Looking Walls - Phased Array Radar Looking Walls http blog array2016 org p 24 5 days ago Walking Randomly MATLAB Vectorisation double-edged sword - Imagine new MATLAB programmer create N x N matrix called j j first attempt solution might 6 days ago slice pizza Morning Madness Ode Mercedes - morning madness drive along skyline Cutting fog thick stew waves dew 70mph hugging memorized curves without 1 week ago Decision Science News Gelman sense dubious Science article - Statistician Andrew Gelman sense something dubious Science article soon published post Gelman sense 1 week ago 0xDE Graham Erd Egyptian fractions - recent paper Ron Graham surveys work Paul Erd Egyptian fractions know Erd s' second paper subject didn't p 1 week ago Geomblog ITA conference really enjoy - Continuing thoughts STOC 2017 reboot went back Boaz's original question would make likely go STOC thought I'd 1 week ago Pillow Lab Blog Fast Kronecker trick Gaussian Process regression expressive kernels - May 11th presented following paper lab meeting Fast Kernel Learning Multidimensional Pattern Extrapolation Andrew Gordon Wilson Elad Gil 2 weeks ago Machine Learning etc ICLR 2015 - ICLR posters caught eye larger image simple implement idea gives impressive results force two groups units un 2 weeks ago Libres pens es d'un math maticien ordinaire adventure Google search - interesting experience Google recently related Electronic Journal Probability EJP Electronic Communications Probability ECP 2 weeks ago tombone's blog Dyson 360 Eye Baidu Deep Learning Embedded Vision Summit Santa Clara - Bringing Computer Vision Consumer Mike Aldred Electronics Lead Dyson Ltd vision research priority decades results 3 weeks ago Secrets Consulting Requirements Hints Variations - volumes Exploring Requirements follow chapter section hints variations topic chapter Many readers tel 3 weeks ago Harvest Imaging Blog Third HARVEST IMAGING FORUM December 2015 - successful forums 2013 2014 third one organized December 2015 Voorburg Hague Netherlands basic inte 3 weeks ago m bandit COLT 2015 accepted papers cool videos - Like last year compiled list COLT 2015 accepted papers together links arxiv version whenever could find one papers 3 weeks ago Information Structuralist Counting bits Vapnik Chervonenkis - Machine learning enabling computers improve performance given task get data express intuition quantitative 4 weeks ago Le Petit Chercheur Illustr Quasi-isometric embeddings vector sets quantized sub-Gaussian projections - Last January honored invited RWTH Aachen University Holger Rauhut Sjoerd Dirksen give talk general topic quantized co 4 weeks ago Machine Learning Theory Randomized experimentation - One good thing machine learning present people actually use back-ends many systems interact daily basis 5 weeks ago La vertu d'un LA virtue - fortunate hive D dom nologie la science du traitement de donn es signal images etc - O l'on propose le n ologisme d dom nologie pour d signer la technique la pratique la science du traitement de signal et de l'analyse d'images au c 1 month ago robots net Robots Podcast Farewell robots net join us Robohub - Since May 2007 colleagues Robots Podcast Robohub working robots net bring latest news views robo 1 month ago Machine Learning Deep Learning Works II Renormalization Group - Deep Learning amazing Deep Learning successful Deep Learning old-school Neural Networks modern hardware w 1 month ago Follow Data Genomics Today Tomorrow presentation - Slideshare link widget presentation gave Genomics Today Tomorrow event Uppsala couple weeks ago March 19 2015 sp 1 month ago Adventures Signal Processing Open Science Open Access Journals Missing - end could value proposition future journals 1 month ago Thoughts Mysterious Universe State Probabilistic Programming - two weeks last July cocooned hotel Portland living breathing probabilistic programming student probabilistic p 1 month ago Epistasis Blog Biomedical Informatics Faculty Positions University Pennsylvania - recently moved research lab Perelman School Medicine University Pennsylvania serve Director Institute Biomed 2 months ago Petros Boufounos Internship Opening Sensor Fusion - new internship opening MERL area sensor fusion posting follows MM880 Sensor fusion MERL looking well qualified 2 months ago G-media Le blog Nouveaut d veloppeurs domotique et openpicus Prise Gigogne et Dolphin View - Dans cet article nous allons voir comment utiliser la prise gigogne avec mesure de comptage depuis le logiciel Dolphin View Mat riel n cessaire une c 2 months ago Neurevolution Neurevolution relaunch - hard believe started blog eight years ago way back grad students long way ve come Patryk Dir 2 months ago Inductio Ex Machina Upcoming Conference Sydney Predictive APIs Apps - big kick-off Barcelona last year 200 attendees second International Conference Predictive APIs Apps held Sydn 2 months ago Ars Mathematica Nine Chapters Semigroup Art - Googling something came across Nine Chapters Semigroup Art leisurely introduction theory semigroups 2 months ago yellow noise takis champs magnetiques palais de tokyo february 2015 - takis champs magnetiques palais de tokyo february 2015 3 months ago polylogblog CPM 2015 - Ely asked remind everyone deadline 26th Annual Symposium Combinatorial Pattern Matching fast approaching 2nd F 4 months ago Mirror Image Kinect depth sensor works stereo triangulation simple ways speed convnet little - quite complex methods making convolutional networks converge faster Natural gradient dual coordinate ascent second order hessian fre 5 months ago natural language processing blog myth strong baseline - probably count fingers number papers I've submitted reviewer hasn't complained baseline way don't mean 6 months ago Building Intelligent Probabilistic Systems Harvard Center Research Computation Society Call Fellows Visiting Scholars - Harvard Center Research Computation Society CRCS solicits applications Postdoctoral Fellows Visiting Scholars Programs 7 months ago Pixel shaker Pics Manipulated Photos Notable Historic Figures Digital Era Images - Manipulating photos happened way Photoshop around series shows afters famous notable figures digital era 7 months ago Victoria Stodden input OSTP RFI reproducibility - Sept 23 2014 US Office Science Technology Policy Whitehouse accepting comments Strategy American Innovation 8 months ago Ga l Varoquaux Hiring engineer mine large brain connectivity databases - Work us leverage leading-edge machine learning neuroimaging Parietal research team work improving way brain images analyz 8 months ago Computers don't see Compiling OpenCV 3 0 alpha CUDA support MacOS X - quick tip people troubles compiling OpenCV 3 0 alpha MacOS X variable called CUDA_TOOLKIT_DIR cmake configura 8 months ago Herr Strathmann - home Shogun NYC - late August invited NYC present Shogun open-source Machine Learning software workshop link organised John Langford Seeing Sh 8 months ago Lousodrome Vie au Japon Le certificat de r sidence j minhy - Le certificat de r sidence j minhy en japonais est un simple document d une page qui certifie votre adresse de r sidence et qui est demand pour 10 months ago trekkinglemon's fresh squeeze Data processing flashy yellow Peyresq 2014 Last day - 2014 edition Peyresq summer school finished coda let us summarize last talk Continue reading 10 months ago Camdp com updates DataOrigami Launch - I'm proud announce latest project dataorigami net still go check 11 months ago Neighborhood Infinity Cofree meets Free - - LANGUAGE RankNTypes MultiParamTypeClasses TypeOperators - Introduction spoke BayHac 2014 free monads asked co 1 year ago Statistical Trader Follow twitter StatTrader - Since working full time managing Data Sciences team Bloomberg Global Data haven't done sort long-form blogging used 1 year ago Ivan Oseledets homepage Introduction cross approximation - written short incomplete introductory page skeleton decomposition using maxvol algorithm update references 1 year ago MAKE Magazine New Project TV-B-Gone Kit - Tired LCD TVs everywhere Want break advertisements re trying eat Want zap screens across street TV-B-Go 1 year ago i2pi com focus - focus 1 year ago brain map statistics connection Linus Pauling fMRI - think Linus Pauling two things come mind work nature chemical bond awarded 1954 Nobel Prize Chem 1 year ago Computational Information Geometry Wonderland New blog address Moving Wordpress - use anymore blog system rather use Wordpress supports many goodies like latex Please update yo 1 year ago Martin Tall Gaze Interaction Introducing Eye Tribe Tracker - It's great pride today introduce Eye Tribe Tracker It's worlds smallest remote tracker first use USB3 0 one 100 1 year ago De Rerum Natura Functional programming - least interesting concept programming languages purity Java prime example Everything object much prefer Python wa 1 year ago Doyung Pig Hive Pig Hive - Hadoop ecosystem Data processing Pig Hive Pig Hive pig hive data 2 years ago BlackbordRMT Dyson Brownian motion thirty stationnary Brownian motions Ornstein Uhlenbeck - image Image Hosted ImageShack us Dyson Brownian motion thirty stationnary Brownian motions Ornstein Uhlenbeck processes constr 2 years ago Freakonometrics Mod lisation et pr vision cas d' cole - Quelques lignes de code que l'on reprendra au prochain cours avec une transformation en log et une tendance lin aire Consid rons la recherche du mot c 2 years ago Brain Windows WordPress accepts Bitcoin - WordPress blog platform accepts bitcoin hosting costs via BitPay WordPress hosts Brain Windows many world biggest best blogs 2 years ago Science Sands Science Sands moved - Dear readers migrated blog along things new site http davidketcheson info New posts longer appear blo 2 years ago Journey Randomness SMC sampler - monte carlo sequential monte carlo SMC sampler Jasra Doucet Del Moral 2 years ago Hao's TechBlog Sampling Rate Pixels compare sampling rate camera single-pixel camera - beginning I'd like make clear two terms Nyquist frequency Nyquist rate may take thing even text 2 years ago bpchesney org Nested Linear Programs Find Agreement among Sensors - Suppose 3 sensors B C set readings goes column matrix b matrix constructed bA bB bC repr 2 years ago Stupid Matlab Hacks busy recently hacks mean time go play - busy recently hacks mean time go play http www mathworks com matlabcentral fileexchange 37104-kappatau mak 2 years ago Becoming Astronaut Orbiters going - month NASA commenced delivery four Space Shuttle orbiters final destinations extensive decommissioning process 3 years ago FUTUREPICTURE Note comments - six months ago hit serious rash spam 20 000 comments posted span two weeks Unfortunately ti 3 years ago CyberGi Os ciborgues da Campus Party - Essa semana fui na quinta edi o da Campus Party e ontem dia 9 tive o prazer de conhecer dois ciborgues o Rob Spencer que fez o document rio Eyeborg q 3 years ago Collective Research Interaction Sound Signal Processing Sonification Handbook - yet heard Sonification Handbook edited Thomas Hermann Andy Hunt John G Neuhoff published even better freely avai 3 years ago Espace Vide PCA Compressive Measurements Video - I've little bit fun visualizations today outcomes pretty nice potentially artistic thought I'd share 3 years ago inspiration etc Session 4 5th Graders - Doodling 2 - Session 4 - Doodling Approximately one half hours Starting one element Adding proximity expanding drawing Tool 3 years ago Marcio Marim Welcome new website - time work second version didn publish happy release new website share creations interests whi 3 years ago Cognitive Radio Blog G Vazquez-Vilar PhD Thesis Interference Management Cognitive Radio - image Thesis Cognitive radio dissertation examination took place couple weeks ago Happily passed say one unexpect 3 years ago OISblog Liquid Crystal Eyeglasses - Pixel Optics introduced Empower eyeglasses use liquid crystal lenses actively adjust power 4 years ago Big Numbers Going commission - m going focus completely school blog going hiatus months ll start ve passed hurd 4 years ago Another Dimension three musketeers - Given vectors three quantities interesting indeed fact concept inner product vector space revolves largely around thos 4 years ago Electrons holes Indefinite hiatus - blog put indefinite hiatus 4 years ago Arthur Charpentier Blog transfert - mentioned past weeks blog transfered please update links bookmarks redirected shortly http freako 4 years ago Chaotic Pearls Indonesian Contoh Program Phase Unwrapping - Ide dari phase-unwraping PU progresif ini muncul di suatu sore hari ketika saya sedang berjalan-jalan di sekitar kampus sekitar tahun 2002-an Saya ber 4 years ago YALL1 ALgorithms L1 Announcements - June 4 2010 Toeplitz circulant sampling demos released June 4 2010 YALL1 version 1 0 released open-source Download link YALL1 n 4 years ago Hashimoto Laboratory's Blog Personal Mobility Next Level - Improving concept self-balancing unicycle Honda introduced brand new U3-X new personal mobility platform regular large whee 5 years ago Lianlin Li's Compressive Sensing blog Chinese Igor's Blog today - Today Post updated Igor's blog following CS Matrix Completion via Thresholding Dick Gordon's Op-Ed Lianlin L 5 years ago Guan Gui's blog expoit channel structure - 6 years ago Three-Toed Sloth - ML Counterexamples Pt 2 - Regression Post-PCA camdp com blogs - Willow Garage Blog Willow Garage - Latest News - KinectHacks net - Little Knowledge - Blog Robotics - Show 25 Show Another Blog List Haldane's Sieve SWEEPFINDER2 Increased sensitivity robustness flexibility - SWEEPFINDER2 Increased sensitivity robustness flexibility Michael DeGiorgio Christian D Huber Melissa J Hubisz Ines Hellmann Rasmus Nielsen Su 4 hours ago Information Processing John Nash dead 86 - original title post won Nobel Memorial Prize see sad news bottom Beautiful Mind Nash went see von Neuman 4 days ago Scientific Clearing House Selection week - Violinist Hilary Hahn hails Baltimore pianist Valentina Lisitsa play first movement American composer Charles Ives Fourth Sonata 6 days ago leon bottou org news news graph_transducer_networks_explained - Graph Transducer Networks explained scavenging old emails couple weeks ago found copy early technical report describes 2 weeks ago tombone's blog Dyson 360 Eye Baidu Deep Learning Embedded Vision Summit Santa Clara - Bringing Computer Vision Consumer Mike Aldred Electronics Lead Dyson Ltd vision research priority decades results 3 weeks ago Property Testing Review News April 2015 - April busy time property testing 9 papers posted online month let jump right Testing properties graphs Approximately Co 3 weeks ago Relax Conquer Courant Institute Mathematical Sciences - Finally longer job market excited announce join Courant Institute Mathematical Sciences Assistant Profess 1 month ago Moody Rd Competing data science contest without reading data - Machine learning competitions become extremely popular format solving prediction classification problems sorts famous ex 2 months ago AK Tech Blog Neustar SIAM SODA 2015 - Author Note Hello readers m Sonya Berg first post Neustar research blog data scientist Neustar Research foc 4 months ago Mostly linguistically computational Adventure collaborative filtering information retrieval matrix factorization stuff Count-Min-Log Strange effect - followed previous steps mentionning previous post odd behaviour Count-Min-Log MAX sampling w 5 months ago Deep Learning Recent Reddit AMA Deep Learning - Recently Geoffrey Hinton Yann Lecun Yoshua Bengio reddit AMA subscribers r MachineLearning asked questions AMA contains 6 months ago jim learning choose mentor - http www cell com neuron fulltext S0896-6273 13 00907-0 https www cs princeton edu courses archive spring15 cos598D http www cs princeton edu cours 1 year ago Blog Interphase Transport Phenomena Laboratory Texas M University Fun Boiling - 1 year ago Math Good another WordPress site Linear Algebra good - Linear algebra branch mathematics concerned solving linear systems natural think linear algebra solving unkno 1 year ago BayesRules STAT 330 November 29 2012 - finished discussion model selection averaging went missing data models cases fact data observed h 2 years ago Comments Lecture Schedule Embryo Physics Course - Blog List FlowingData Compare curve reality income versus college attendance - image Draw line grow poorer families less likely go college grow richer families likely 1 hour ago SynBioFromLeukipposInstitute Scoop Three developments help synthetic biology live promise Genetic Literacy Project - Three developments help synthetic biology live promise Dominic Basulto May 28 2015 Washington Post See Scoop via 2 hours ago Retraction Watch chocolate-diet sting study retracted coverage doesn surprise news watchdog - Yesterday John Bohannon described i09 com successfully created health news conducted flawed trial health benefits chocolate 4 hours ago Statistical Modeling Causal Inference Social Science Cracked com Huffington Post Wall Street Journal New York Times - David Christopher Bell goes trouble link Palko explain Every Map Popular _________ State Bullshit long 4 hours ago olimex MOD-LCD3310 OSHW monochrome LCD 84 48 pixels board UEXT connector - MOD-LCD3310 Open Source Hardware board released Apache 2 0 Licensee low cost 84 48 pixels LCD connect development bo 6 hours ago Science-Based Medicine Florida strikes Brian Clement - Brian Clement charlatan Unfortunately doesn seem problem State Florida made two turned three attempts g 12 hours ago Sage Open Source Mathematics Software Guiding principles SageMath Inc - February year 2015 founded Delaware C Corporation called SageMath Inc first stab guiding principles compan 21 hours ago Quomodocumque evil impulse good - learned teaching Rabbi Rebecca Ben-Gideon last week turning mind Rabbi Nahman said Rabbi Samuel name Behold 1 day ago Machine Vision 4 Users re backlighting cylindrical parts - see backlighting used time machine vision training classes trade shows typically gauging locating shapes Look closely though 1 day ago Skulls Stars comics moment inspires Suicide Squad - Update Forgot say thank Dad mailing complete Suicide Squad collection made whole post possible suspect peop 1 day ago InnoCentive Challenges Bioabsorbable Elastomeric Film - Seeker looking bio-absorbable elastomeric film specific properties could existing product one adapted 1 day ago What's new Cancellation multilinear Hilbert transform - ve uploaded arXiv paper Cancellation multilinear Hilbert transform submitted Collectanea Mathematica paper uses method 2 days ago Ben Krasnow physics floating screwdrivers - explain jet air float common screwdriver Plans make fluid turbulence disc http makezine com projects rheoscopic-coffee-tab 2 days ago Short Fat Matrices Three Paper Announcements - ve pretty busy lately writing researching visitors announcements serve quick summary ve 1 Tables 1 week ago 2Physics Current Fluctuations - left right Pierre Pfe er Fabian Hartmann Sven H ing Martin Kamp Lukas Worschech Authors Pierre Pfe er1 Fabian Hartmann1 Sven H ing1 2 1 week ago Large Scale Machine Learning Animals Open Data Science Conference - May 30 Boston - 1 week ago Zhilin's Scientific Journey Yann LeCun's Comments Extreme Learning Machine ELM - Yann LeCun https www facebook com yann lecun posts 10152872571572143 Facebook commented ELM quoted What's great Ext 2 weeks ago Gowers's Weblog Nick Clegg Liberal Democrat - life found Liberal Democrat policies Liberal-SDP Alliance policies Liberal policies n 4 weeks ago JeremyBlum com Shapeoko2 CNC Mill Build Log Review - new Inventables Shapeoko2 CNC mill carving away Watch timelapse build-log thorough review do-it-yourself CNC milling machine Continue 4 weeks ago F Pedregosa IPython Jupyter notebook gallery - Draft - TL DR created gallery IPython Jupyter notebooks Check - image Notebook gallery couple months ago put online website d 5 weeks ago Richard Baraniuk Probabilistic Theory Deep Learning - Patel Nguyen R G Baraniuk Probabilistic Theory Deep Learning arXiv preprint arxiv org abs 1504 00641 2 April 2014 grand challeng 1 month ago Welcome Sparse Land Discreteness Sparsity - Discrete signals may sparse whereas sparse signals may discrete Let us consider following signal x 1 1 -1 1 -1 -1 1 1 2 months ago Various Consequences Reliability Growth Enhancing Defense System Reliability - report pdf National academies reliability growth interesting There's lot good stuff design reliability physics fai 2 months ago Inductio Ex Machina Upcoming Conference Sydney Predictive APIs Apps - big kick-off Barcelona last year 200 attendees second International Conference Predictive APIs Apps held Sydn 2 months ago Highly Scalable Blog Data Mining Problems Retail - Retail one important business domains data science data mining applications prolific data numerous optimization p 2 months ago regularize Farewell Ernie Esser - Today learned Ernie Esser passed away March 8th 2015 age 34 Ernie PostDoc UBC Vancouver PhD UCLA worked applied math 2 months ago Thingiverse Blog MakerBot PrintShop 1 4 3D Print iPad 3G 4G - Last week introduced exciting new feature MakerBot 3D Ecosystem ability start 3D print remotely using smartphone MakerBot P 3 months ago much little time Post-doc Molecular Informatics Opening NCATS - post-doc opening Informatics group NCATS work computational aspects high throughput combination screening topics includ 3 months ago next big thing syndrome Books read 2014 - disclaimer book cover images post Amazon Affiliate links click buy book receive cents form 5 months ago Pursuits Null Space Blog done moved - first hesitant handling latex Rolf Mathcination pointed excellent tool latex-to-wordpress wil 5 months ago Proof Pudding M571 Fall 2014 Lecture 5 - 1 Agenda QR Factorization Gram-Schmidt classical modified Householder QR reflectors ended discussion projectors several impo 6 months ago Tianyi Zhou's Research Blog Learn Low-rank Sparse Structures via Randomized Alternating Projections List Submodular Optimization Streaming Data Update - Coresets k-Segmentation Streaming Data NIPS 2014 Streaming Submodular Optimization Massive Data Summarization Fly KDD 2014 8 months ago Wondering Star Lunar Detection Ultra-High-Energy Cosmic Rays Neutrinos - Spotted ArXiv Physics blog using SKA array Moon collector would certainly qualify sensors size pl 8 months ago Dan's Blog make paper spherical panorama - image image Photos usually show rectangular fragment scene image taken Typical panoramic images display landscap 8 months ago Seth's blog Videos Seth Roberts Memorial Talks - Video recordings Ancestral Health Society public talks August 10 2014 honoring Seth life work posted http bit ly 1v33kbM Many 9 months ago Thoughts Artificial Intelligence - blog moved new post metaphor mathematics new blog 9 months ago Lupi Software thoughts Monitorama 2014 PDX - Monitorama fantastic conference came mixed feelings Great work done open source software however based con 1 year ago Andrej Karpathy Blog Interview Data Science Weekly Neural Nets ConvNetJS - Quick post thought mention ve given interview two months ago ConvNetJS background perspectives neural 1 year ago Ivan Oseledets homepage Introduction cross approximation - written short incomplete introductory page skeleton decomposition using maxvol algorithm update references 1 year ago Artificial Intelligence Blog John Dobson 1915 2014 - September 2008 met John Harrisburg slept overnight friend Zoungy place enjoyed beautiful drive Cherry Springs th 1 year ago Hey What's BIG idea 3D Printing Tangible Idea - heywhatsthebigidea net B J Rao write another article 3D printing internet already offers abundance information subject Moun 1 year ago Normal Deviate END - addition best comedy TV show ever Seinfeld great source wisdom one episode Jerry counsels George hit high 1 year ago Math Good another WordPress site Linear Algebra good - Linear algebra branch mathematics concerned solving linear systems natural think linear algebra solving unkno 1 year ago tcs math - mathematics theoretical computer science Kadison-Singer Quantum Mechanics - accounts Kadison-Singer problem mention arose considerations foundations quantum mechanics Specifically question abo 1 year ago programming machine learning blog BibTeX-powered publications list Pelican pelican-bibtex - Hook Wouldn like manage academic publications list easily within context static website Without resorting external services 2 years ago Mathblogging org -- Blog Mathematical Instruments Gianluigi Filippelli - post part series Mathematical Instruments introduce math bloggers listed site Today Gianluigi Filipp 2 years ago openPicus blog Better BBQ Flyport Cosm - knew Austria famous BBQ remember summer 2006 great barbeque austrian friends Klagenfurt fantastic 8 2 years ago Dirac Sea Survey Non Parametric Bayesian marginal applications - Survey Non Parametric Bayesian marginal applications mystery Zoubin one favorite researchers papers usually 2 years ago Biophotonics Review Biology Games Biomedical Research - Games become important part cultures bringing lots entertainment creativity societies Beyond traditional understanding o 2 years ago Clouds Fukushima 9 Months later - regards airborne radiation ground measurement reading NaI detector KEK Tsukuba showing steady decline past 3 years ago Gigapixel News Journal ipConfigure Presents First Gigapixel Wide Area Surveillance Platform - ipConfigure privately owned software research development company announces world first multi-Gigapixel surveillance platform designed 3 years ago Gustavo Tinkers Source Code GUI - Hosted https github com goretkin soundcard-radar 3 years ago Passive Vision Passive Vision - Welcome plan use blog ideas research teaching moment m looking forward teaching Computer Vision class 60 5 years ago Dick Gordon's blog - n 45 years ago Show 25 Show Search Pages Link Loading Previous Entries 2015 207 May 2015 37 Compressive Sensing work Three-dimensional coo Robust Rotation Synchronization via Low-rank Low-Rank Matrix Recovery Row-and-Column Affin Self-Dictionary Sparse Regression Hyperspectra Randomized Robust Subspace Recovery High Dimen Book Dictionary Learning Visual Computing Compressed Nonnegative Matrix Factorization Fas Saturday Morning Videos Slides Videos IC PCANet Simple Deep Learning Baseline Image Four million page views million million Great Convergence FlowNet Learning Optical F CSjob Post-Doc Structured Low-Rank Approximati Low-rank Modeling Applications Image Solving Random Quadratic Systems Equations N Identifiability Blind Deconvolution Subspa Tensor time Adaptive Higher-order Spectral Estima Blue Skies Foundational principles large scal Tensor sparsification via bound spectral Saturday Morning Videos Reruns Hamming's time Important Things Commodi Newton Sketch Linear-time Optimization Algorith Self-Expressive Decompositions Matrix Approxim Tonight Paris Machine Learning Meetup 9 Season Factorization Machines implementation Theano Frequent Directions Simple Deterministic Mat fastFM Library Factorization Machines - imp Structured Block Basis Factorization Scalable Tensorial Kernel kernel-based framework tens Kernel Spectral Clustering applications - impl Sparse LSSVM Reductions Large-Scale Data Random Bits Regression Strong General Predictor Phase retrieval via Kaczmarz methods - implementat Great Convergence Deep Learning Compressive Phase Transition Joint-Sparse Recovery Mul Saturday Morning Videos IMA Workshop Convexity Saturday Morning Videos IMA AP Workshop Inform Nuit Blanche Review April 2015 Apr 2015 45 Mar 2015 44 Feb 2015 31 Jan 2015 50 2014 536 Dec 2014 52 Nov 2014 43 Oct 2014 38 Sep 2014 41 Aug 2014 48 Jul 2014 52 Jun 2014 43 May 2014 56 Apr 2014 47 Mar 2014 44 Feb 2014 35 Jan 2014 37 2013 454 Dec 2013 43 Nov 2013 38 Oct 2013 38 Sep 2013 33 Aug 2013 36 Jul 2013 43 Jun 2013 29 May 2013 38 Apr 2013 40 Mar 2013 29 Feb 2013 47 Jan 2013 40 2012 488 Dec 2012 44 Nov 2012 39 Oct 2012 46 Sep 2012 28 Aug 2012 52 Jul 2012 20 Jun 2012 38 May 2012 60 Apr 2012 41 Mar 2012 50 Feb 2012 29 Jan 2012 41 2011 465 Dec 2011 47 Nov 2011 49 Oct 2011 47 Sep 2011 36 Aug 2011 24 Jul 2011 25 Jun 2011 47 May 2011 50 Apr 2011 56 Mar 2011 39 Feb 2011 17 Jan 2011 28 2010 358 Dec 2010 47 Nov 2010 35 Oct 2010 32 Sep 2010 28 Aug 2010 30 Jul 2010 33 Jun 2010 26 May 2010 27 Apr 2010 28 Mar 2010 28 Feb 2010 19 Jan 2010 25 2009 274 Dec 2009 22 Nov 2009 23 Oct 2009 24 Sep 2009 25 Aug 2009 25 Jul 2009 23 Jun 2009 20 May 2009 16 Apr 2009 25 Mar 2009 27 Feb 2009 21 Jan 2009 23 2008 302 Dec 2008 20 Nov 2008 23 Oct 2008 28 Sep 2008 28 Aug 2008 22 Jul 2008 17 Jun 2008 28 May 2008 22 Apr 2008 31 Mar 2008 32 Feb 2008 25 Jan 2008 26 2007 179 Dec 2007 23 Nov 2007 21 Oct 2007 14 Sep 2007 18 Aug 2007 13 Jul 2007 13 Jun 2007 9 May 2007 11 Apr 2007 9 Mar 2007 22 Feb 2007 19 Jan 2007 7 2006 30 Dec 2006 4 Nov 2006 4 Oct 2006 2 Sep 2006 2 Aug 2006 2 Jul 2006 2 Jun 2006 1 Mar 2006 2 Feb 2006 2 Jan 2006 9 2005 88 Dec 2005 1 Nov 2005 6 Oct 2005 3 Sep 2005 12 Aug 2005 1 Jul 2005 7 Jun 2005 4 May 2005 12 Apr 2005 7 Mar 2005 12 Feb 2005 8 Jan 2005 15 2004 214 Dec 2004 18 Nov 2004 8 Oct 2004 20 Sep 2004 44 Aug 2004 29 Jul 2004 13 Jun 2004 13 May 2004 18 Apr 2004 10 Mar 2004 22 Feb 2004 8 Jan 2004 11 2003 12 Dec 2003 10 Nov 2003 2 Books Wish List Start-ups like InView Technology Corporation See Inside World First Compressive Sensing Camera - Read InView white paper InView210 scientific SWIR camera InView210-CSCameraWhitePaper-Feb2015 See inside world first high 2 months ago Centice Centice Drug Analysis Systems Sold Major Federal Agency Aid Prescription Pill Abuse Operations - Nationwide program allows agents identify 3 800 prescription pills illicit drugs RESEARCH TRIANGLE PARK N C October 28 2014 Centice Co 7 months ago Press GraphLab twitterscroll - post twitterscroll appeared first GraphLab Inc 1 year ago Zoomin' - Aqueti TV Interview Gigapixel Images - Scott McCain interviewed July 10 2013 discuss gigapixel imaging WLOS ABC's Asheville affiliate 1 year ago Metamarkets Blog - wise io Machine Learning Service Big Data Analytics - Translate blog Focused Interest Compressed Sensing Compressive Sampling Compressive Sensing Mapping blog entries Compressed Sensing Cognition - Machine Learning Space Search Rescue Compressive Sensing Technology Watch Compressive Sensing Big Picture Compressive Sensing Hardware Compressed Sensing Videos Compressive Sensing Calendar Compressive Sensing Jobs Local Compressed Sensing Codes CS LinkedIn Group Recent links Blog CS Compressive Sensing 2 0 Community Compressive Sensing 2 0 blogs webpages Saturday Morning Cartoons Sherpa Romeo Publisher copyright policies self-archiving Categories Subjects Interest CS 2163 compressive sensing 1627 compressed sensing 1615 compressive sampling 1589 MF 521 implementation 359 Applied Math 209 ML 209 MatrixFactorization 194 space 152 AMP 113 calibration 105 CSHardware 103 CSjobs 90 CS Community 72 SaturdayMorningVideos 71 CSCommunity 66 BlindDeconvolution 64 QuantCS 62 phaseretrieval 62 RandNLA 60 hyperspectral 60 nonlinearCS 59 nuclear 59 technology 58 SundayMorningInsight 57 CSVideo 50 python 50 tensor 49 cognition 47 Meetups 46 Algorithm 45 grouptesting 45 meetup 43 1bit 42 publishing 41 synbio 41 graphlab 38 RandomFeatures 37 darpa 37 CSmeeting 36 AI 33 NuitBlancheReview 33 search rescue 33 weather modeling 33 remote sensing 32 Csstats 31 wow 30 business 29 bayes 28 data fusion 28 machine learning 28 MLParis 27 jim gray 24 neuroscience 23 autonomous 22 dimensionality reduction 22 mapmaker 20 thesis 20 AlexSmola 19 Kaczmarz 19 ParisMachineLearning 19 geocam 19 space debris 19 space situational awareness 19 medical 18 phaserecovery 18 ChristophStuder 17 ImagingWithNature 17 SAHD 17 maps 17 mishap 17 sleep 17 CSCalendar 16 monday morning algorithm 16 transport 16 CAI 15 energy 15 nanopore 15 phasediagrams 15 hasp 13 superresolution 13 ADMM 12 PatrickGill 12 Technologies Exist 12 causality 12 darpa urban challenge 12 sudoku 12 CSDiscussion 11 ICLR2015 11 TRL 11 qa 11 thermal engineering 11 GPU 10 fft 10 sie 10 videos 10 Computational Neuroscience 9 MultiplicativeNoise 9 RandomForest 9 StarTracker 9 aroundtheblogs 9 france 9 ELM 8 GenomeTV 8 HammingsTime 8 PredictingTheFuture 8 Good 8 collaborative task manager 8 exploration 8 random projections 8 situational awareness 8 sparsity 8 wavelet 8 GreatThoughtsFriday 7 TheGreatConvergence 7 collaborative work 7 innovation 7 mems 7 random lens imaging 7 CSCartoons 6 CitingNuitBlanche 6 CompressibleWGN 6 accidentalcamera 6 complexity vizualisation 6 maxent 6 randomization 6 startups 6 streaming 6 thedip 6 RMM 5 UQ 5 book 5 coded aperture 5 muscle 5 tex-mems 5 BP 4 British Petroleum 4 CfP 4 CompressiveSensingWhatIsItGoodFor 4 DataDrivenSensorDesign 4 HusHambug 4 Comment 4 ReproducibleResearch 4 google maps 4 hypergeocam 4 internet traffic 4 jionc 4 microsystems 4 scaling 4 technologie 4 Deepwater Horizon 3 disruptive technology 3 financement de la recherche 3 google 3 julia 3 radiation detection 3 recherche 3 sketching 3 Columbia 2 DC law 2 LowRank 2 MLZurich 2 MMDS 2 ManifoldSignalProcessing 2 NO-C-WE 2 TheNuitBlancheChronicles 2 UAV 2 aggregators 2 anecdote 2 challenge 2 diet 2 genomics 2 kinect hacks 2 microcontroller 2 notebynotecooking 2 sensor network 2 AWGN 1 BaltiAndBioinformatics 1 Blogger 1 CS MF 1 CT 1 CompanyX 1 JOTRSOI 1 Leonardo 1 QIS 1 RMT 1 SKA 1 SaturdayMorningCartoons 1 SensorsTheSizeOfAPlanet 1 YouAreNotPayingAttention 1 advice 1 aha 1 biographies 1 control 1 crowdfunding 1 csoped 1 dataset 1 donoho-tao 1 extremesampling 1 herschel 1 hushamburg 1 iLab 1 inverse problems 1 iot 1 jacques devooght 1 lfe 1 lua 1 memory 1 mindmaps 1 nanopre 1 octopus 1 oped 1 privacy 1 reference 1 request 1 rr 1 seinfeld 1 solver 1 theano 1 wonderingstar 1 youkeepusingthatword 1 sites interest Blogroll Natural Language Blog Hal Daume III Polylog Blog Andrew McGregor Eric Tramel's Espace Vide Blog Compressed Sensing Lianlin Li's Compressive Sensing blog Chinese Space Engineering Research Center Space Engineering Blog Frank Nielsen's Information Geometry blog David Brady's Blog Le Petit Chercheur Illustre Chaotic Pearls Indonesian De Rerum Natura Michele Guieu's blog Ergodic Walk Laurent Duval's site Laurent Duval's blog Thesilog Diffusion des savoirs - Ecole Normale Superieure What's New Terry Tao Statistical Modeling Causal Inference Social Science Andrew Gelman Aleks Jakulin Masanao Yajima Machine Learning etc Yaroslav Bulatov slice Pizza Muthu Mutukrishnan Geomblog Piotr Indyk Suresh Machine Learning Theory John Langford Lemonodor John Wiseman Yet another Machine Learning blog Pierre Dangauthier Make Magazine blog Theses en ligne Neurevolution blog Pedro Davalos website Damaris' blog Olivier's blog Julie's blog Michele Guieu's site Location visitors Nuit Blanche Dilbert counters Powered Blogger"),
("Andrew Ng: Why 'Deep Learning' Is a Mandate for Humans, Not Just Machines | WIRED", "Andrew Ng Deep Learning Mandate Humans Machines Skip Start Article Visually Open Nav Go Wired Home Page SUBSCRIBE Open Search Field Search Business Design Entertainment Gear Science Security Photo Video Magazine Business Conference 2015 WIRED INSIDER INNOVATION INSIGHTS MakeTechHuman Fallback Image Get OurNewsletter WIRED's biggest stories delivered inbox Submit Thank Invalid Email Follow UsOn Twitter 5 mins See 11 BMWs famous artists turned masterpieces wrd cm 1FiV8bw Follow We're OnInstagram Follow Follow UsOn Facebook Don't miss latest news features videos Follow We're OnPinterest See what's inspiring us Follow Follow UsOn Youtube Don't miss WIRED's latest videos Follow Advertisement Slide 1 1 Caption maketechhuman Skip Article Header Skip Start Article Sponsor Content Author Caleb Garling Caleb Garling Andrew Ng Deep Learning Mandate Humans Machines maketechhuman venture capital research funding indication artificial intelligence play leading role shaping future tech innovators private public sector prominent defining role Andrew Ng chief scientist China search giant Baidu Ng taught AI Stanford led Google Brain project founded online education pioneer Coursera last year took post China Google hopes figuring teach computers see hear world populous country Small wonder China represents huge opportunity machine intelligence applications Baidu world fifth trafficked website Shopping site Taobao messaging app QQ media company Sina microblogging platform Weibo Chinese properties hold spots within top 15 Baidu designs application according Ng mobile comes first cell phones primary channel access Chinese consumers Ng soft-spoken undercurrent passion discussing research Today manages growing team Baidu U campus Sunnyvale Calif believe hype robot revolution says believe researchers scratching surface machine potential Killer robots concern prefers fret microprocessor run time pushing voice recognition place humans actually trust lot work Ng believes enough good ideas smart companies someday soon ll able speak rather tap want something smartphones recent chat Skype edited brevity clarity Ng outlined thinks within reach isn machine intelligence excites potential AI deep learning number organizations us others amazing computer vision technology things seemed impossible even year ago think struggle figuring compelling products know us found killer app yet Silicon Valley lot startups using computer vision agriculture shopping lot clothes shopping Baidu example find picture movie star actually use facial recognition identify movie star tell things like age hobbies wearing clothing recognize find related clothing buy show pretty popular Could advertisers eventually bid placement relation image re right re finding related clothing number verticals like recognizing interesting people recognizing holiday destination showing pictures destination probably potential computer vision even bigger things think ve figured valid reason worried destructive artificial intelligence think hundreds years people invent technology haven heard yet maybe computer could turn evil future uncertain know going happen five years reason say worry AI turning evil reason worry overpopulation Mars Hundreds years hope ve colonized Mars ve never set foot planet productively worry problem like working AI every day think AI akin building rocket ship need huge engine lot fuel large engine tiny amount fuel won make orbit tiny engine ton fuel even lift build rocket need huge engine lot fuel analogy deep learning one key processes creating artificial intelligence rocket engine deep learning models fuel huge amounts data feed algorithms spent time Google view self-driving cars sat close team m friends lot sense re contributing directly think self-driving cars little people think debate one two universes re first universe incremental path self-driving cars meaning cruise control adaptive cruise control self-driving cars highways keep adding stuff 20 years self-driving car universe two one organization maybe Carnegie Mellon Google invents self-driving car bam self-driving cars wasn available Tuesday sale Wednesday m universe one think lot confusion easy self-driving cars big difference able drive thousand miles versus able drive anywhere turns machine-learning technology good pushing performance 90 99 percent accuracy challenging get four nines 99 99 percent ll give re firmly way safer drunk driver founded Coursera championed value online education programs think future education education system succeeded far teaching generations different routine tasks tractors displaced farming labor taught next generation work factories ve never really good teaching huge number people non-routine creative work buy argument future labor less peril automation lower cost goods need work 10-20 hours week would said zero hours see minimum living wage long-term solution m sure favorite think society benefits human race empowered aspiring great things Giving people skill sets great things take work Go Back Top Skip Start Article maketechhuman Skip Social Skip Latest News 6K Share story Facebook Share story Twitter Share story Pinterest Share story via Email Comment story Skip Latest News Skip Comments Fallback Image Latest News Code Google Officially Announces Android M Latest OS 5 mins gear Google Clever Inbox Email App Open Everyone 46 mins Entertainment Roundup Audio Visuals Fred Armisen Golden Girl Jenny Lewis 2 hours Apps Uber Unveils App Updates Help Deaf Drivers 3 hours Design Inside L Oreal Plan 3-D Print Human Skin 4 hours News Skip Comments Skip Footer View comments Uncategorized Headline Google's Clever Inbox Email App Open Everyone Author gear gear Google's Clever Inbox Email App Open Everyone 46 mins Headline Audio Visuals Fred Armisen Golden Girl Jenny Lewis Author Entertainment Roundup Entertainment Roundup Audio Visuals Fred Armisen Golden Girl Jenny Lewis 2 hours Headline Uber Unveils App Updates Help Deaf Drivers Author Apps Apps Uber Unveils App Updates Help Deaf Drivers 3 hours Headline Inside L Oreal Plan 3-D Print Human Skin Author Design Design Inside L Oreal Plan 3-D Print Human Skin 4 hours Headline Watch Google O Keynote Live Right Author Silicon Valley Silicon Valley Watch Google O Keynote Live Right 4 hours Recommend Powered Outbrain Subscribe WIRED Get OurNewsletter WIRED's biggest stories delivered inbox Submit Thank Invalid Email Follow UsOn Twitter 5 mins See 11 BMWs famous artists turned masterpieces wrd cm 1FiV8bw Follow We're OnInstagram Follow Follow UsOn Facebook Don't miss latest news features videos Follow We're OnPinterest See what's inspiring us Follow Follow UsOn Youtube Don't miss WIRED's latest videos Follow Follow UsOn Twitter 5 mins See 11 BMWs famous artists turned masterpieces wrd cm 1FiV8bw Follow We're OnInstagram Follow Follow UsOn Facebook Don't miss latest news features videos Follow We're OnPinterest See what's inspiring us Follow Follow UsOn Youtube Don't miss WIRED's latest videos Follow Go Previous Section Article Go Next Section Article Wired Facebook Wired Twitter Wired Pinterest Wired Youtube Wired Tumblr Wired Instagram Subscribe Advertise Site Map Press Center FAQ Customer Care Contact Us Newsletter Wired Staff Jobs RSS Use site constitutes acceptance user agreement effective 3 21 12 privacy policy effective 3 21 12 California privacy rights material site may reproduced distributed transmitted cached otherwise used except prior written permission Cond Nast"),
('Infographic: R vs Python for data science', 'DataCamp Blog Go DataCamp Choosing R Python data analysis infographic Introduction DataCamp LATEST POSTS Free Kaggle Machine Learning Tutorial R 28th May 2015DataCamp R Certifications Available LinkedIn Profile 18th May 2015 R Choosing R Python data analysis infographic Posted DataCamp May 12th 2015 think ll agree say HARD know whether use Python R data analysis especially true re newbie data analyst looking right language start turns many good resources help figure strengths weaknesses languages often go great detail provide tailored answer questions use Machine Learning need fast solution go Python R today post present new Infographic Data Science Wars R vs Python highlights great detail differences betweens two languages data science point view next time debating R vs Python machine learning statistics maybe even Internet Things look infographic find answer R vs Python DataCamp often get emails asking whether one use R Python performing day-to-day data analysis tasks Python R amongst popular languages data analysis supporters opponents Python often praised general-purpose language easy-to-understand syntax R functionality developed statisticians mind thereby giving field-specific advantages great features data visualization new infographic Data Science Wars R vs Python therefore everyone interested two statistical programming languages relate infographic explores strengths R Python vice versa aims provide basic comparison two programming languages data science statistics perspective Interested infographics Check Statistical Language Wars Become Data Scientist 8 Easy Steps Twitter Facebook DataCamp https www datacamp com View Comments 0 Navigation Go DataCamp DataCamp posts found R-bloggers com Copyright DataCamp Blog 2015 rights reserved Proudly published WordPress'),
('fastFM: A Library for Factorization Machines (x-post r/CompressiveSensing)', "Nuit Blanche name Igor Carron homepage Page Views Nuit Blanche since July 2010 Follow IgorCarron Cite Nuit Blanche related pages recent Compressive Sensing article Scientific Reports Attendant Project Page Please join comment Google Community 1502 CompressiveSensing subreddit 811 Facebook page LinkedIn Compressive Sensing group 3293 Advanced Matrix Factorization Group 1017 Reference pages include Big Picture Compressive Sensing Learning Compressive Sensing Advanced Matrix Factorization Jungle Page Highly Technical Reference Pages - Aggregators Technologies Exist CAI Cable Igor's Adventures Matrix Factorization search Reproducible Research page Paris Machine Learning Meetup Archives Meetup com register 2222 members LinkedIn post jobs 721 Google 233 Facebook follow-on discussions Twitter Monday May 11 2015 fastFM Library Factorization Machines - implementation - Email ThisBlogThis Share TwitterShare FacebookShare Pinterest fastFM Library Factorization Machines Immanuel Bayer Factorization Machines FM used narrow range applications part standard toolbox machine learning models pity even though FMs recognized successful recommender system type applications general model deal sparse high dimensional features Factorization Machine implementation provides easy access many solvers supports regression classification ranking tasks implementation simplifies use FM's wide field applications implementation potential improve understanding FM model drive new development introduction lirary Easy interfacing dynamic interactive languages R Python Matlab ii Python interface allows interactive work iii publicly available testsuite thatstrongly simpli es modi cations adding new features iv Code released theBSD-license allows integration almost open source project GitHub repository https github com ibayer fastFM Related Thierry Silbermann presented libFM Factorization Machines Paris Machine Learning 6 Season 2 presentaton 1 24 45 2 10 42 minutes video Thierry Silbermann University Konstanz libFM Factorization Machines Join CompressiveSensing subreddit Google Community post Liked entry subscribe Nuit Blanche's feed there's came also subscribe Nuit Blanche Email explore Big Picture Compressive Sensing Matrix Factorization Jungle join conversations compressive sensing advanced matrix factorization calibration issues Linkedin labels implementation MF python Igor 5 11 2015 10 30 00 comments Post Comment Newer Post Older Post Home Subscribe Post Comments Atom Printfriendly Nuit Blanche Stemming Data Tsunami One Algorithm Time Tweet Igor Search Nuit Blanche Loading Subscribe E-MAIL Nuit Blanche Get new entries directly mailbox enter email address Nuit Blanche Nuit Blanche blog focuses Compressive Sensing Advanced Matrix Factorization Techniques Machine Learning well many engaging ideas techniques needed handle make sense high dimensional data also known Big Data Nuit Blanche french expression translates nighter restless night Contact Cassini MSL Oppy HiRISE SOHO SDO Rosetta-Philae Rosetta Contact igorcarron gmail com Webon LinkedIn Twitter Pages Home Reproducible Research implementations Randomized Numerical Linear Algebra RandNLA Advanced Matrix Factorization Learning Compressed Sensing It's CAI Cable Igor's Adventures Matrix Factorization Machine Learning Meetups Around World Compressed Sensing Pages Focused Interest Pages Datasets Challenges Nuit Blanche Conversations Linking Nuit Blanche blogs CS Meetings Real Time Experiments Highly Technical Reference Pages - Aggregators Recent Nuit Blanche entries Paris Machine Learning Meetup Archives Pinterest Boards Imaging Nature Technologies Exist Wondering Star Computational Photography Subscribe LinkedIn Matrix Factorization Group 1001 members right one Link stats Subscribe Nuit Blanche RSS Feed Posts Atom Posts Comments Atom Comments Subscribe LinkedIn Compressive Sensing group 3145 members right one Link stats Google Badge updated profile LinkedIn reflect activities Nuit Blanche means provide recommendations based experience reading blog Nuit Blanche QR code Search Nuit Blanche LoadingLatest news Compressive Compressed Sensing Arxiv Full Text Search Arxiv Google Compressive Sensing Compressed Sensing 24 hours week month Rice University Compressive Sensing repositoryLatest news Matrix Factorization Arxiv old Arxiv new Google 24 hours week month Readership Statistics another set watching blog feedreaders 740 readers receive every entries mailboxes 600 people come site directly everyday detailed information following blog entries far site seen 3 500 000 pageviews since counter installed 2007 Nuit Blanche Referenced Dead Tree World Big Picture Compressive Sensing mentioned article La Recherche french speaking equivalent competitor Science October 2010 issue page 20-21 Wired Magazine piece Compressed Sensing featuring links blog Big Picture March 1 2010 Emmanuel Candes Terry Tao wrote Nuit Blanche Dec '08 issue IEEE Information Theory Society Newsletter Xiaochuan Pan Emil Sidky Michael Vannier wrote Nuit Blanche commercial CT scanners still employ traditional filtered back-projection image reconstruction Check also acknowledgments Ghost Imaging paper one Like Link Xi'an's Og Toscana 3 - Filed Mountains pictures Running Travel Wines Tagged Chianti farmhouse Italia ruins sunset Tuscany 33 minutes ago Hack Day Eye-Controlled Wheelchair Advances Talented Teenage Hackers - Myrijam Stoetzer friend Paul Foltin 14 15 years old kids Duisburg Germany working eye movement controller wheel chair Th 50 minutes ago Terahertz Technology Abstract-Terahertz response patterned epitaxial graphene - Christian Sorger Sascha Preu1 2 Johannes Schmidt3 Stephan Winnerl3 Yuliy V Bludov4 Nuno M R Peres4 Mikhail Vasilevskiy4 Heiko B Weber1 http 1 hour ago Endeavour Data code regulation - Data code code data distinction software code input data blurry best arbitrary worst distinction 6 hours ago Another Word Cybersecurity Authoritative Reports Resources Topic Need Librarians - Cybersecurity Authoritative Reports Resources Topic Rita Tehan Information Specialist Congressional Research Service summary 18 hours ago Image Sensors World Sony 2015 IR Day - Sony held 2015 Investor Relations Day today Device Segment presentation Tomoyuki Suzuki Executive Deputy President Corporate Executive Offic 21 hours ago free hunch Interactive R Tutorial Machine Learning Titanic Competition - Always wanted compete Kaggle competition sure right skill set DataCamp created free interactive tutorial help 21 hours ago High Noon GMT Oh torn 'twixt love an' tenure Behold molten children Recurrent Neural Networks Generating Even Word God - Please note post taken seriously show artificial system learned enough lot text generate new text 2 days ago What's new Cancellation multilinear Hilbert transform - ve uploaded arXiv paper Cancellation multilinear Hilbert transform submitted Collectanea Mathematica paper uses method 2 days ago G del's Lost Letter P NP John Alicia Nash 1928 1933 2015 - condolences Awesome Stories source John Nash wife Alicia killed taxi accident New Jersey Turnpike Saturday afternoon wer 2 days ago Timothy Lottes OS Project 7 - PS 2 Misc - started back long break Finished PS 2 driver Simplified keyboard interface one 64-bit bit array memory simple p 3 days ago Ergodic Walk AISTATS 2015 talks one day - attended AISTATS day change year unfortunately due teaching missed poster Shuang Song presented work 3 days ago Herve La cuisine cr erait-elle son objet - La chimie cr e son objet la phrase est paradoxale dit qu'elle est du chimiste Marcellin Berthelot mais est-elle vraiment de lui Voir Marcellin 3 days ago ChapterZero quick thought Supernatural tv shows - finished season 9 Supernatural ve got give show credit one demands deus ex machina ending Anything less 3 days ago Mr Vacuum Tube Phased Array Radar Looking Walls - Phased Array Radar Looking Walls http blog array2016 org p 24 5 days ago Walking Randomly MATLAB Vectorisation double-edged sword - Imagine new MATLAB programmer create N x N matrix called j j first attempt solution might 6 days ago slice pizza Morning Madness Ode Mercedes - morning madness drive along skyline Cutting fog thick stew waves dew 70mph hugging memorized curves without 1 week ago Decision Science News Gelman sense dubious Science article - Statistician Andrew Gelman sense something dubious Science article soon published post Gelman sense 1 week ago 0xDE Graham Erd Egyptian fractions - recent paper Ron Graham surveys work Paul Erd Egyptian fractions know Erd s' second paper subject didn't p 1 week ago Geomblog ITA conference really enjoy - Continuing thoughts STOC 2017 reboot went back Boaz's original question would make likely go STOC thought I'd 1 week ago Pillow Lab Blog Fast Kronecker trick Gaussian Process regression expressive kernels - May 11th presented following paper lab meeting Fast Kernel Learning Multidimensional Pattern Extrapolation Andrew Gordon Wilson Elad Gil 2 weeks ago Machine Learning etc ICLR 2015 - ICLR posters caught eye larger image simple implement idea gives impressive results force two groups units un 2 weeks ago Libres pens es d'un math maticien ordinaire adventure Google search - interesting experience Google recently related Electronic Journal Probability EJP Electronic Communications Probability ECP 2 weeks ago tombone's blog Dyson 360 Eye Baidu Deep Learning Embedded Vision Summit Santa Clara - Bringing Computer Vision Consumer Mike Aldred Electronics Lead Dyson Ltd vision research priority decades results 3 weeks ago Secrets Consulting Requirements Hints Variations - volumes Exploring Requirements follow chapter section hints variations topic chapter Many readers tel 3 weeks ago Harvest Imaging Blog Third HARVEST IMAGING FORUM December 2015 - successful forums 2013 2014 third one organized December 2015 Voorburg Hague Netherlands basic inte 3 weeks ago m bandit COLT 2015 accepted papers cool videos - Like last year compiled list COLT 2015 accepted papers together links arxiv version whenever could find one papers 3 weeks ago Information Structuralist Counting bits Vapnik Chervonenkis - Machine learning enabling computers improve performance given task get data express intuition quantitative 4 weeks ago Le Petit Chercheur Illustr Quasi-isometric embeddings vector sets quantized sub-Gaussian projections - Last January honored invited RWTH Aachen University Holger Rauhut Sjoerd Dirksen give talk general topic quantized co 4 weeks ago Machine Learning Theory Randomized experimentation - One good thing machine learning present people actually use back-ends many systems interact daily basis 5 weeks ago La vertu d'un LA virtue - fortunate hive D dom nologie la science du traitement de donn es signal images etc - O l'on propose le n ologisme d dom nologie pour d signer la technique la pratique la science du traitement de signal et de l'analyse d'images au c 1 month ago robots net Robots Podcast Farewell robots net join us Robohub - Since May 2007 colleagues Robots Podcast Robohub working robots net bring latest news views robo 1 month ago Machine Learning Deep Learning Works II Renormalization Group - Deep Learning amazing Deep Learning successful Deep Learning old-school Neural Networks modern hardware w 1 month ago Follow Data Genomics Today Tomorrow presentation - Slideshare link widget presentation gave Genomics Today Tomorrow event Uppsala couple weeks ago March 19 2015 sp 1 month ago Adventures Signal Processing Open Science Open Access Journals Missing - end could value proposition future journals 1 month ago Thoughts Mysterious Universe State Probabilistic Programming - two weeks last July cocooned hotel Portland living breathing probabilistic programming student probabilistic p 1 month ago Epistasis Blog Biomedical Informatics Faculty Positions University Pennsylvania - recently moved research lab Perelman School Medicine University Pennsylvania serve Director Institute Biomed 2 months ago Petros Boufounos Internship Opening Sensor Fusion - new internship opening MERL area sensor fusion posting follows MM880 Sensor fusion MERL looking well qualified 2 months ago G-media Le blog Nouveaut d veloppeurs domotique et openpicus Prise Gigogne et Dolphin View - Dans cet article nous allons voir comment utiliser la prise gigogne avec mesure de comptage depuis le logiciel Dolphin View Mat riel n cessaire une c 2 months ago Neurevolution Neurevolution relaunch - hard believe started blog eight years ago way back grad students long way ve come Patryk Dir 2 months ago Inductio Ex Machina Upcoming Conference Sydney Predictive APIs Apps - big kick-off Barcelona last year 200 attendees second International Conference Predictive APIs Apps held Sydn 2 months ago Ars Mathematica Nine Chapters Semigroup Art - Googling something came across Nine Chapters Semigroup Art leisurely introduction theory semigroups 2 months ago yellow noise takis champs magnetiques palais de tokyo february 2015 - takis champs magnetiques palais de tokyo february 2015 3 months ago polylogblog CPM 2015 - Ely asked remind everyone deadline 26th Annual Symposium Combinatorial Pattern Matching fast approaching 2nd F 4 months ago Mirror Image Kinect depth sensor works stereo triangulation simple ways speed convnet little - quite complex methods making convolutional networks converge faster Natural gradient dual coordinate ascent second order hessian fre 5 months ago natural language processing blog myth strong baseline - probably count fingers number papers I've submitted reviewer hasn't complained baseline way don't mean 6 months ago Building Intelligent Probabilistic Systems Harvard Center Research Computation Society Call Fellows Visiting Scholars - Harvard Center Research Computation Society CRCS solicits applications Postdoctoral Fellows Visiting Scholars Programs 7 months ago Pixel shaker Pics Manipulated Photos Notable Historic Figures Digital Era Images - Manipulating photos happened way Photoshop around series shows afters famous notable figures digital era 7 months ago Victoria Stodden input OSTP RFI reproducibility - Sept 23 2014 US Office Science Technology Policy Whitehouse accepting comments Strategy American Innovation 8 months ago Ga l Varoquaux Hiring engineer mine large brain connectivity databases - Work us leverage leading-edge machine learning neuroimaging Parietal research team work improving way brain images analyz 8 months ago Computers don't see Compiling OpenCV 3 0 alpha CUDA support MacOS X - quick tip people troubles compiling OpenCV 3 0 alpha MacOS X variable called CUDA_TOOLKIT_DIR cmake configura 8 months ago Herr Strathmann - home Shogun NYC - late August invited NYC present Shogun open-source Machine Learning software workshop link organised John Langford Seeing Sh 8 months ago Lousodrome Vie au Japon Le certificat de r sidence j minhy - Le certificat de r sidence j minhy en japonais est un simple document d une page qui certifie votre adresse de r sidence et qui est demand pour 10 months ago trekkinglemon's fresh squeeze Data processing flashy yellow Peyresq 2014 Last day - 2014 edition Peyresq summer school finished coda let us summarize last talk Continue reading 10 months ago Camdp com updates DataOrigami Launch - I'm proud announce latest project dataorigami net still go check 11 months ago Neighborhood Infinity Cofree meets Free - - LANGUAGE RankNTypes MultiParamTypeClasses TypeOperators - Introduction spoke BayHac 2014 free monads asked co 1 year ago Statistical Trader Follow twitter StatTrader - Since working full time managing Data Sciences team Bloomberg Global Data haven't done sort long-form blogging used 1 year ago Ivan Oseledets homepage Introduction cross approximation - written short incomplete introductory page skeleton decomposition using maxvol algorithm update references 1 year ago MAKE Magazine New Project TV-B-Gone Kit - Tired LCD TVs everywhere Want break advertisements re trying eat Want zap screens across street TV-B-Go 1 year ago i2pi com focus - focus 1 year ago brain map statistics connection Linus Pauling fMRI - think Linus Pauling two things come mind work nature chemical bond awarded 1954 Nobel Prize Chem 1 year ago Computational Information Geometry Wonderland New blog address Moving Wordpress - use anymore blog system rather use Wordpress supports many goodies like latex Please update yo 1 year ago Martin Tall Gaze Interaction Introducing Eye Tribe Tracker - It's great pride today introduce Eye Tribe Tracker It's worlds smallest remote tracker first use USB3 0 one 100 1 year ago De Rerum Natura Functional programming - least interesting concept programming languages purity Java prime example Everything object much prefer Python wa 1 year ago Doyung Pig Hive Pig Hive - Hadoop ecosystem Data processing Pig Hive Pig Hive pig hive data 2 years ago BlackbordRMT Dyson Brownian motion thirty stationnary Brownian motions Ornstein Uhlenbeck - image Image Hosted ImageShack us Dyson Brownian motion thirty stationnary Brownian motions Ornstein Uhlenbeck processes constr 2 years ago Freakonometrics Mod lisation et pr vision cas d' cole - Quelques lignes de code que l'on reprendra au prochain cours avec une transformation en log et une tendance lin aire Consid rons la recherche du mot c 2 years ago Brain Windows WordPress accepts Bitcoin - WordPress blog platform accepts bitcoin hosting costs via BitPay WordPress hosts Brain Windows many world biggest best blogs 2 years ago Science Sands Science Sands moved - Dear readers migrated blog along things new site http davidketcheson info New posts longer appear blo 2 years ago Journey Randomness SMC sampler - monte carlo sequential monte carlo SMC sampler Jasra Doucet Del Moral 2 years ago Hao's TechBlog Sampling Rate Pixels compare sampling rate camera single-pixel camera - beginning I'd like make clear two terms Nyquist frequency Nyquist rate may take thing even text 2 years ago bpchesney org Nested Linear Programs Find Agreement among Sensors - Suppose 3 sensors B C set readings goes column matrix b matrix constructed bA bB bC repr 2 years ago Stupid Matlab Hacks busy recently hacks mean time go play - busy recently hacks mean time go play http www mathworks com matlabcentral fileexchange 37104-kappatau mak 2 years ago Becoming Astronaut Orbiters going - month NASA commenced delivery four Space Shuttle orbiters final destinations extensive decommissioning process 3 years ago FUTUREPICTURE Note comments - six months ago hit serious rash spam 20 000 comments posted span two weeks Unfortunately ti 3 years ago CyberGi Os ciborgues da Campus Party - Essa semana fui na quinta edi o da Campus Party e ontem dia 9 tive o prazer de conhecer dois ciborgues o Rob Spencer que fez o document rio Eyeborg q 3 years ago Collective Research Interaction Sound Signal Processing Sonification Handbook - yet heard Sonification Handbook edited Thomas Hermann Andy Hunt John G Neuhoff published even better freely avai 3 years ago Espace Vide PCA Compressive Measurements Video - I've little bit fun visualizations today outcomes pretty nice potentially artistic thought I'd share 3 years ago inspiration etc Session 4 5th Graders - Doodling 2 - Session 4 - Doodling Approximately one half hours Starting one element Adding proximity expanding drawing Tool 3 years ago Marcio Marim Welcome new website - time work second version didn publish happy release new website share creations interests whi 3 years ago Cognitive Radio Blog G Vazquez-Vilar PhD Thesis Interference Management Cognitive Radio - image Thesis Cognitive radio dissertation examination took place couple weeks ago Happily passed say one unexpect 3 years ago OISblog Liquid Crystal Eyeglasses - Pixel Optics introduced Empower eyeglasses use liquid crystal lenses actively adjust power 4 years ago Big Numbers Going commission - m going focus completely school blog going hiatus months ll start ve passed hurd 4 years ago Another Dimension three musketeers - Given vectors three quantities interesting indeed fact concept inner product vector space revolves largely around thos 4 years ago Electrons holes Indefinite hiatus - blog put indefinite hiatus 4 years ago Arthur Charpentier Blog transfert - mentioned past weeks blog transfered please update links bookmarks redirected shortly http freako 4 years ago Chaotic Pearls Indonesian Contoh Program Phase Unwrapping - Ide dari phase-unwraping PU progresif ini muncul di suatu sore hari ketika saya sedang berjalan-jalan di sekitar kampus sekitar tahun 2002-an Saya ber 4 years ago YALL1 ALgorithms L1 Announcements - June 4 2010 Toeplitz circulant sampling demos released June 4 2010 YALL1 version 1 0 released open-source Download link YALL1 n 4 years ago Hashimoto Laboratory's Blog Personal Mobility Next Level - Improving concept self-balancing unicycle Honda introduced brand new U3-X new personal mobility platform regular large whee 5 years ago Lianlin Li's Compressive Sensing blog Chinese Igor's Blog today - Today Post updated Igor's blog following CS Matrix Completion via Thresholding Dick Gordon's Op-Ed Lianlin L 5 years ago Guan Gui's blog expoit channel structure - 6 years ago Three-Toed Sloth - ML Counterexamples Pt 2 - Regression Post-PCA camdp com blogs - Willow Garage Blog Willow Garage - Latest News - KinectHacks net - Little Knowledge - Blog Robotics - Show 25 Show Another Blog List Haldane's Sieve SWEEPFINDER2 Increased sensitivity robustness flexibility - SWEEPFINDER2 Increased sensitivity robustness flexibility Michael DeGiorgio Christian D Huber Melissa J Hubisz Ines Hellmann Rasmus Nielsen Su 4 hours ago Information Processing John Nash dead 86 - original title post won Nobel Memorial Prize see sad news bottom Beautiful Mind Nash went see von Neuman 4 days ago Scientific Clearing House Selection week - Violinist Hilary Hahn hails Baltimore pianist Valentina Lisitsa play first movement American composer Charles Ives Fourth Sonata 6 days ago leon bottou org news news graph_transducer_networks_explained - Graph Transducer Networks explained scavenging old emails couple weeks ago found copy early technical report describes 2 weeks ago tombone's blog Dyson 360 Eye Baidu Deep Learning Embedded Vision Summit Santa Clara - Bringing Computer Vision Consumer Mike Aldred Electronics Lead Dyson Ltd vision research priority decades results 3 weeks ago Property Testing Review News April 2015 - April busy time property testing 9 papers posted online month let jump right Testing properties graphs Approximately Co 3 weeks ago Relax Conquer Courant Institute Mathematical Sciences - Finally longer job market excited announce join Courant Institute Mathematical Sciences Assistant Profess 1 month ago Moody Rd Competing data science contest without reading data - Machine learning competitions become extremely popular format solving prediction classification problems sorts famous ex 2 months ago AK Tech Blog Neustar SIAM SODA 2015 - Author Note Hello readers m Sonya Berg first post Neustar research blog data scientist Neustar Research foc 4 months ago Mostly linguistically computational Adventure collaborative filtering information retrieval matrix factorization stuff Count-Min-Log Strange effect - followed previous steps mentionning previous post odd behaviour Count-Min-Log MAX sampling w 5 months ago Deep Learning Recent Reddit AMA Deep Learning - Recently Geoffrey Hinton Yann Lecun Yoshua Bengio reddit AMA subscribers r MachineLearning asked questions AMA contains 6 months ago jim learning choose mentor - http www cell com neuron fulltext S0896-6273 13 00907-0 https www cs princeton edu courses archive spring15 cos598D http www cs princeton edu cours 1 year ago Blog Interphase Transport Phenomena Laboratory Texas M University Fun Boiling - 1 year ago Math Good another WordPress site Linear Algebra good - Linear algebra branch mathematics concerned solving linear systems natural think linear algebra solving unkno 1 year ago BayesRules STAT 330 November 29 2012 - finished discussion model selection averaging went missing data models cases fact data observed h 2 years ago Comments Lecture Schedule Embryo Physics Course - Blog List FlowingData Compare curve reality income versus college attendance - image Draw line grow poorer families less likely go college grow richer families likely 1 hour ago SynBioFromLeukipposInstitute Scoop Three developments help synthetic biology live promise Genetic Literacy Project - Three developments help synthetic biology live promise Dominic Basulto May 28 2015 Washington Post See Scoop via 2 hours ago Retraction Watch chocolate-diet sting study retracted coverage doesn surprise news watchdog - Yesterday John Bohannon described i09 com successfully created health news conducted flawed trial health benefits chocolate 4 hours ago Statistical Modeling Causal Inference Social Science Cracked com Huffington Post Wall Street Journal New York Times - David Christopher Bell goes trouble link Palko explain Every Map Popular _________ State Bullshit long 4 hours ago olimex MOD-LCD3310 OSHW monochrome LCD 84 48 pixels board UEXT connector - MOD-LCD3310 Open Source Hardware board released Apache 2 0 Licensee low cost 84 48 pixels LCD connect development bo 6 hours ago Science-Based Medicine Florida strikes Brian Clement - Brian Clement charlatan Unfortunately doesn seem problem State Florida made two turned three attempts g 12 hours ago Sage Open Source Mathematics Software Guiding principles SageMath Inc - February year 2015 founded Delaware C Corporation called SageMath Inc first stab guiding principles compan 21 hours ago Quomodocumque evil impulse good - learned teaching Rabbi Rebecca Ben-Gideon last week turning mind Rabbi Nahman said Rabbi Samuel name Behold 1 day ago Machine Vision 4 Users re backlighting cylindrical parts - see backlighting used time machine vision training classes trade shows typically gauging locating shapes Look closely though 1 day ago Skulls Stars comics moment inspires Suicide Squad - Update Forgot say thank Dad mailing complete Suicide Squad collection made whole post possible suspect peop 1 day ago InnoCentive Challenges Bioabsorbable Elastomeric Film - Seeker looking bio-absorbable elastomeric film specific properties could existing product one adapted 1 day ago What's new Cancellation multilinear Hilbert transform - ve uploaded arXiv paper Cancellation multilinear Hilbert transform submitted Collectanea Mathematica paper uses method 2 days ago Ben Krasnow physics floating screwdrivers - explain jet air float common screwdriver Plans make fluid turbulence disc http makezine com projects rheoscopic-coffee-tab 2 days ago Short Fat Matrices Three Paper Announcements - ve pretty busy lately writing researching visitors announcements serve quick summary ve 1 Tables 1 week ago 2Physics Current Fluctuations - left right Pierre Pfe er Fabian Hartmann Sven H ing Martin Kamp Lukas Worschech Authors Pierre Pfe er1 Fabian Hartmann1 Sven H ing1 2 1 week ago Large Scale Machine Learning Animals Open Data Science Conference - May 30 Boston - 1 week ago Zhilin's Scientific Journey Yann LeCun's Comments Extreme Learning Machine ELM - Yann LeCun https www facebook com yann lecun posts 10152872571572143 Facebook commented ELM quoted What's great Ext 2 weeks ago Gowers's Weblog Nick Clegg Liberal Democrat - life found Liberal Democrat policies Liberal-SDP Alliance policies Liberal policies n 4 weeks ago JeremyBlum com Shapeoko2 CNC Mill Build Log Review - new Inventables Shapeoko2 CNC mill carving away Watch timelapse build-log thorough review do-it-yourself CNC milling machine Continue 4 weeks ago F Pedregosa IPython Jupyter notebook gallery - Draft - TL DR created gallery IPython Jupyter notebooks Check - image Notebook gallery couple months ago put online website d 5 weeks ago Richard Baraniuk Probabilistic Theory Deep Learning - Patel Nguyen R G Baraniuk Probabilistic Theory Deep Learning arXiv preprint arxiv org abs 1504 00641 2 April 2014 grand challeng 1 month ago Welcome Sparse Land Discreteness Sparsity - Discrete signals may sparse whereas sparse signals may discrete Let us consider following signal x 1 1 -1 1 -1 -1 1 1 2 months ago Various Consequences Reliability Growth Enhancing Defense System Reliability - report pdf National academies reliability growth interesting There's lot good stuff design reliability physics fai 2 months ago Inductio Ex Machina Upcoming Conference Sydney Predictive APIs Apps - big kick-off Barcelona last year 200 attendees second International Conference Predictive APIs Apps held Sydn 2 months ago Highly Scalable Blog Data Mining Problems Retail - Retail one important business domains data science data mining applications prolific data numerous optimization p 2 months ago regularize Farewell Ernie Esser - Today learned Ernie Esser passed away March 8th 2015 age 34 Ernie PostDoc UBC Vancouver PhD UCLA worked applied math 2 months ago Thingiverse Blog MakerBot PrintShop 1 4 3D Print iPad 3G 4G - Last week introduced exciting new feature MakerBot 3D Ecosystem ability start 3D print remotely using smartphone MakerBot P 3 months ago much little time Post-doc Molecular Informatics Opening NCATS - post-doc opening Informatics group NCATS work computational aspects high throughput combination screening topics includ 3 months ago next big thing syndrome Books read 2014 - disclaimer book cover images post Amazon Affiliate links click buy book receive cents form 5 months ago Pursuits Null Space Blog done moved - first hesitant handling latex Rolf Mathcination pointed excellent tool latex-to-wordpress wil 5 months ago Proof Pudding M571 Fall 2014 Lecture 5 - 1 Agenda QR Factorization Gram-Schmidt classical modified Householder QR reflectors ended discussion projectors several impo 6 months ago Tianyi Zhou's Research Blog Learn Low-rank Sparse Structures via Randomized Alternating Projections List Submodular Optimization Streaming Data Update - Coresets k-Segmentation Streaming Data NIPS 2014 Streaming Submodular Optimization Massive Data Summarization Fly KDD 2014 8 months ago Wondering Star Lunar Detection Ultra-High-Energy Cosmic Rays Neutrinos - Spotted ArXiv Physics blog using SKA array Moon collector would certainly qualify sensors size pl 8 months ago Dan's Blog make paper spherical panorama - image image Photos usually show rectangular fragment scene image taken Typical panoramic images display landscap 8 months ago Seth's blog Videos Seth Roberts Memorial Talks - Video recordings Ancestral Health Society public talks August 10 2014 honoring Seth life work posted http bit ly 1v33kbM Many 9 months ago Thoughts Artificial Intelligence - blog moved new post metaphor mathematics new blog 9 months ago Lupi Software thoughts Monitorama 2014 PDX - Monitorama fantastic conference came mixed feelings Great work done open source software however based con 1 year ago Andrej Karpathy Blog Interview Data Science Weekly Neural Nets ConvNetJS - Quick post thought mention ve given interview two months ago ConvNetJS background perspectives neural 1 year ago Ivan Oseledets homepage Introduction cross approximation - written short incomplete introductory page skeleton decomposition using maxvol algorithm update references 1 year ago Artificial Intelligence Blog John Dobson 1915 2014 - September 2008 met John Harrisburg slept overnight friend Zoungy place enjoyed beautiful drive Cherry Springs th 1 year ago Hey What's BIG idea 3D Printing Tangible Idea - heywhatsthebigidea net B J Rao write another article 3D printing internet already offers abundance information subject Moun 1 year ago Normal Deviate END - addition best comedy TV show ever Seinfeld great source wisdom one episode Jerry counsels George hit high 1 year ago Math Good another WordPress site Linear Algebra good - Linear algebra branch mathematics concerned solving linear systems natural think linear algebra solving unkno 1 year ago tcs math - mathematics theoretical computer science Kadison-Singer Quantum Mechanics - accounts Kadison-Singer problem mention arose considerations foundations quantum mechanics Specifically question abo 1 year ago programming machine learning blog BibTeX-powered publications list Pelican pelican-bibtex - Hook Wouldn like manage academic publications list easily within context static website Without resorting external services 2 years ago Mathblogging org -- Blog Mathematical Instruments Gianluigi Filippelli - post part series Mathematical Instruments introduce math bloggers listed site Today Gianluigi Filipp 2 years ago openPicus blog Better BBQ Flyport Cosm - knew Austria famous BBQ remember summer 2006 great barbeque austrian friends Klagenfurt fantastic 8 2 years ago Dirac Sea Survey Non Parametric Bayesian marginal applications - Survey Non Parametric Bayesian marginal applications mystery Zoubin one favorite researchers papers usually 2 years ago Biophotonics Review Biology Games Biomedical Research - Games become important part cultures bringing lots entertainment creativity societies Beyond traditional understanding o 2 years ago Clouds Fukushima 9 Months later - regards airborne radiation ground measurement reading NaI detector KEK Tsukuba showing steady decline past 3 years ago Gigapixel News Journal ipConfigure Presents First Gigapixel Wide Area Surveillance Platform - ipConfigure privately owned software research development company announces world first multi-Gigapixel surveillance platform designed 3 years ago Gustavo Tinkers Source Code GUI - Hosted https github com goretkin soundcard-radar 3 years ago Passive Vision Passive Vision - Welcome plan use blog ideas research teaching moment m looking forward teaching Computer Vision class 60 5 years ago Dick Gordon's blog - n 45 years ago Show 25 Show Search Pages Link Loading Previous Entries 2015 207 May 2015 37 Compressive Sensing work Three-dimensional coo Robust Rotation Synchronization via Low-rank Low-Rank Matrix Recovery Row-and-Column Affin Self-Dictionary Sparse Regression Hyperspectra Randomized Robust Subspace Recovery High Dimen Book Dictionary Learning Visual Computing Compressed Nonnegative Matrix Factorization Fas Saturday Morning Videos Slides Videos IC PCANet Simple Deep Learning Baseline Image Four million page views million million Great Convergence FlowNet Learning Optical F CSjob Post-Doc Structured Low-Rank Approximati Low-rank Modeling Applications Image Solving Random Quadratic Systems Equations N Identifiability Blind Deconvolution Subspa Tensor time Adaptive Higher-order Spectral Estima Blue Skies Foundational principles large scal Tensor sparsification via bound spectral Saturday Morning Videos Reruns Hamming's time Important Things Commodi Newton Sketch Linear-time Optimization Algorith Self-Expressive Decompositions Matrix Approxim Tonight Paris Machine Learning Meetup 9 Season Factorization Machines implementation Theano Frequent Directions Simple Deterministic Mat fastFM Library Factorization Machines - imp Structured Block Basis Factorization Scalable Tensorial Kernel kernel-based framework tens Kernel Spectral Clustering applications - impl Sparse LSSVM Reductions Large-Scale Data Random Bits Regression Strong General Predictor Phase retrieval via Kaczmarz methods - implementat Great Convergence Deep Learning Compressive Phase Transition Joint-Sparse Recovery Mul Saturday Morning Videos IMA Workshop Convexity Saturday Morning Videos IMA AP Workshop Inform Nuit Blanche Review April 2015 Apr 2015 45 Mar 2015 44 Feb 2015 31 Jan 2015 50 2014 536 Dec 2014 52 Nov 2014 43 Oct 2014 38 Sep 2014 41 Aug 2014 48 Jul 2014 52 Jun 2014 43 May 2014 56 Apr 2014 47 Mar 2014 44 Feb 2014 35 Jan 2014 37 2013 454 Dec 2013 43 Nov 2013 38 Oct 2013 38 Sep 2013 33 Aug 2013 36 Jul 2013 43 Jun 2013 29 May 2013 38 Apr 2013 40 Mar 2013 29 Feb 2013 47 Jan 2013 40 2012 488 Dec 2012 44 Nov 2012 39 Oct 2012 46 Sep 2012 28 Aug 2012 52 Jul 2012 20 Jun 2012 38 May 2012 60 Apr 2012 41 Mar 2012 50 Feb 2012 29 Jan 2012 41 2011 465 Dec 2011 47 Nov 2011 49 Oct 2011 47 Sep 2011 36 Aug 2011 24 Jul 2011 25 Jun 2011 47 May 2011 50 Apr 2011 56 Mar 2011 39 Feb 2011 17 Jan 2011 28 2010 358 Dec 2010 47 Nov 2010 35 Oct 2010 32 Sep 2010 28 Aug 2010 30 Jul 2010 33 Jun 2010 26 May 2010 27 Apr 2010 28 Mar 2010 28 Feb 2010 19 Jan 2010 25 2009 274 Dec 2009 22 Nov 2009 23 Oct 2009 24 Sep 2009 25 Aug 2009 25 Jul 2009 23 Jun 2009 20 May 2009 16 Apr 2009 25 Mar 2009 27 Feb 2009 21 Jan 2009 23 2008 302 Dec 2008 20 Nov 2008 23 Oct 2008 28 Sep 2008 28 Aug 2008 22 Jul 2008 17 Jun 2008 28 May 2008 22 Apr 2008 31 Mar 2008 32 Feb 2008 25 Jan 2008 26 2007 179 Dec 2007 23 Nov 2007 21 Oct 2007 14 Sep 2007 18 Aug 2007 13 Jul 2007 13 Jun 2007 9 May 2007 11 Apr 2007 9 Mar 2007 22 Feb 2007 19 Jan 2007 7 2006 30 Dec 2006 4 Nov 2006 4 Oct 2006 2 Sep 2006 2 Aug 2006 2 Jul 2006 2 Jun 2006 1 Mar 2006 2 Feb 2006 2 Jan 2006 9 2005 88 Dec 2005 1 Nov 2005 6 Oct 2005 3 Sep 2005 12 Aug 2005 1 Jul 2005 7 Jun 2005 4 May 2005 12 Apr 2005 7 Mar 2005 12 Feb 2005 8 Jan 2005 15 2004 214 Dec 2004 18 Nov 2004 8 Oct 2004 20 Sep 2004 44 Aug 2004 29 Jul 2004 13 Jun 2004 13 May 2004 18 Apr 2004 10 Mar 2004 22 Feb 2004 8 Jan 2004 11 2003 12 Dec 2003 10 Nov 2003 2 Books Wish List Start-ups like InView Technology Corporation See Inside World First Compressive Sensing Camera - Read InView white paper InView210 scientific SWIR camera InView210-CSCameraWhitePaper-Feb2015 See inside world first high 2 months ago Centice Centice Drug Analysis Systems Sold Major Federal Agency Aid Prescription Pill Abuse Operations - Nationwide program allows agents identify 3 800 prescription pills illicit drugs RESEARCH TRIANGLE PARK N C October 28 2014 Centice Co 7 months ago Press GraphLab twitterscroll - post twitterscroll appeared first GraphLab Inc 1 year ago Zoomin' - Aqueti TV Interview Gigapixel Images - Scott McCain interviewed July 10 2013 discuss gigapixel imaging WLOS ABC's Asheville affiliate 1 year ago Metamarkets Blog - wise io Machine Learning Service Big Data Analytics - Translate blog Focused Interest Compressed Sensing Compressive Sampling Compressive Sensing Mapping blog entries Compressed Sensing Cognition - Machine Learning Space Search Rescue Compressive Sensing Technology Watch Compressive Sensing Big Picture Compressive Sensing Hardware Compressed Sensing Videos Compressive Sensing Calendar Compressive Sensing Jobs Local Compressed Sensing Codes CS LinkedIn Group Recent links Blog CS Compressive Sensing 2 0 Community Compressive Sensing 2 0 blogs webpages Saturday Morning Cartoons Sherpa Romeo Publisher copyright policies self-archiving Categories Subjects Interest CS 2163 compressive sensing 1627 compressed sensing 1615 compressive sampling 1589 MF 521 implementation 359 Applied Math 209 ML 209 MatrixFactorization 194 space 152 AMP 113 calibration 105 CSHardware 103 CSjobs 90 CS Community 72 SaturdayMorningVideos 71 CSCommunity 66 BlindDeconvolution 64 QuantCS 62 phaseretrieval 62 RandNLA 60 hyperspectral 60 nonlinearCS 59 nuclear 59 technology 58 SundayMorningInsight 57 CSVideo 50 python 50 tensor 49 cognition 47 Meetups 46 Algorithm 45 grouptesting 45 meetup 43 1bit 42 publishing 41 synbio 41 graphlab 38 RandomFeatures 37 darpa 37 CSmeeting 36 AI 33 NuitBlancheReview 33 search rescue 33 weather modeling 33 remote sensing 32 Csstats 31 wow 30 business 29 bayes 28 data fusion 28 machine learning 28 MLParis 27 jim gray 24 neuroscience 23 autonomous 22 dimensionality reduction 22 mapmaker 20 thesis 20 AlexSmola 19 Kaczmarz 19 ParisMachineLearning 19 geocam 19 space debris 19 space situational awareness 19 medical 18 phaserecovery 18 ChristophStuder 17 ImagingWithNature 17 SAHD 17 maps 17 mishap 17 sleep 17 CSCalendar 16 monday morning algorithm 16 transport 16 CAI 15 energy 15 nanopore 15 phasediagrams 15 hasp 13 superresolution 13 ADMM 12 PatrickGill 12 Technologies Exist 12 causality 12 darpa urban challenge 12 sudoku 12 CSDiscussion 11 ICLR2015 11 TRL 11 qa 11 thermal engineering 11 GPU 10 fft 10 sie 10 videos 10 Computational Neuroscience 9 MultiplicativeNoise 9 RandomForest 9 StarTracker 9 aroundtheblogs 9 france 9 ELM 8 GenomeTV 8 HammingsTime 8 PredictingTheFuture 8 Good 8 collaborative task manager 8 exploration 8 random projections 8 situational awareness 8 sparsity 8 wavelet 8 GreatThoughtsFriday 7 TheGreatConvergence 7 collaborative work 7 innovation 7 mems 7 random lens imaging 7 CSCartoons 6 CitingNuitBlanche 6 CompressibleWGN 6 accidentalcamera 6 complexity vizualisation 6 maxent 6 randomization 6 startups 6 streaming 6 thedip 6 RMM 5 UQ 5 book 5 coded aperture 5 muscle 5 tex-mems 5 BP 4 British Petroleum 4 CfP 4 CompressiveSensingWhatIsItGoodFor 4 DataDrivenSensorDesign 4 HusHambug 4 Comment 4 ReproducibleResearch 4 google maps 4 hypergeocam 4 internet traffic 4 jionc 4 microsystems 4 scaling 4 technologie 4 Deepwater Horizon 3 disruptive technology 3 financement de la recherche 3 google 3 julia 3 radiation detection 3 recherche 3 sketching 3 Columbia 2 DC law 2 LowRank 2 MLZurich 2 MMDS 2 ManifoldSignalProcessing 2 NO-C-WE 2 TheNuitBlancheChronicles 2 UAV 2 aggregators 2 anecdote 2 challenge 2 diet 2 genomics 2 kinect hacks 2 microcontroller 2 notebynotecooking 2 sensor network 2 AWGN 1 BaltiAndBioinformatics 1 Blogger 1 CS MF 1 CT 1 CompanyX 1 JOTRSOI 1 Leonardo 1 QIS 1 RMT 1 SKA 1 SaturdayMorningCartoons 1 SensorsTheSizeOfAPlanet 1 YouAreNotPayingAttention 1 advice 1 aha 1 biographies 1 control 1 crowdfunding 1 csoped 1 dataset 1 donoho-tao 1 extremesampling 1 herschel 1 hushamburg 1 iLab 1 inverse problems 1 iot 1 jacques devooght 1 lfe 1 lua 1 memory 1 mindmaps 1 nanopre 1 octopus 1 oped 1 privacy 1 reference 1 request 1 rr 1 seinfeld 1 solver 1 theano 1 wonderingstar 1 youkeepusingthatword 1 sites interest Blogroll Natural Language Blog Hal Daume III Polylog Blog Andrew McGregor Eric Tramel's Espace Vide Blog Compressed Sensing Lianlin Li's Compressive Sensing blog Chinese Space Engineering Research Center Space Engineering Blog Frank Nielsen's Information Geometry blog David Brady's Blog Le Petit Chercheur Illustre Chaotic Pearls Indonesian De Rerum Natura Michele Guieu's blog Ergodic Walk Laurent Duval's site Laurent Duval's blog Thesilog Diffusion des savoirs - Ecole Normale Superieure What's New Terry Tao Statistical Modeling Causal Inference Social Science Andrew Gelman Aleks Jakulin Masanao Yajima Machine Learning etc Yaroslav Bulatov slice Pizza Muthu Mutukrishnan Geomblog Piotr Indyk Suresh Machine Learning Theory John Langford Lemonodor John Wiseman Yet another Machine Learning blog Pierre Dangauthier Make Magazine blog Theses en ligne Neurevolution blog Pedro Davalos website Damaris' blog Olivier's blog Julie's blog Michele Guieu's site Location visitors Nuit Blanche Dilbert counters Powered Blogger"),
('IEPY 0.9.4 released: An open source tool for InformationExtraction in Python', "Toggle Navigation Machinalis Company Services Solutions Blog Contact IEPY 0 9 4 released open source tool Information Extraction Python Posted El Andrawos 2 weeks 2 days ago Comments We're happy announce IEPY 0 9 4 released - Added multicore preprocess - Added support Stanford 3 5 2 preprocess models It's open source tool Information Extraction focused Relation Extraction aimed users needing perform Information Extraction large dataset scientists wanting experiment new IE algorithms give example Relation Extraction trying find birth date John von Neumann December 28 1903 February 8 1957 Hungarian American pure applied mathematician physicist inventor polymath IEPY task identify John von Neumann December 28 1903 subject object entities born relation Features corpus annotation tool web-based UI active learning relation extraction tool pre-configured convenient defaults rule based relation extraction tool cases documents semi-structured high precision required shallow entity ontology coreference resolution via Stanford CoreNLP easily hack-able active learning core ideal scientist wanting experiment new algorithms web-based user interface Allows layman users control aspects IEPY Allows decentralization human input Documentation http iepy readthedocs org Demo using Techcrunch articles http iepycrunch machinalis com Github https github com machinalis iepy PyPi https pypi python org pypi iepy twitter machinalis Tags used post Natural language processing open source python scikit-learn InformationExtraction Share post Share Twitter Share Facebook Previous Next posts Participando de FLISOL 2015 Making case Jython Comments Related Posts IEPY 0 9 released open source tool Information Extraction Python Natural language processing open source python scikit-learn InformationExtraction Support Vector Machines Introduction inner workings support vector machines python machine learning Artificial Intelligence scikit-learn Tags 1 Algorithms 5 Artificial Intelligence 1 Basemap 5 Canopy 11 Code 4 Complex Web Development 3 Crunchbase 2 D3js 2 Data Migration 3 Data Processing 1 Data Warehouse 7 Django 1 Geodjango 1 Gevent 2 Google Maps 1 Graphs 5 Html5 3 Iepy 3 Information Extraction 2 Informationextraction 6 Ipython 6 Iso9001 2 Java 4 Javascript 1 Job-Posting 2 Jvm 2 Jython 19 Machinalis 6 Machine Learning 1 Machine Translation 1 Matplotlib 2 Mysql 8 Natural Language Processing 1 Ninja-Ide 1 Nodejs 1 Numpy 17 Open Source 1 Openlayers 1 Os X 5 Pandas 1 Postgis 3 Postgresql 2 Pydata 37 Python 6 Qms 1 Rabbitmq 1 Redis 4 Scikit-Learn 1 Social Media 3 Socketio 3 Talk 2 Techcrunch 3 Testing 3 Websockets Feeds RSS Atom missing piece machinalis 2009-2015 Github Facebook Twitter Linkedin Hello We'll happy hear tell us story get back soon Name Email Message media contacts Ok aren't form friend reach us ways Twitter machinalis Telephone 54 351 4710977 Email info machinalis com Address Av Roque Saenz Pe 1357 - X5000JFIC rdoba Argentina"),
('Finding Topics in Harry Potter using K-Means Clustering', "DogDogFish Data Science amongst things Menu Skip content HomeAboutAbout Us Search Search Finding Topics Harry Potter using K-Means Clustering May 11 2015May 12 2015 Matthew Sharpe ll open money-shot clusters able find using whole Harry Potter grouping chapter Every cluster plotted separately far messy practical use let look couple clusters detail One clusters Dursley Privet Drive heavy cluster pretty Griphook Goblin heavy cluster featured storyline book 7 Hopefully piqued interest enough continue scrolling see got clusters see words tie together code generating Github https github com Kali89 HarryPotterClusters graphs documents contained went really interesting talk PyData Latent Dirichlet Allocation topic entirely new thought d love apply favourite book series Harry Potter However didn happen instead get heavy rip excellent post http brandonrose org clustering walks cluster documents using bunch techniques including K-Means Step 1 Get plain text copies Harry Potter books make sure re formatted roughly way often case step took bloody ages Step 2 want different documents 7 number Harry Potter books heathens substantially less number sentences 7 books Treating chapter separate document seemed make sense initialise everything need split books chapter point ve nicely got list chapter titles list associated text string Step 3 ve got chapters re going want tokenise text Basically means converting string discrete tokens d think words reason got fancy name m bit careful terminology tokenisation also takes care things like punctuation isn simple splitting string separate words said ve basically taken path least resistance gone simple tokenisation scheme m also going skip pain utf-8 encoding decoding recoding ve basically dropped character ve found least bit complicated Step 4 Next m going perform TF-IDF chapters convert token number look many times token appears given document many documents token appears overall instance m looking see often given word appears particular chapter many chapters throughout 7 books token appears gives us idea important prominent word given chapter taking account common word throughout chapters example Harry likely feature lot given chapter also likely feature every chapter probably isn especially important given chapter classification Nicolas Flamel however going appear reasonable amount chapters rest therefore know Nicolas Flamel important chapters appear Luckily worry implementation TF-IDF sklearn got gives large sparse matrix TF-IDF score word document entries pay close attention parameters ve passed along TfidfVectorizer ripe changing Firstly max_df 0 75 saying care words appear 75 chapters min_df 0 05 saying care words appear fewer 5 chapters see passed along tokeniser m using English stop-words m removing common English words Finally m generating n-grams 1 4 uninitiated n-grams re basically way splitting text handy little chunks example 3-grams following sentence greatest song world would greatest greatest song e c allows pick common phrases Snape said wizarding world setting begging played Step 5 Performing K-means clustering get output like Cluster 18 words Top words maxime madame maxime karkaroff madame hagrid cedric moody krum champions tournament Chapter hungarian horntail Book gobletOfFire Chapter goblet fire Book gobletOfFire Chapter four champions Book gobletOfFire Chapter beauxbatons durmstrang Book gobletOfFire Chapter beginning Book gobletOfFire Chapter yule ball Book gobletOfFire obviously quite handy little cluster successfully managed take chapters one book given ve allowed K-means access book information bit triumph aware Harry Potter ll see Triwizard Tournament heavy cluster Another example Cluster 10 words Top words wormtail cold voice voldemort lord cauldron riddle cedric man master faithful Chapter riddle house Book gobletOfFire Chapter flesh blood bone Book gobletOfFire Chapter death eaters Book gobletOfFire clusters one book m counting result Added bonus re sequential chapters happen know massive Harry Potter geek chapters towards start end book 4 focus heavily Voldemort Peter Pettigrew far good fact could stop wouldn nice visualise clusters see topics ve picked graphically Even said doesn really matter m still going Step 6 First things first let plot chapters one graph colour code book came great way visualising clusters great way seeing everything laid messy picture chapters projected 2d space section shamelessly copied aforementioned blog ultimately re projecting cosine differences tf-idf matrix terms 2-dimensional space declare colour dictionary books rattle chapters plotting m sure ll agree far messy anybody really anything re following along code ll see next generate subplot figure shown top Finally create plots clusters examples Cluster 3 words Top words umbridge professor dont professor umbridge snape sirius im said hermione harrys youre Chapter muggle born registration commission Book deathlyHallows Chapter hogwarts high inquisitor Book orderOfThePhoenix Chapter o w l Book orderOfThePhoenix Chapter second war begins Book orderOfThePhoenix Chapter educational decree number twenty four Book orderOfThePhoenix Chapter centaur sneak Book orderOfThePhoenix Chapter percy padfoot Book orderOfThePhoenix Chapter detention dolores Book orderOfThePhoenix Chapter occlumency Book orderOfThePhoenix Chapter hogs head Book orderOfThePhoenix Chapter fire Book orderOfThePhoenix Chapter professor umbridge Book orderOfThePhoenix Chapter fight flight Book orderOfThePhoenix Chapter seen unforeseen Book orderOfThePhoenix Chapter career advice Book orderOfThePhoenix Chapter snapes worst memory Book orderOfThePhoenix generates Umbridge cluster Everybody favourite professor Dolores Umbridge another Top words hagrid yeh ter said hagrid professor said hermione malfoy o professor trelawney trelawney Chapter professor trelawney's prediction Book prisonerOfAzkaban Chapter talons tea leaves Book prisonerOfAzkaban Chapter firebolt Book prisonerOfAzkaban Chapter diagon alley Book philosophersStone Chapter foribidden forest Book philosophersStone Chapter keeper keys Book philosophersStone Chapter norbert norwegian ridgeback Book philosophersStone Chapter hagrids tale Book orderOfThePhoenix Chapter eye snake Book orderOfThePhoenix Chapter grawp Book orderOfThePhoenix Chapter hermione's helping hand Book halfBloodPrince Chapter rita skeeter's scoop Book gobletOfFire visually Keeper Keys anti-Umbridge Hagrid cluster ll stop random copy pasting clusters stick Github think ve got idea m pretty happy worked dependent individual character names tried looking 2-grams usually gave said exceptions wizarding world said softly death eater godrics hollow ve also put almost zero effort formatting images know roughly meant look like pictures look good exercise m leaving reader imagination loads stuff could m going eat chicken go swimming ll wait well Share GoogleMoreLinkedInRedditTwitterFacebookLike Like Loading Related Data Visualisation clusteringharry pottermachine learningpython Post navigation Building Search Engine E-Commerce Elasticsearch 7 thoughts Finding Topics Harry Potter using K-Means Clustering Dwyane Ren May 13 2015 08 13 K-Means algorithm needs us assign value k choose reasonable value k 25 Reply Matthew Sharpe May 13 2015 08 39 honesty basically tried 9 16 25 decided liked look 25 going square numbers think makes prettier graph Reply Dwyane Ren May 13 2015 10 36 Haha job looks interesting especially Harry Potter fans Alexey May 13 2015 17 27 using euclidean distance cosine Reply Nixonite May 13 2015 20 04 better ways discover good number clusters Look Silhouette Score perhaps elbow method Cool project perhaps consider unless already gotten bored project adding said stoplist Reply Matthew Sharpe May 13 2015 21 11 Thanks m familiar elbow method Silhouette score look Primarily ve used inspection method ve really got commercial practical use m curious really like Harry Potter re right course m getting said place means 2-grams essentially 1-grams said either prepended appended ll get added list see things look Reply Dwyane Ren May 14 2015 01 41 standard socre Silhouette coefficient indicates good clustering k-means Otherwise traversing k 2 huge number choose k k-means model get maximum silhouette score clustering experiments k 2-1000 corpus 1 million silhouette score lower 0 4 Reply Leave Reply Cancel reply Enter comment Fill details click icon log Email required Address never made public Name required Website commenting using WordPress com account Log Change commenting using Twitter account Log Change commenting using Facebook account Log Change commenting using Google account Log Change Cancel Connecting Notify new comments via email Bloggers Josh Forman-Gornall Matthew Sharpe Sean DurkinFollow Blog via Email Enter email address follow blog receive notifications new posts email Recent Posts Finding Topics Harry Potter using K-Means Clustering May 11 2015 Building Search Engine E-Commerce Elasticsearch January 27 2015 Makes Search Engines Special January 7 2015 Emailing multiple inline images Python November 18 2014 Real-Time Analytics Elasticsearch October 7 2014 Categories AB Testing Big Data Data Visualisation Eclipse Guide Hadoop Kaggle Machine Learning Mathematics NHS Data Python R Stock Market Analysis Ubuntu UK House Prices Uncategorized Create free website blog WordPress com Hemingway Rewritten Theme Follow Follow DogDogFish Get every new post delivered Inbox Join 121 followers Build website WordPress com d bloggers like"),
('Deep Boltzmann Machines with Fine Scalability', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1505 02462 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context cs NE prev next new recent 1505Change browse cs cs LG stat stat ML References CitationsNASA ADS Bookmark Computer Science Neural Evolutionary Computing Title Deep Boltzmann Machines Fine Scalability Authors Taichi Kiwaki Submitted 11 May 2015 v1 last revised 12 May 2015 version v2 Abstract present layered Boltzmann machine BM whose generative performance scales number layers Application deep BMs DBMs limited due poor scalability deep stacking layers largely improve performance widely believed DBMs huge representation power poor empirical scalability mainly due inefficiency optimization algorithms paper theoretically show representation power DBMs actually rather limited inefficiency model result poor scalability Based observations propose alternative BM architecture dub soft-deep BMs sDBMs theoretically show sDBMs possess much greater representation power DBMs Experiments demonstrate able train sDBMs 6 layers without pretraining sDBMs nicely compare state-of-the-art models binarized MNIST Caltech-101 silhouettes Subjects Neural Evolutionary Computing cs NE Learning cs LG Machine Learning stat ML Cite arXiv 1505 02462 cs NE arXiv 1505 02462v2 cs NE version Submission history Taichi Kiwaki Mr view email v1 Mon 11 May 2015 00 54 43 GMT 39kb D v2 Tue 12 May 2015 18 58 39 GMT 38kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('A nice selection of Machine Learning papers, books and tutorials', 'favorites Sign rtsairesearch Adaptive AI Engine RTS Games Project Home Wiki Issues Source Export GitHub Checkout Browse Changes Source path svn r127 r128 Directories svn branches tags trunk Filename Size Rev Date Author files selected directory Select subdirectory one exists another directory tree left Terms - Privacy - Project Hosting Help Powered Google Project Hosting'),
('PyNeural - Python library for training neural networks written in Cython', "Skip content Sign Sign repository Explore Features Enterprise Blog Watch 15 Star 205 Fork 9 tburmeister pyneural Code Issues Pull requests Pulse Graphs HTTPS clone URL Subversion checkout URL clone HTTPS Subversion Download ZIP 25 commits 2 branches 0 releases Fetching contributors Python 56 6 C 43 4 Python C branch master Switch branches tags Branches Tags batch master Nothing show Nothing show pyneural merging readme changes latest commit 5682957f58 tburmeister authored May 6 2015 Permalink Failed load latest commit information README md Update README md May 3 2015 kaggle_digit_example ipynb adding iPython notebook example May 3 2015 neural c made handling remainders minibatches effecient remove Apr 27 2015 neural h added old functions speedup mini-batch size 1 Apr 26 2015 pyneural pyx changed NeuralNet train params keyword args defau May 6 2015 setup py first commit - code complete Apr 18 2015 README md PyNeural simple fast Python library training neural networks PyNeural PyNeural neural network library written Cython powered simple fast C library hood PyNeural uses cblas library perform backprogation algorithm efficiently multicore processors PyNeural exposes simple Python API plays nicely NumPy making easy munge data sets needed quickly use train neural network classifiction Installation Make sure cblas library installed Xcode Mac using Mac recommend using Xcode's Accelerate framework includes BLAS implementation Otherwise check OpenBLAS Clone repo Mac CC clang CFLAGS -DACCELERATE -framework accelerate python setup py build_ext -i Linux CC clang CFLAGS -I path cblas include -L path cblas lib -lcblas python setup py build_ext -i Note realize hacky - add proper makefile install script later Usage Example Import library import pyneural Initialize neural net 784 input features 10 output classes 1 intermediate layer 400 nodes nn pyneural NeuralNet 784 400 10 Train network 5 iterations training set mini-batch size 100 alpha gradient descent coefficient 0 01 L2 penalty 0 0 decay multiplier 1 0 meaning alpha decrease iteration nn train features labels 5 100 0 01 0 0 1 0 Get predicted classes labels prediction nn predict_label features Get predicted probability class label predicted_prob nn predict_prob features Performance Core i5 MacBook Pro PyNeural perform 5 iterations Kaggle digits training set approximately 45 seconds mini-batch size 1 11 seconds mini-batch size 200 versus 190 seconds 5 iterations OpenCV roughly 4x 17x speed-up example code used achieve 99 accuracy Kaggle digit recognizer challenge training time roughly 30 minutes see kaggle_digit_example ipynb Full Example code trains neural net MNIST digits data set neural net trained 784 input features 10 output labels one intermediate layer 400 nodes used variation code achieve 98 accuracy Kaggle test set digits recognizer challenge import pyneural import numpy np import pandas pd training_set pd read_csv 'train csv' labels np array training_set 0 features np array training_set 1 astype float 256 n_rows features shape 0 n_features features shape 1 n_labels 10 labels_expanded np zeros n_rows n_labels xrange n_rows labels_expanded labels 1 nn pyneural NeuralNet n_features 400 n_labels nn train features labels_expanded 5 1 0 01 0 0 1 0 data set shuffled iteration 0 completed 9 897468 seconds data set shuffled iteration 1 completed 9 881156 seconds data set shuffled iteration 2 completed 9 771729 seconds data set shuffled iteration 3 completed 9 732244 seconds data set shuffled iteration 4 completed 9 771301 seconds preds nn predict_label features n_correct np sum preds labels print f percent correct training set 100 0 n_correct n_rows 97 040476 percent correct Note Mini-Batch size Using mini-batches significantly speed computation time iteration training set observed increasing mini-batch size increases speed point example example least laptop mini-batch size 100 faster mini-batch size 50 mini-batch size 500 slower code uses slightly different computations mini-batches size 1 mini-batch size 1 faster mini-batch small size greater 1 Additionally increasing size mini-batch decreases rate convergence source even though increasing size mini-batch may increase speed iteration also decreases relative utility iteration I'm really sure best balance anecdotally mini-batch size 100 seems good default library Word Warning still much work progress can't guarantee work every platform I'm sure plenty ways break break probably won't give helpful error messages try remedy things well add features performance future Status API Training Shop Blog 2015 GitHub Inc Terms Privacy Security Contact Something went wrong request Please try"),
('Yes, Neural Networks Have Grandmother Cells [OC]', "AboutAbout Data Science Association Data Science Board Directors DSA News Contact Membership ConferencesEvents20132013-11-12 Predictive Analytics Automated Intelligent Outlier Detection 2013-12-10 Sequencing Human Genome Personalized Medicine Bioinformatics 2014H12014-01-14 Adding Predictive Analytics BI Team Random Forest Decision Trees using R 2014-02-26 Druid Data Ingest Text Mining Python 2014-03-25 Sports Science Denver Nuggets Optimal Human Performance Health 2014-04-12 Data Science Forum Vail Ski Retreat 2014-05-28 Deep Learning Neural Networks Natural Language Processing 2014-06-25 Big Data Business Advantage GPUs FPGAs Real-time Ingest Processing 2014H22014-07-14 In-memory Computing Principles 2014-08-27 Bayesian Optimization B A-Z Testing Artificial Intelligence Debate 2014-09-17 Real-time Analytics Visualizing Data Motion 2014-10-09 Celebrate Data Science Cloud 2014-11-05 Spark Gotchas Anti-Patterns Julia Language 2014-12-03 LineageDB Architecture Big Data Analytics Data Quality 2015-01-21 Top 20 Data Quality Solutions Random Walks Scale Space Theory 2015-02-28 Machine Learning Contest Kick-off Data Science Presentations 2014 Conference Data Science Association Conference Planning Committee International Journal Data Science - Call Papers - Deadline July 1 2015 Next Issue Machine Learning Contest Award Ceremony - December 5 2015 Machine Learning Contest Kick-off - February 28 2015 ResourcesLibrary Committees Data Science News Podcasts Code Conduct Store Blogs hereHome Blogs michaelmalak's blog Yes Neural Networks Grandmother Cells Yes Neural Networks Grandmother Cells 11 May 2015 michaelmalak 0 Comments neural network portions image come Wikipedia age-old debate neural networks artificial biological whether grandmother cell neuron cell node somewhere net activated one's grandmother viewed assuming biological vision scenario computer vision application biological neural networks jury answer leaning toward side artificial neural networks Google answer you'll almost always come across admonishment avoid grandmother cells neural networks beginners neural networks advice easily misunderstood precisely grandmother cells avoided internal nodes reason internal nodes supposed latent variables e intermediate properties like big eyes big teeth internal node already recognizing grandma indication overfitting perhaps neural network created large amount training data search space artificial neural networks whose job classify images one classes grandma grandmother cell namely output layer That's simply get output classifier artificial neural network michaelmalak's blog Log register post comments Member login Username Password Become member Request new password feed prevent automated spam submissions leave field empty Upcoming Show full calendar YeSQL Battling NoSQL Hype Cycle Postgres 2015 05 28 - 6 00pm 10 00pm IQSS Dataverse Community Meeting 2015 - Harvard University 2015 06 09 - 7 00am 2015 06 11 - 9 00pm In-Memory Computing Summit 2015 2015 06 29 - 7 30am 2015 06 30 - 9 30pm International Conference Machine Learning ICML 2015 07 06 - 7 15am 2015 07 11 - 9 15pm Knowledge Discovery Data Mining KDD 2015 08 10 - 7 15am 2015 08 13 - 9 15pm Copyright 2015 Data Science Association back"),
('Nuances of eXtreme Gradient Boosting', "Home Fellowship Startup ML Blog Next ML Home Fellowship Startup ML Blog Next ML May 11 2015 Arshak Navruzyan Notes eXtreme Gradient Boosting May 11 2015 Arshak Navruzyan Trevor Hastie hypothesizes slide 17 terms model accuracy Boosting Random Forest Bagging Single TreeThanks XGBoost it's become computationally feasible test Hastie's claim boosting's superiority pretty much size shape dataset darling many recent Kaggle competitions XGBoost library fast general purpose gradient boosting parallelized using OpenMP also provides AllReduce-based distributed version implements generalized linear model gradient boosted regression trees quick laptop-scale experiment covertype see boosting indeed achieve slightly better prediction accuracy 2 5 error vs Random Forest benchmark 2 6 Covertype classic dataset used multi-class non-linear algorithm benchmarking data consists 54 variables 581 012 observations 7 classes minority View Notebook crib notes building model Setting shrinkage parameter low eta 0 1 yield significant improvement model generalization however must offset computationally rounds boosting capture residuals XGBoost expects multi-class problems 0 zero indexed dataset comes class labels 1-7 need shift class labels -1 Watchlist affect model training simply way assess prediction error independent sample training process isn't used training categorical variable particular dataset already booleaned wasn't case One-Hot-Encoding must applied categoricals see Orchestra jl XGBoost grow 1 tree per class per round boosting case 7 x 500 3 500 model complexity grows prediction time increase Thus model able score 110k observations 18 seconds may fine use cases perhaps slow adtech Shrinkage parameter eta affect prediction score leaf nodes shape tree whereas parameters gamma max_depth min_child_weight max_delta_step subsample colsample_bytree influence tree shape left eta 0 1 eta 0 5 right colsample_bytree 1 colsample_bytree 0 5 ResourcesFinally great resources want continue learning boosting Tianqi Chen - Introduction Boosted TreesTrevor Hastie - Gradient Boosting Machine LearningMikhail Golovnya - Advances Gradient Boosting Power Post-ProcessingHastie Tibshirani Friedman - Elements Statistical Learning Chapter 10 Hastie Tibshirani - Stanford Statistical Learning Course May 11 2015 Arshak Navruzyan Arshak Navruzyan Avoiding Extensive Feature Engineering IoT deeplearning 30 layers speech recognition GoogleIO2015 http co 9kqn4BmhUm 26 minutes ago Quora answer applications data science machine learning media entertainment industry http co QMjNe4bB1H 2 days ago Next ML machinelearning conference fintech co-organized bloomberg http co yfvSSiZkma Aug 6 SF http co iaSRnFTIjx 4 days ago RT karpathy New epic blog post Unreasonable Effectiveness Recurrent Neural Networks http co OKkaxvIGnS immense fun write 6 days ago RT DominoDataLab See Hedgefunds use Domino build models faster collaborate better track results datascience https co ma1AXz4qNo week ago Quora answer companies data science internships http co RZC9wiz5N8 week ago RT toysoldier182 One favorite entrepreneurship programs world startupml datascience MachineLearning startup week ago fusion good governance exponential technology humanitarian endeavor http co EnbZrNYzup TimDraper dfjsteve week ago Notes eXtreme Gradient Boosting http co sXjHKCzg0v machinelearning BigData 2 weeks ago Quora answer software skills every data scientist know http co Gmnv6LZU36 2 weeks ago Get news updates First Name Last Name Email Address Sign Thank Back Top Resources Get Involved Events Get Touch Terms 2014-2015 Startup ML LLC"),
('A Fixed-Size Encoding Method for Variable-Length Sequences', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1505 01504 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context cs NE prev next new recent 1505Change browse cs cs CL cs LG References CitationsNASA ADS Bookmark Computer Science Neural Evolutionary Computing Title Fixed-Size Encoding Method Variable-Length Sequences Application Neural Network Language Models Authors ShiLiang Zhang Hui Jiang MingBin Xu JunFeng Hou LiRong Dai Submitted 6 May 2015 Abstract paper propose new fixed-size ordinally-forgetting encoding FOFE method almost uniquely encode variable-length sequence words fixed-size representation FOFE model word order sequence using simple ordinally-forgetting mechanism according positions words work applied FOFE feedforward neural network language models FNN-LMs Experimental results shown without using recurrent feedbacks FOFE based FNN-LMs significantly outperform standard fixed-input FNN-LMs also popular RNN-LMs Comments 6 pages 4 figures technical report Subjects Neural Evolutionary Computing cs NE Computation Language cs CL Learning cs LG Cite arXiv 1505 01504 cs NE arXiv 1505 01504v1 cs NE version Submission history Hui Jiang view email v1 Wed 6 May 2015 20 14 25 GMT 256kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('Image Question Answering: A Visual Semantic Embedding Model and a New Dataset', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1505 02074 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context cs LG prev next new recent 1505Change browse cs cs AI cs CL cs CV References CitationsNASA ADS Bookmark Computer Science Learning Title Image Question Answering Visual Semantic Embedding Model New Dataset Authors Mengye Ren Ryan Kiros Richard Zemel Submitted 8 May 2015 Abstract work aims address problem image-based question-answering QA new models datasets work propose use recurrent neural networks visual semantic embeddings without intermediate stages object detection image segmentation model performs 1 8 times better recently published results dataset Another main contribution automatic question generation algorithm converts currently available image description dataset QA form resulting 10 times bigger dataset evenly distributed answers Comments 10 pages 7 figures 5 tables review Deep Learning Workshop paper ICML 2015 Subjects Learning cs LG Artificial Intelligence cs AI Computation Language cs CL Computer Vision Pattern Recognition cs CV Cite arXiv 1505 02074 cs LG arXiv 1505 02074v1 cs LG version Submission history Mengye Ren view email v1 Fri 8 May 2015 15 59 44 GMT 2151kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('Particle swarm optimization in F#', "Daniel Slater's Blog thoughts rants ramblings programming machine learning development Monday May 4 2015 Particle Swarm Optimization F particle swarm optimization good Lets say function takes array inputs produces single output objective want find input results lowest possible output function function differentiable could compute gradient follow back reach objective hypothetical scenario it's range inputs quite small could search entire input space find best output scenario you've reached point particle swarm optimization referred PSO might good idea would use One example might function runs simulation aircraft flies inputs wing span tail length different kinds engines used etc output much fuel aircraft uses simulation perhaps could functions simulating economic model PSO many particles execute function set parameters execution particle look result got attempt find better input parameters looking best result attempting move towards also best result particles way particles search input space way prioritize successful areas Also many particles searching different sets parameters hope avoid problems local minima velocity inertia_weights previous_velocity constant1 random1 local_best_parameters-parameters time-1 constant2 random2 global_best_parameters-previous_parameters parameters time parameters time-1 velocity time inertia_weight array number likely 0 1 used dampen previous velocity stop particles heading middle nowhere Also often velocity constrained maximum value constant1 constant2 used control much particle affected local global bests Normally set 1 2 intuitive way understanding thing works imagining collection bugs one particle bug trying find food area can't see food sense smell tells close loss function position parameters function Imagine bugs spread area moving around iteration reconsider direction going Based smell consider current position better best position ever isn't adjust direction move towards bugs also social creatures hive mind also consider current position compare best position bugs ever reached adjust move towards time bug getting better better personal best global best also slowly improving Imagine swarming across area gradually grouping tightly around successful points Hope makes things less confusing use genetic algorithm overlap genetic algorithms naturally fit discreet variables e g bool vs PSO fits better continuous either modified take full range think working continuous variable PSO work efficient finding good solutions probably massively variable depending data one annoying situations machine learning try things see looks like it's working F best thing F easy write easy read parallel code lightweight syntax PSO achieve massive benefits running parallel Also day job mostly working C Java Python F makes nice change pace Show code wanted write code pure functional style means aiming functions deterministic output solely dependent input side effects state PSO algorithm lot variables constant 1 constant 2 inertia weight etc set class variables wanted pure passed arguments functions use would mean lot messy long functions made args type would hold variables type Args inertia_weight list float particles int iterations int max_velocity float success_threshold float c1 float c2 float member inertia_weight inertia_weight member particles defaultArg particles 10 member iterations defaultArg iterations 100 member max_velocity defaultArg max_velocity 10 0 member success_threshold defaultArg success_threshold 0 0001 member c1 defaultArg c1 2 0 member c2 defaultArg c2 2 0 Next particle class Note went lists type variables didn't want use Arrays immutable Sequences functionally pure choice reality quite lot slower lists Also structuredFormatDisplay used print nicely StructuredFormatDisplay Particle Parameters type Particle val Parameters list float val Velocity list float val Local_best list float val Local_best_loss float new parameters velocity local_best local_best_loss Parameters parameters Velocity velocity Local_best local_best Local_best_loss local_best_loss main algorythm let update_particle args Args loss_func particle Particle global_best list float Particle let limit_velocity velocity max_velocity velocity gt List map fun x - gt x gt 0 0 min x max_velocity else min x -max_velocity let random new System Random let r1 random NextDouble args c1 let r2 random NextDouble args c2 let velocity List map2 fun w v - gt w v args inertia_weight particle Velocity multiple last velocity inertia weight List map2 fun l p - gt r1 l-p particle Local_best particle Parameters get attraction local best List map2 fun g p - gt r2 g-p global_best particle Parameters get attration global best gt List map3 fun x y z - gt x y z add result 3 calculations together gt limit_velocity lt args max_velocity limit velocity max let new_parameters particle Parameters velocity gt List map2 fun x y - gt x y let new_loss loss_func new_parameters new_loss lt particle Local_best_loss Particle new_parameters velocity new_parameters new_loss else Particle new_parameters velocity particle Local_best particle Local_best_loss PSO need run full collection particles like let update_particles args Args particles list Particle global_best_params list float global_best_loss float loss_func list Particle list float float let updated_particles particles List map fun x - update_particle args loss_func x global_best_params let best_from_this_iteration updated_particles List minBy fun x - x Local_best_loss global_best_loss best_from_this_iteration Local_best_loss updated_particles global_best_params global_best_loss else updated_particles best_from_this_iteration Local_best best_from_this_iteration Local_best_loss method running loop stop condition reached let rec private run_until_stop_condition args Args particles list Particle global_best_params list float global_best_loss float loss_func iterations_to_run let stop_condition args Args iterations global_best_loss iterations 0 global_best_loss args success_threshold let new_particles new_global_best_params new_global_best_loss update_particles args particles global_best_params global_best_loss loss_func let new_iterations_to_run iterations_to_run - 1 stop_condition args iterations_to_run new_global_best_loss new_global_best_params new_global_best_loss new_particles else run_until_stop_condition args new_particles new_global_best_params new_global_best_loss loss_func new_iterations_to_run example usage looks like let target_point 23 0 54 0 let loss_func x list float x List map2 fun x y - sin 1 0 x-sin 1 0 y target_point List sum abs let random new System Random let initial_weights seq true yield random NextDouble 1000 0 random NextDouble 1000 0 let args Args inertia_weight 0 8 0 8 particles 10 success_threshold 0 01 iterations 10 let global_best_params global_best_loss particles execute args loss_func initial_weights link full code git hub next post look options parallelizing code Posted Daniel Slater 6 12 Email ThisBlogThis Share TwitterShare FacebookShare Pinterest comments Post Comment Newer Post Home Subscribe Post Comments Atom Daniel Slater View complete profile Subscribe Posts Atom Posts Comments Atom Comments Blog Archive 2015 3 May 3 Programming programming computer game Net run Particle Swarm Optimization F part 2 - Paralle Particle Swarm Optimization F"),
('Weekly collection of the best news and resources on Artificial Intelligence and Machine Learning - in your inbox.', "Latest Issue Archives Search Search Search Toggle Menu Subscribe weekly collection best news resources Artificial Intelligence Machine Learning free Subscribe Email Email Subscribe spam ever email address ever used Artificial Intelligence Weekly Issue 3 May 28th 2015 Welcome 3rd issue AI Weekly week introducing new section Topic week since interesting posts discussions around Recurrent Neural Networks RNNs week I'll start new section deep-dive RNNs enjoy Thanks subscribing David News Ethics artificial intelligence 4 leading researchers express opinions development AI related challenges Stuart Russell Take stand AI weapons Sabine Hauert Shape debate don't shy Russ Altman Distribute AI benefits fairly Manuela Veloso Embrace robot human world nature com Google step closer developing machines human-like intelligence Google said work algorithms would encode thoughts bring common sense computers devices theguardian com Computational aesthetics algorithm spots beauty humans overlook team University Turin developed algorithm evaluate beauty image help us find best content platforms Flickr technologyreview com Also news week Stuart Russell expresses concerns AI calling researchers look beyond goal merely making artificial intelligence powerful team Queen Mary University UK come deep neural network framework beats humans sketch recognition Keen software house makes AI learn create hierarchy goals Jason Koebler Vice wonders super-intelligent agents would ignore us humans rather destroying us thoughts personalization services developed Nara Logics new app lets depressed teens chat celebrities dead friends Learning Key lessons machine learning Paper written Machine Learning researchers practitioners 12 key lessons datasets feature selection overfitting model construction washington edu Free ebook - Introduction Genetic Algorithms Genetic algorithms designed solve practical problems mimicking natural selection process free ebook good introduction field mpg de Learning see moving Complex neural networks often trained using millions hand-labelled images real-life though brains don't get trained way rather looking moving objects scenes paper explores possibility performing advanced training using egomotion supervisory signal feature learning arxiv org Topic week Recurrent Neural Networks seem rage week Enjoy little deep-dive topic unreasonable effectiveness Recurrent Neural Networks Great post recurrent neural networks bunch fun examples RNNs used generate Shakespearean essays Linux code github io unreasonable effectiveness Character-level Language Models Yoav Goldberg replies post introduces unsmoothed maximum-likelihood character level language models demonstrates effectiveness generating natural language outputs ipython org RNN - Handwriting generation demo Great online demonstration built Alex Graves uses Long Short-term Memory recurrent neural networks synthesize handwritten words result realistic test different handwriting styles toronto edu Training Recurrent Neural Networks thesis Thorough overview RNNs spare time willing dive specifics PhD thesis Ilya Sutskever utoronto ca Software tools code eight bits enough Deep Neural Networks Interesting article levels precision needed deep neural networks work appropriately petewarden com Brains Neurons brain makes decisions Researchers EPFL developed first biologically plausible model handle non-Markovian decision-making deepstuff org Major milestone reached Brain Preservation Challenge team Heidelberg come technique preserve mouse brain ultrastructure level preservation make possible perform electron microscopic imaging entire connectome Huge milestone brainpreservation org inspiration Hacking epic NHL goal celebration Using real-time machine learning detect goals occur live NHL games Fran ois Maillet developed nice hack create hue light show Philips hue lights Awesome idea great post blog implementation details francoismaillet com newsletter weekly collection best news resources Artificial Intelligence Machine Learning find worthwhile please forward friends colleagues share favorite network Share Twitter Share Linkedin Share Google Suggestions comments welcome reply email Thanks Previous issue May 21st 2015 3 3 Subscribe Email Email Subscribe spam ever email address ever used Artificial Intelligence Weekly 2015 Artificial Intelligence Weekly Terms Use Privacy Policy"),
('Talk | IEPY: Una plataforma para Extraccin de Informacin en Python', "Skip navigation UploadSign inSearch Loading Close Yeah keep Undo Close video unavailable Watch QueueTV QueueWatch QueueTV Queue Remove allDisconnect Loading Watch Queue TV Queue __count__ __total__ Franco Luque IEPY Una plataforma para Extracci n de Informaci n en Python LCC - FCEIA - UNR SubscribeSubscribedUnsubscribe51 Subscription preferences Loading Loading Working Add Want watch later Sign add video playlist Sign Share Report Need report video Sign report inappropriate content Sign Transcript 226 views 6 Like video Sign make opinion count Sign 7 0 Don't like video Sign make opinion count Sign 1 Loading Loading Transcript interactive transcript could loaded Loading Loading Rating available video rented feature available right Please try later Published Feb 20 2015XII Jornadas de Ciencias de la Computaci n Octubre de 2014Abstract En esta charla presentar IEPY Information Extraction Python una plataforma de software libre para el desarrollo de aplicaciones de Extracci n de Informaci n La Extracci n de Informaci n trata el problema del lisis de documentos de texto estructurado para encontrar y estructurar determinada informaci n de inter En IEPY la informaci n se estructura en entidades como personas lugares y fechas y relaciones entre las entidades como la presencia de una persona en determinado lugar y o momento En una etapa de preprocesamiento IEPY analiza los documentos autom ticamente segment ndolos en palabras y oraciones haciendo etiquetado POS Luego IEPY realiza el reconocimiento de entidades Named Entity Recognition o NER Todos estos pasos se pueden realizar utilizando herramientas existentes como NLTK o el Stanford Tagger y NER o se pueden definir y utilizar m dulos propios Finalmente en el m dulo principal IEPY realiza la extracci n de relaciones Relationship Extraction o RE Para ello primero se encuentran todas las posibles evidencias de relaciones en los documentos Luego IEPY entra en el ciclo principal en el que se requiere la intervenci n del usuario para responder preguntas acerca de las evidencias Luego de cada ronda de preguntas se entrenan clasificadores autom ticos que permiten extraer nuevas relaciones y generar nuevas preguntas hasta que el usuario decide terminar de responder http www fceia unr edu ar lcc jcc 2014 Category Education License Standard YouTube License Show Show less Loading Autoplay autoplay enabled suggested video automatically play next Next Rafael Nam Imaginando - Duration 28 51 LCC - FCEIA - UNR 52 views 28 51 Play nextPlay Taih Pire M todos de navegaci n basada en visi n para robots m viles - Duration 1 01 43 LCC - FCEIA - UNR 159 views 1 01 43 Play nextPlay Gabriel Tolosa Recuperaci n de Informaci n de Gran Escala - Duration 1 10 23 LCC - FCEIA - UNR 25 views 1 10 23 Play nextPlay Maximiliano Ca ellas Bitcoin en profundidad Un viaje criptogr fico - Duration 1 09 54 LCC - FCEIA - UNR 212 views 1 09 54 Play nextPlay Carlos Ches evar Inteligencia Artificial y Gobierno Electr nico - Duration 43 22 LCC - FCEIA - UNR 42 views 43 22 Play nextPlay Marcelo Arroyo lisis est tico con clang-llvm - Duration 56 25 LCC - FCEIA - UNR 57 views 56 25 Play nextPlay Alejandro D az-Caro Lambda c lculo m dulo isomorfismos de tipos - Duration 1 06 46 LCC - FCEIA - UNR 309 views 1 06 46 Play nextPlay Gustavo Betarte Certificaci n de algoritmos criptogr ficos constant-time - Duration 1 02 45 LCC - FCEIA - UNR 35 views 1 02 45 Play nextPlay Jornadas de Ciencias de la Computaci n 2014 Entrega de diplomas - Duration 14 05 LCC - FCEIA - UNR 87 views 14 05 Play nextPlay Jornadas de Ciencias de la Computaci n 2014 Acto de apertura - Duration 9 41 LCC - FCEIA - UNR 56 views 9 41 Play nextPlay Inteligencia Artificial y procesamiento de lenguaje natural DevHangout con dav009 - Duration 55 14 DevAcademy la 1 447 views 55 14 Play nextPlay JCC 2014 Para qu estudiar ciencias de la computaci n - Duration 50 27 LCC - FCEIA - UNR 419 views 50 27 Play nextPlay Training custom model Stanford NER - Duration 4 32 Thuy Pham 50 views 4 32 Play nextPlay NLTK vs Twitter Voyage Linguistic Frontiers - Max Thayer - Duration 20 24 PyCon Canada 890 views 20 24 Play nextPlay Stanford Tagger integration - Duration 1 06 diadoTorbalan 376 views 1 06 Play nextPlay Procesamiento de Lenguaje Natural Post Mejorando la - Duration 1 22 45 Pablo Rigazzi 1 22 45 Play nextPlay Miner de Texto - Duration 8 07 JOS LUIS CHAVEZ MONTERO 51 views 8 07 Play nextPlay Language English Country India Safety History Help Loading Loading Loading Press Copyright Creators Advertise Developers YouTube Terms Privacy Policy Safety Send feedback Try something new Loading Working Sign add Watch Later Add Loading playlists"),
("Google Deepmind's Gorila - parallel actor / critic reinforcement learning - David Silver [pdf]", 'PDF-1 5 15 0 obj'),
('RNN controlled creatures evolved to survive killer planks in JS. Life is tough.', 'planks otoro net hardmaru'),
('Interview with the #2 Kaggler on his competition plans, iteration cycles and other tips', "free hunch sport data science Archives 2015 Jan Feb Mar Apr May 2014 Jan Feb Mar Apr May Jun Jul Aug Sep Nov Dec 2013 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 2012 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 2011 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 2010 Apr May Jun Jul Aug Sep Oct Nov Dec Kaggle News Tutorials Winners' Interviews Profiling Top Kagglers KazAnova Currently 2 World Posted May 7 2015 Triskelion Kagglers Master Kagglers top 10 Kagglers people consistently win Kaggle competitions series try find got top leaderboards First KazAnova -- Marios Michailidis -- current number 2 nearly 300 000 data scientists Marios PhD student machine learning UCL senior data scientist dunnhumby organizer Kaggle competitions 'Shopper Challenge' 'Product Launch Challenge' Marios Michailidis Q start Kaggle competitions wanted new challenge field learn Grand Masters software development machine learning algorithms also led creating GUI credit scoring analytics name KazAnova- nick name frequently use Kaggle keep reminding passion field started could go far Kaggle seemed right place learn experts first plan action working new competition First understand problem metric tested on- key as-soon-as possible create reliable cross-validation process best would resemble leaderboard test set general allow explore many different algorithms approaches knowing impact could yield Understand importance different algorithmic families see maximize intensity linear non-linear type problem Try many different approaches techniques given problem seize possible angles terms algorithms 'selection hyper parameter optimization feature engineering missing values' treatment- treat elements hyper parameters final solution iteration cycle look like Sacrifice couple submissions beginning contest understand importance different algorithms -- save energy last 100 meters following process multiple models Select model recursive loop following steps Transform data scaling log x 1 values treat missing values PCA none Optimize hyper parameters model feature engineering model generate new features features' selection model reducing Redo previous steps optimum parameters likely changed slightly Save hold-out predictions used later meta-modelling Check consistency CV scores leaderboard problematic re-assess cross-validation process re-do steps Create partnerships Ideally look people likely taken different approaches Historically contrast looking friends people learn people fun - much winning Find good way ensemble Screen KazAnova Analytics GUI favorite machine learning algorithms like Gradient Boosting Tree methods general Scalable Non-linear capture deep interactions Less prone outliers favorite machine learning libraries Scikit forests XGBoost GBM LibLinear linear models Weka Encog neural nets Lasagne nets although learnt recently RankLib functions like NDCG Image slides 'Introduction Boosted Trees' Tianqi Chen XGBoost approach hyper-tuning parameters manually tried use something like Gridsearch feel learn algorithms work way manually time feel something it's machine 40 competitions I've found get top 90 best hyper parameters first try manual approach paid approach solid CV final submission selection LB fit regards CV try best resemble tested many situations random split would work example Acquire valued shoppers' challenge mainly tested different products offers available train set made CV always try predict 1 offer using rest offers could resemble test leaderboard better random split final selection normally go best Leaderboard submission best CV submission case happy collision select something different possible respectable CV result case lucky prayer god overfitting secret 3rd submission words wins competitions Understand problem well Discipline well-thorough documented approach follow religiously defines modelling process framework cross-validate select models avoids fitting requires lot discipline Allow room try problem-specific things new approaches within framework hours put access right tools Make key partnerships Ensembling favourite Kaggle competition Acquire valued shoppers' challenge won relevant team also honour collaborate Gert Jacobusse final private leaderboard Valued Shopper's Challenge least favourite Kaggle competition experience DecMeg BCI channel-wave type competitions big data domain specific found hard even make CV working properly plus quite bad Hopefully improve field machine learning excited like recommender systems considered separate field broad spectrum techniques use field specific able understand customer likes challenging rewarding machine learning researchers study study Steffen Rendle Leo Breiman Alexander Karatzoglou Michael Jahrer Andreas scher Machine Learning Data Mining Group National Taiwan University Jun Wang Philip Treleaven tell us something last algorithm hand-coded LibFM Avazu competition believed could work well particular problem could make work well LibFFM apparently important domain expertise solving data science problems competitions really important fact employed recommendation science field kind work within team helped win Acquire valued Shoppers challenge However think go long way following standard approaches even don't know field well also beauty machine learning fact algorithms significant job consider creative trick find approach multiple ensemble meta-stacking use term course univariate model tuning save models' outputs make meta-models univariate models selections times end different ensembles Meta models Sometimes go third Meta model Meta models inputs Meta-Meta model currently using data science work competing Kaggle help Classified Kaggle help optimizing methods learning new skills meet nice people passion date new tools generally stay touch what's going field opinion trade-off high model complexity training test runtime big discussion principle guess trade-off needs understanding better models necessarily interpretable ones complex model likely score better necessarily less stable dangerous guess optimum solution somewhere middle e g ensemble 100 models Naive Bayes best 8 finishes KazAnova get better Kaggle competitions guess helped lot Seeing previous solutions end-of-competition threads Participate Kaggle forums Learn tools Read papers websites machine learning tutorials Optimize processes use sparse matrices cut unnecessary steps write efficient code Save everything I've done reuse improve E g keep separate folder competition I've completed Dedicate time reduce video games Collaborate others found following resources useful benchmark Paul Duan Amazon competition first ever attempt Python general modelling framework competition Python code achieve 0 90 AUC logistic regression Miroslaw Horbal create pairwise interactions cross-validation text analysis benchmark Abhishek Beating benchmark StumbleUpon Evergreen Challenge XGBoost benchmark Higgs Boson competition Bing Xu Tinrtgu's FTRL Logistic model Avazu Beat benchmark less 1MB memory Data science Bowl tutorial image classification IPython Notebook Tutorial H2O R deep learning benchmark Arno Candel Africa Soil competition Lasagne nolearn tutorial Otto competition admin Andrew Ng's Coursera course Machine learning University Utah Machine learning slides Wikipedia Google partnerships important achieving good results Sometimes cannot measure impact one competition learn others may applicable future I've lucky made good fun collaborations far learnt especially Gert I've learnt model ensembling feature engineering Fourier transforms Triskelion Phil Culliton Vowpal Wabbit TVS avoid over-fitting Bluefool Domcastro 1st derivatives BART Mike Python Giulio competition management Lasagne Bio Marios Michailidis Senior Data Scientist dunnhumby part-time PhD machine learning University College London UCL focus improving recommender systems worked marketing credit sectors UK Market led many analytics projects various themes including Acquisition Retention Uplift fraud detection portfolio optimization spare time created KazAnova GUI credit scoring 100 made Java Marios loves competing Kaggle learning new machine learning tricks told us create something good ML community soon Triskelion Hendrik Jacob van Veen Kaggle username Triskelion write MLWave use Kaggle learn machine learning predictive modeling Kaggle News Tutorials Winners' Interviews free hunch kaggle's blog covering sport data science follow us twitter facebook 2015 kaggle inc rss"),
('Actually Useful Accepted ICML Papers (Arxiv Links)', 'Skip content Sign Sign repository Explore Features Enterprise Blog Watch 12 Star 70 Fork 17 KirkHadley icml2015_papers Code Issues Pull requests Pulse Graphs HTTPS clone URL Subversion checkout URL clone HTTPS Subversion Download ZIP Links ICML 2015 papers available arxiv 2 commits 1 branch 0 releases 1 contributor Python 100 0 Python branch master Switch branches tags Branches Tags master Nothing show Nothing show icml2015_papers tidying latest commit 1be1f524f6 KirkHadley authored May 10 2015 Permalink Failed load latest commit information README md tidying May 10 2015 create_markdown py first batch links May 10 2015 scraper py first batch links May 10 2015 README md ICML 2015 Papers ICML released utterly useless unlinked list papers accepted 2015 conference hacked scraper got links papers available arxiv list includes 60 papers accepted Please advise papers may missed due title quirks etc likely update publicly available papers arxiv Approval Voting Incentives Crowdsourcing 1406 3852 low variance consistent test relative - arXiv Spectral Clustering via Power Method -- Provably - arXiv 1312 4564 Adaptive Stochastic Alternating Direction - arXiv Lower Bound Optimization Finite Sums Learning Word Representations Hierarchical Sparse Learning Transferable Features Deep Adaptation transferable features deep neural networks Relationship Sum-Product Networks - arXiv 1505 00526 Explicit Sampling Dependent Spectral Stochastic PCA SVD Algorithm Exponential Learning Local Invariant Mahalanobis Distances 1501 03273 Classification Low Rank Missing Data Telling cause effect deterministic linear dynamical High Dimensional Bayesian Optimisation Bandits via 1504 03991 Theory Dual-sparse Regularized - arXiv General Analysis Convergence ADMM Stochastic Primal-Dual Coordinate Method Regularized Spectral MLE Top- K Rank Aggregation Pairwise Exploring Algorithmic Limits Matrix Rank Minimization Batch Normalization Accelerating Deep Network Training Distributed Estimation Generalized Matrix Rank Efficient 1402 5876 Manifold Gaussian Processes Regression Online Regret Bounds Undiscounted Continuous - arXiv Fundamental Incompatibility Hamiltonian Monte Faster Rates Frank-Wolfe Method Strongly Online Tracking Learning Discriminative Saliency Map Statistical Perspective Randomized Sketching 1411 3224 TD 0 function approximation - arXiv Learning Parametric-Output HMMs Two Aliased States Latent Gaussian Processes Distribution Estimation Variational inference sparse spectrum Gaussian process Stochastic Dual Coordinate Ascent Adaptive Probabilities JUMP-Means Small-Variance Asymptotics Markov Jump 1211 0358 Deep Gaussian Processes - arXiv Fast Bilingual Distributed Representations without Word Cascading Bandits Random Coordinate Descent Methods Minimizing Counterfactual Risk Minimization Learning Logged Linear Dynamical System Model Text Unsupervised Learning Video Representations using MADE Masked Autoencoder Distribution Estimation Large-scale Log-determinant Computation Differentially Private Bayesian Optimization Rademacher Observations Private Data Boosting Bayesian empirical Bayesian forests Ladder Reliable Leaderboard Machine Learning Enabling scalable stochastic gradient-based inference Reified Context Models Learning Fast-Mixing Models Structured Prediction 1406 6947 Deep Learning Multi-View Representation 1406 7443 Efficient Learning Large-Scale Combinatorial 1406 4311 Sparse Estimation Swept - arXiv Unsupervised Domain Adaptation Backpropagation Markov Chain Monte Carlo Variational Inference Power Randomization Distributed Submodular Non-Gaussian Discriminative Factor Models via Max Nested Sequential Monte Carlo Methods 1402 1389 Distributed Variational Inference Sparse 1402 1412 Variational Inference Sparse Gaussian Rebuilding Factorized Information Criterion Asymptotically 1311 0776 Composition Theorem Differential Privacy Strongly Adaptive Online Learning 1411 0860 CUR Algorithm Partially Observed Matrices Scaling-up Empirical Risk Minimization Optimization Towards Learning Theory Causation DRAW Recurrent Neural Network Image Generation Distributed Gaussian Processes 1302 2684 Tensor Approach Learning Mixed - arXiv Consistent Estimation Dynamic Multi-layer Networks 1405 3229 Rate Convergence Error Bounds Convex Learning Multiple Tasks Structure - arXiv 1304 5610 Tight Performance Bounds Approximate Approximate Modified Policy Iteration Long Short-Term Memory Tree Structures Predictive Entropy Search Bayesian Optimization Generative Moment Matching Networks Deep Learning Limited Numerical Precision Teaching Deep Convolutional Neural Networks Play Go Kernel Interpolation Scalable Structured Gaussian 1407 2538 Learning Deep Structured Models - arXiv Personalized PageRank Solution Paths Scalable Variational Inference Log-supermodular Models Variational Inference Gaussian Process Modulated Probabilistic Backpropagation Scalable Learning Trust Region Policy Optimization 1410 5518 Symmetric Asymmetric LSHs Inner Adding vs Averaging Distributed Primal-Dual Optimization Feature-Budgeted Random Forest Show Attend Tell Neural Image Caption Generation Learning Search Better Teacher Gated Feedback Recurrent Neural Networks 1502 03671 Phrase-based Image Captioning - arXiv Gradient-based Hyperparameter Optimization 1406 1901 Subsampling Methods Persistent Homology Binary Embedding Fundamental Limits Fast Algorithm Scalable Bayesian Optimization Using Deep Neural Networks Scalable Nonparametric Bayesian Inference Point Deep Unsupervised Learning using Nonequilibrium Compressing Neural Networks Hashing Trick - arXiv Optimal Adaptive Algorithms Online Boosting 1411 1134 Global Convergence Stochastic Gradient 1504 06785 Complete Dictionary Recovery Sphere PASSCoDe Parallel ASynchronous Stochastic dual Co Optimizing Neural Networks Kronecker-factored Novelty Detection Multi-Instance Multi-Label - arXiv 1212 4663 Concentration Measure Inequalities PU Learning Matrix Completion Distributed Proximal Method Composite Convex Posterior Sampling Stochastic Gradient Monte Carlo Inference Partially Observed Multitype Branching Status API Training Shop Blog 2015 GitHub Inc Terms Privacy Security Contact Something went wrong request Please try'),
('Genetic Programming in Python, with a scikit-learn inspired API', 'Skip content Sign Sign repository Explore Features Enterprise Blog Watch 12 Star 171 Fork 8 trevorstephens gplearn Code Issues Pull requests Pulse Graphs HTTPS clone URL Subversion checkout URL clone HTTPS Subversion Download ZIP Genetic Programming Python scikit-learn inspired API http gplearn readthedocs org 53 commits 1 branch 1 release 1 contributor Python 99 1 Python branch master Switch branches tags Branches Tags master Nothing show 0 1 0 Nothing show gplearn release 0 1 0 latest commit c1e7c348fc trevorstephens authored May 5 2015 Permalink Failed load latest commit information continuous_integration add dependency versions travis Apr 13 2015 doc add examples notebook May 3 2015 gplearn release 0 1 0 May 5 2015 coveragerc initialize docs Apr 20 2015 gitignore py3 verbose output gitignore updates IDE Mar 28 2015 landscape yaml ignore scikit-learn utils landscape coveralls Mar 28 2015 travis yml add dependency versions travis Apr 14 2015 LICENSE Initial commit Mar 25 2015 MANIFEST updates release May 4 2015 README rst updates release May 5 2015 setup py updates release May 5 2015 README rst Welcome gplearn gplearn implements Genetic Programming Python scikit-learn inspired compatible API Genetic Programming GP used perform wide variety tasks gplearn purposefully constrained solving symbolic regression problems motivated scikit-learn ethos powerful estimators straight-forward implement Symbolic regression machine learning technique aims identify underlying mathematical expression best describes relationship begins building population naive random formulas represent relationship known independent variables dependent variable targets order predict new data successive generation programs evolved one came selecting fittest individuals population undergo genetic operations gplearn retains familiar scikit-learn fit predict API works existing scikit-learn pipeline grid search modules package attempts squeeze lot functionality scikit-learn-style API lot parameters tweak reading documentation make relevant ones clear problem gplearn currently supports regression SymbolicRegressor well transformation automated feature engineering SymbolicTransformer designed support regression problems also work binary classification Future versions package expand class support complicated multi-target classification problems much planned gplearn built scikit-learn fairly recent copy 0 15 2 required installation come across issues running installing package please submit bug report hope get excellent results using gplearn please drop line blog used Status API Training Shop Blog 2015 GitHub Inc Terms Privacy Security Contact Something went wrong request Please try'),
('[1503.04881] Long Short-Term Memory Over Tree Structures', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1503 04881 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF PostScript formats Current browse context cs CL prev next new recent 1503Change browse cs cs LG cs NE References CitationsNASA ADS DBLP - CS Bibliography listing bibtex Xiaodan Zhu Parinaz Sobhani Hongyu Guo Bookmark Computer Science Computation Language Title Long Short-Term Memory Tree Structures Authors Xiaodan Zhu Parinaz Sobhani Hongyu Guo Submitted 16 Mar 2015 Abstract chain-structured long short-term memory LSTM showed effective wide range problems speech recognition machine translation paper propose extend tree structures memory cell reflect history memories multiple child cells multiple descendant cells recursive process call model S-LSTM provides principled way considering long-distance interaction hierarchies e g language image parse structures leverage models semantic composition understand meaning text fundamental problem natural language understanding show outperforms state-of-the-art recursive model replacing composition layers S-LSTM memory blocks also show utilizing given structures helpful achieving performance better without considering structures Comments February 6th 2015 work submitted International Conference Machine Learning ICML Subjects Computation Language cs CL Learning cs LG Neural Evolutionary Computing cs NE Cite arXiv 1503 04881 cs CL arXiv 1503 04881v1 cs CL version Submission history Xiaodan Zhu view email v1 Mon 16 Mar 2015 23 59 02 GMT 242kb authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('Zero-bias autoencoders and the benefits of co-adapting features', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org stat arXiv 1402 3337 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context stat ML prev next new recent 1402Change browse cs cs CV cs LG cs NE stat References CitationsNASA ADS Bookmark Statistics Machine Learning Title Zero-bias autoencoders benefits co-adapting features Authors Kishore Konda Roland Memisevic David Krueger Submitted 13 Feb 2014 v1 last revised 8 Apr 2015 version v5 Abstract Regularized training autoencoder typically results hidden unit biases take large negative values show negative biases natural result using hidden layer whose responsibility represent input data act selection mechanism ensures sparsity representation show negative biases impede learning data distributions whose intrinsic dimensionality high also propose new activation function decouples two roles hidden layer allows us learn representations data high intrinsic dimensionality standard autoencoders typically fail Since decoupled activation function acts like implicit regularizer model trained minimizing reconstruction error training data without requiring additional regularization Subjects Machine Learning stat ML Computer Vision Pattern Recognition cs CV Learning cs LG Neural Evolutionary Computing cs NE Cite arXiv 1402 3337 stat ML arXiv 1402 3337v5 stat ML version Submission history Kishore Konda view email v1 Thu 13 Feb 2014 23 37 39 GMT 2105kb D v2 Mon 10 Nov 2014 21 39 48 GMT 2328kb D v3 Sat 20 Dec 2014 02 07 47 GMT 1879kb D v4 Sat 28 Feb 2015 01 15 33 GMT 1687kb D v5 Wed 8 Apr 2015 14 51 11 GMT 1686kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('[1505.00387] Highway Networks', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1505 00387 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context cs LG prev next new recent 1505Change browse cs cs NE References CitationsNASA ADS Bookmark Computer Science Learning Title Highway Networks Authors Rupesh Kumar Srivastava Klaus Greff J rgen Schmidhuber Submitted 3 May 2015 Abstract plenty theoretical empirical evidence depth neural networks crucial ingredient success However network training becomes difficult increasing depth training deep networks remains open problem extended abstract introduce new architecture designed ease gradient-based training deep networks refer networks architecture highway networks since allow unimpeded information flow across several layers information highways architecture characterized use gating units learn regulate flow information network Highway networks hundreds layers trained directly using stochastic gradient descent variety activation functions opening possibility studying extremely deep efficient architectures Comments Extended Abstract 6 pages 2 figures Subjects Learning cs LG Neural Evolutionary Computing cs NE MSC classes 68T01 ACM classes 2 6 G 1 6 Cite arXiv 1505 00387 cs LG arXiv 1505 00387v1 cs LG version Submission history Rupesh Kumar Srivastava view email v1 Sun 3 May 2015 01 56 57 GMT 311kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
("'Robots will not be flawless, and the best future of human-robot partnerships will lie not in a race for who is more moral but in a symbiosis that lets each of the partners do what they do best, with the other available as a reality check.'", "Error 403 We're sorry could fulfill request teaching-robots-to-behave-ethically server invalid request received browser may caused malfunctioning proxy server browser privacy software technical support key 671b-09fd-1756-6707 use key fix problem unable fix problem please contact diana footnote1 com sure provide technical support key shown"),
('Julia, the scripting language of the future. Really.', "PolyStat Ecole Polytechnique de Montreal Statistics Machine Learning Data Mining Graduate Students Blog Thursday May 7 2015 programming machine learning speed Julia Machine learning really exploring models moving around modifying code Indeed machine learning level abstraction developers expect quite higher c c would easily allow exploration new models new solutions want able start write code level abstraction close possible straight math equations explains popularity languages like R Matlab languages allow user write code think equations tensors instead thinking containers types references problem scripting languages extremely slow shown following graph taken www julialang org unit value 1 table value C execution time numbers truly show part picture think look around net quickly find none numbers really controversial Recently Python gained huge popularity world machine learning Python indeed great fairly high level programming language huge community NumPy provides awesome Matlab-like tensor manipulation typed fast operations implemented native languages C C Fortran Python however really big limitations Indeed main implementation language uses straight bytecode interpreter offers way restrict dynamic typing allow optimization pretty slow although still much faster R Matlab reliance old non-thread-safe C code forced developers CPython implement known Global Interpreter Lock Basically means single Python interpreter process one thread executing bytecode time means non IO based shared memory multithreading confused multiprocessing impossible greatly limiting options parallelism Python Enter Julia Julia JIT Time compiled language meaning instead simply interpreting bytecode subroutines compiled native language slightly advance allows following executions functions loop much quicker would interpreted pass fact code actually compiled instead interpreted also allows fir static analysis optimisation made code exemple removing code visible end effects dead code propagating constants etc Julia also optional static typing allowing even greater static analysis optimization Shared memory parallelism still developed Julia currently working implementation thread safe demonstration version Julia Linux code still needs produced order Julia support shared memory parallelism mainstream platforms extremely interesting feature Julia built incredibly easy interface C C Python Note say incredibly easy it's exaggeration examples code integration observed C Python Julia also fully supports IPython style online notebooks like also beautiful plotting library Gadfly aside able use Matplotlib really easily even though Julia still bit young fully considered production code extremely promissing project would watch really closely Posted Jules Gagnon-Marchand 4 18 PM Email ThisBlogThis Share TwitterShare FacebookShare Pinterest 2 comments Vahid Partovi NiaMay 7 2015 5 00 PMThanks Jules liked post heard first Julia Mostafa visiting us weeks Sounds interesting follow follow Julia evolution ReplyDeleteMina MiMay 8 2015 1 47 PMinteresting least know thing exists ReplyDeleteAdd commentLoad Newer Post Older Post Home Subscribe Post Comments Atom Contributors Hanene Jemai Amir Homaie Mouloud Belbahri fatemeh farnia mahroo vahidpour Omar Sabounji Alexandre Arsenault Niloofar Ayati Jules Gagnon-Marchand Sajjad Ghaemi mahdi zolnouri Damoon Robatian Elham Karimi Sheida Shams Farnoush Farhadi Vahid Partovi Nia Shaima Tilouche Mina Mi Blog Archive 2015 23 May 17 Statistical Shapes Analysis Distance de Hamming Big Data Nonnegative Matrix Factorization NMF Name GPU server Quadcopter machine learning Black white PCA Principle Component Analysis Outliers Text Messages Catastrophes without network Performance GPU vs CPU Optimization models wood procurement activitie programming machine learning speed Jul teach machines Probability non-measurablity Forestogram Parallel coordinates April 4 March 2 Picture Window template Template images merrymoonmary Powered Blogger"),
('Neural Nets with Caffe Utilizing the GPU', 'joy data turning data insightful knowledge business personal curiosity Search Main menu Skip primary content donations imprint Post navigation Previous Neural Nets Caffe Utilizing GPU Posted 2015 05 09 Raffael Vogler Caffe open-source deep learning framework originally created Yangqing Jia allows leverage GPU training neural networks opposed deep learning frameworks like Theano Torch program algorithms instead specify network means configuration files Obviously approach less time consuming programming everything also forces stay within boundaries framework course Practically though won matter time framework Caffe provides quite powerful continuously advanced subject article composition multi-layer feed-forward network model trained based data Otto Group Product Classification Challenge Kaggle ll also take look applying model new data eventually ll see visualize network graph trained weights won explain details would bloat text beyond bearable scale Also like straightforward code says thousand words Instead check IPython Notebook programmatical details focus describing concepts tripping stones encountered Setting likely caffe yet installed system yes good recommend working EC2 instance allowing GPU-processing f x g2 2xlarge instance instructions work EC2 look Guide EC2 Command Line setting caffe prerequisits work GPU Powered DeepLearning NVIDIA DIGITS EC2 playing around Caffe also recommend installing IPython Notebook instance instructions ll find Defining Model Meta-Parameters Training model application requires least three configuration files format configuration files follows interface description language called protocol buffers supeficially resembles JSON significantly different actually supposed replace use cases data document needs validateable means custom schema like one Caffe serializable training need one prototxt-file keeping meta-parameters config prototxt training model another defining graph network model_train_test prototxt connecting layers acyclical directed fashion Note data flows bottom top regards order layers specified example network composed five layers data layer one TRAINing one TESTing inner product layer weights rectified linear units hidden layer inner product layer weights II output layer Soft Max classification soft max layer giving loss accuracy layer see network improves training following excerpt model_train_test prototxt shows layers 4 5A layer name ip2 type InnerProduct bottom ip1 top ip2 inner_product_param num_output 9 weight_filler type xavier bias_filler type constant value 0 layer name accuracy type Accuracy bottom ip2 bottom label top accuracy include phase TEST 12345678910111213141516171819202122232425262728 layer name ip2 type InnerProduct bottom ip1 top ip2 inner_product_param num_output 9 weight_filler type xavier bias_filler type constant value 0 layer name accuracy type Accuracy bottom ip2 bottom label top accuracy include phase TEST third prototxt-file model_prod prototxt specifies network used applying case mostly congruent specification training lacks data layers read data data source production Soft Max layer won yield loss value classification probabilities Also accuracy layer gone Note also beginning specify input dimensions expected 1 93 1 1 certainly confusing four dimensions referred input_dim order defines explicit context specified Supported Data Sources one first mental obstacle overcome trying get started Caffe simple providing caffe executable CSV let way Practically not-image data three options LMDB Lightning Memory-Mapped Database LevelDB HDF5 format HDF5 probably easiest use b c simply store data sets files using HDF5 format LMDB LevelDB databases ll go protocol size data set stored HDF5 limited memory discarded choice LMDB LevelDB rather arbitrary LMDB seemed powerful faster mature judging sources skimmed LevelDB seems actively maintained judging GitHub repo also larger Google stackoverflow footprint Blobs Datums Caffe internally works data structure called blobs used pass data forward gradients backward four dimensional array whose four dimensions referred N batch_size channels height width relevant us b c ll shape cases structure store LMDB feeded directly Caffe shape straight-forward images batch 64 images defined 100 200 RGB-pixels would end array shaped 64 3 200 100 batch 64 feature vectors length 93 blob shape 64 93 1 1 Load Data LMDB see individual cases feature vectors stored Datum objects Integer valued features stored byte string data float valued features float_data beginning made mistake assign float valued features data caused model learn anything storing Datum LMDB serialize object byte string representation Bottom Line Getting grip Caffe surprisingly non-linear experience means entry point continuous learning path lead good understanding system information required something useful Caffe distributed onto many different tutorial sections source code GitHub IPython notebooks forum threads took time compose tutorial accompanying code following maxim summarize learned text would liked read beginning think Caffe bright future ahead provided grow horizontally adding new features also vertically refactoring improving user experience definitely great tool high performance deep learning case want image processing convolutional neural networks recommend take look NVIDIA DIGITS offers comfortable GUI purpose original article published www joyofdata de entry posted Machine Learning tagged Kaggle Machine Learning Raffael Vogler Bookmark permalink Leave Reply Cancel reply email address published Required fields marked Name Email Website Visual Text may use HTML tags attributes href title abbr title acronym title b blockquote cite cite code class title data-url del datetime em q cite strike strong pre class title data-url span class title data-url Copy paste code ruheny Leave field empty Recent CommentsAnton Getting Started Pentaho BI Server 5 Mondrian SaikuRaffael Vogler Interactive Heatmaps Google Maps API v3Salman Altuwariki Interactive Heatmaps Google Maps API v3Raffael Vogler GPU Powered DeepLearning NVIDIA DIGITS EC2Luke GPU Powered DeepLearning NVIDIA DIGITS EC2 FEATURED ARTICLE Relation Word Order Compression Ratio Degree Structure Search content Database 9 Excel 12 Machine Learning 12 MOOC 4 Pentaho 10 R 28 Statistics 13 Tool 13 Visualization 13 Links QuickLaTeX R-bloggers visualizationsInteractive Heatmaps Google Maps API v3Tool Visualization Connections Agents Entities Context RedtubegateVisualization voting behaviour 17th German Bundestag Statistics Use JerusalemLife Death NUTSAnimated visualization growing network carpoolingsRegional ratio young women men EUComparison word frequency english literatureAnimated scatterplot two stock quotes chartsInsider deals DAX companies past ten yearsCorrelations quotes 30 German stocksIncrease Deaths Due Viral Hepatitis Germany 1998Frequency character combinations three languagesTagsHadoop heatmap hexbin igraph JavaScript JSON Kaggle Kettle PDI LibreOffice literature Machine Learning map MapReduce math melt Mondrian MongoDB mooc MySQL OCR Pentaho PHP pivot table PowerPivot PRD Python quantified politics R real-life reflections reshape RODBC Saiku scatter plot shell bash fish Shiny signal processing statistics stocks text mining troubleshooting Twitter VBA visualization XML officialimprint legal Proudly powered WordPress Insert edit link Close Enter destination URL URL Link Text Open link new window tab link existing content Search search term specified Showing recent items Search use arrow keys select item Cancel'),
('bagofwords.net, my naive bayesian subreddit classifier webapp', 'bagofwords beta characters left 1000 guess guess Note classifier could likely improved performance classifier data inherently limited class document relationship much weaker would document classification scenarios nothing keeping someone talking watercolors thread r guns example Interestingly stop-word removal significantly improves accuracy attempt integrating Porter de-stemming algorithm small negative effect'),
('[Paper] Facebook studying ways to identify people in different angles and positions in photos', "FacebookResearchOur Research Academic Programs PublicationsEventsBlogSearchPublicationBeyond Frontal Faces Improving Person Recognition Using Multiple CuesNing ZhangManohar PaluriYaniv TaigmanRob FergusLubomir BourdevInternational Conference Computer Vision Pattern Recognition CVPR 1 AbstractWe explore task recognizing peoples' identities photo albums unconstrained setting facilitate introduce new People Photo Albums PIPA dataset consisting 60000 instances 2000 individuals collected public Flickr photo albums half person images containing frontal face recognition task challenging due large variations pose clothing camera viewpoint image resolution illumination propose Pose Invariant PErson Recognition PIPER method accumulates cues poselet-level person recognizers trained deep convolutional networks discount pose variations combined face recognizer global recognizer Experiments three different settings confirm unconstrained setup PIPER significantly improves performance DeepFace one best face recognizers measured LFW dataset AI ResearchDownload PaperJoin UsDo want help billion people world connect share View Open PositionsCodeLearn open source tools technologies challenging scaling experiences Go Facebook CodeFacebook 2015AboutCareersPrivacyCookiesTermsHelp"),
('Visual Turing Test Dataset: VQA Visual Question Answering', "Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1505 00468 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context cs CL prev next new recent 1505Change browse cs cs CV References CitationsNASA ADS Bookmark Computer Science Computation Language Title VQA Visual Question Answering Authors Stanislaw Antol Aishwarya Agrawal Jiasen Lu Margaret Mitchell Dhruv Batra C Lawrence Zitnick Devi Parikh Submitted 3 May 2015 Abstract propose task free-form open-ended Visual Question Answering VQA Given image natural language question image task provide accurate natural language answer Mirroring many real-world scenarios helping visually impaired questions answers open-ended Visual questions selectively target different areas image including background details underlying context result system succeeds VQA typically needs detailed understanding image complex reasoning system producing generic image captions Moreover VQA amenable automatic evaluation since many open-ended answers contain words closed set answers provided multiple-choice format provide dataset containing 100 000's images questions discuss information provides Numerous baselines VQA provided compared human performance Subjects Computation Language cs CL Computer Vision Pattern Recognition cs CV Cite arXiv 1505 00468 cs CL arXiv 1505 00468v1 cs CL version Submission history C Lawrence Zitnick view email v1 Sun 3 May 2015 20 07 39 GMT 9324kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact"),
('[1505.01809] Language Models for Image Captioning: The Quirks and What Works', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1505 01809 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context cs CL prev next new recent 1505Change browse cs cs AI cs CV cs LG References CitationsNASA ADS Bookmark Computer Science Computation Language Title Language Models Image Captioning Quirks Works Authors Jacob Devlin Hao Cheng Hao Fang Saurabh Gupta Li Deng Xiaodong Geoffrey Zweig Margaret Mitchell Submitted 7 May 2015 Abstract Two recent approaches achieved state-of-the-art results image captioning first uses pipelined process set candidate words generated convolutional neural network CNN trained images maximum entropy language model used arrange words coherent sentence second uses penultimate activation layer CNN input recurrent neural network RNN generates caption sequence paper compare merits different language modeling approaches first time using state-of-the-art CNN input examine issues different approaches including linguistic irregularities caption repetition data set overlap combining key aspects RNN methods achieve new record performance benchmark COCO dataset Comments See http URL project information Subjects Computation Language cs CL Artificial Intelligence cs AI Computer Vision Pattern Recognition cs CV Learning cs LG Cite arXiv 1505 01809 cs CL arXiv 1505 01809v1 cs CL version Submission history Margaret Mitchell view email v1 Thu 7 May 2015 18 36 14 GMT 684kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('Getting Started with Deep Learning', "O'Reilly Ideas Learning Events Shop Feedback Log Log Artificial intelligence Getting Started Deep Learning walk popular open-source tools show process images Pete Warden January 1 2016 webcast currently available beta watch original recording Thanks patience Pete Warden Pete CTO Jetpac Inc startup focused analyzing billions public photos He's recipient NSF grant computer vision work worked image processing Apple five years published number popular open source data analysis projects O'Reilly books blogs https petewarden com petewarden Twitter Artificial intelligence build run first deep learning network Pete Warden Step-by-step instruction training neural network Artificial intelligence Untapped opportunities AI Beau Cronin AI's viable approaches lie outside organizational boundaries Google large Internet companies Artificial intelligence AI dueling definitions Beau Cronin understanding AI different Artificial intelligence search model modeling intelligence Beau Cronin True artificial intelligence require rich models incorporate real-world phenomena Sign first know matters Sign Us Company Work Us Customer Service Contact Us 2015 O'Reilly Media Inc Terms Service Privacy Policy Editorial Independence"),
('Using K-Means to cluster wine dataset', "datayo data mining Menu Search Skip content R Course Search Using K-Means cluster wine dataset Recently joined Cluster Analysis course coursera content first week Partitioning-Based Clustering Methods learned cluster algorithms based distance K-Means K-Medians K-Modes would like turn learn practice write post excercise course post use K-Means clustering wine data set found one excellent posts K-Mean r-statistics website Meet data wine data set contains results chemical analysis wines grown specific area Italy Three types wine represented 178 samples results 13 chemical analyses recorded sample Type variable transformed categoric variable data wine package rattle head wine Type Alcohol Malic Ash Alcalinity Magnesium Phenols 1 1 14 23 1 71 2 43 15 6 127 2 80 2 1 13 20 1 78 2 14 11 2 100 2 65 3 1 13 16 2 36 2 67 18 6 101 2 80 4 1 14 37 1 95 2 50 16 8 113 3 85 5 1 13 24 2 59 2 87 21 0 118 2 80 6 1 14 20 1 76 2 45 15 2 112 3 27 Flavanoids Nonflavanoids Proanthocyanins Color Hue 1 3 06 0 28 2 29 5 64 1 04 2 2 76 0 26 1 28 4 38 1 05 3 3 24 0 30 2 81 5 68 1 03 4 3 49 0 24 2 18 7 80 0 86 5 2 69 0 39 1 82 4 32 1 04 6 3 39 0 34 1 97 6 75 1 05 Dilution Proline 1 3 92 1065 2 3 40 1050 3 3 17 1185 4 3 45 1480 5 2 93 735 6 2 85 1450 Explore Preprocessing Data Let's see structure wine data set str wine 'data frame' 178 obs 14 variables Type Factor w 3 levels 1 2 3 1 1 1 1 1 1 1 1 1 1 Alcohol num 14 2 13 2 13 2 14 4 13 2 Malic num 1 71 1 78 2 36 1 95 2 59 1 76 1 87 2 15 1 64 1 35 Ash num 2 43 2 14 2 67 2 5 2 87 2 45 2 45 2 61 2 17 2 27 Alcalinity num 15 6 11 2 18 6 16 8 21 15 2 14 6 17 6 14 16 Magnesium int 127 100 101 113 118 112 96 121 97 98 Phenols num 2 8 2 65 2 8 3 85 2 8 3 27 2 5 2 6 2 8 2 98 Flavanoids num 3 06 2 76 3 24 3 49 2 69 3 39 2 52 2 51 2 98 3 15 Nonflavanoids num 0 28 0 26 0 3 0 24 0 39 0 34 0 3 0 31 0 29 0 22 Proanthocyanins num 2 29 1 28 2 81 2 18 1 82 1 97 1 98 1 25 1 98 1 85 Color num 5 64 4 38 5 68 7 8 4 32 6 75 5 25 5 05 5 2 7 22 Hue num 1 04 1 05 1 03 0 86 1 04 1 05 1 02 1 06 1 08 1 01 Dilution num 3 92 3 4 3 17 3 45 2 93 2 85 3 58 3 58 2 85 3 55 Proline int 1065 1050 1185 1480 735 1450 1290 1295 1045 1045 Wine data set contains 1 categorical variables label 13 numerical variables numerical variables scaled use scale function scaling centering data assign training data data train - scale wine -1 Data already centered scaled summary data train Alcohol Malic Min -2 42739 Min -1 4290 1st Qu -0 78603 1st Qu -0 6569 Median 0 06083 Median -0 4219 Mean 0 00000 Mean 0 0000 3rd Qu 0 83378 3rd Qu 0 6679 Max 2 25341 Max 3 1004 Ash Alcalinity Min -3 66881 Min -2 663505 1st Qu -0 57051 1st Qu -0 687199 Median -0 02375 Median 0 001514 Mean 0 00000 Mean 0 000000 3rd Qu 0 69615 3rd Qu 0 600395 Max 3 14745 Max 3 145637 Magnesium Phenols Min -2 0824 Min -2 10132 1st Qu -0 8221 1st Qu -0 88298 Median -0 1219 Median 0 09569 Mean 0 0000 Mean 0 00000 3rd Qu 0 5082 3rd Qu 0 80672 Max 4 3591 Max 2 53237 Flavanoids Nonflavanoids Min -1 6912 Min -1 8630 1st Qu -0 8252 1st Qu -0 7381 Median 0 1059 Median -0 1756 Mean 0 0000 Mean 0 0000 3rd Qu 0 8467 3rd Qu 0 6078 Max 3 0542 Max 2 3956 Proanthocyanins Color Min -2 06321 Min -1 6297 1st Qu -0 59560 1st Qu -0 7929 Median -0 06272 Median -0 1588 Mean 0 00000 Mean 0 0000 3rd Qu 0 62741 3rd Qu 0 4926 Max 3 47527 Max 3 4258 Hue Dilution Min -2 08884 Min -1 8897 1st Qu -0 76540 1st Qu -0 9496 Median 0 03303 Median 0 2371 Mean 0 00000 Mean 0 0000 3rd Qu 0 71116 3rd Qu 0 7864 Max 3 29241 Max 1 9554 Proline Min -1 4890 1st Qu -0 7824 Median -0 2331 Mean 0 0000 3rd Qu 0 7561 Max 2 9631 Model Fitting fun part begins use NbClust function determine best number clusteres k K-Means nc - NbClust data train min nc 2 max nc 15 method kmeans barplot table nc Best n 1 xlab Numer Clusters ylab Number Criteria main Number Clusters Chosen 26 Criteria According graph find best number clusters 3 Beside NbClust function provides 30 indices determing number clusters proposes best clustering scheme draw sum square error SSE scree plot look bend elbow graph determine appropriate k wss - 0 1 15 wss - sum kmeans data train centers withinss plot 1 15 wss type b xlab Number Clusters ylab Within groups sum squares two methods suggest k 3 best choice us It's reasonsable take notice original data set also contains 3 classes Fit model fit wine data K-Means k 3 fit km - kmeans data train 3 interpret result fit km K-means clustering 3 clusters sizes 51 65 62 Cluster means Alcohol Malic Ash Alcalinity 1 0 1644436 0 8690954 0 1863726 0 5228924 2 -0 9234669 -0 3929331 -0 4931257 0 1701220 3 0 8328826 -0 3029551 0 3636801 -0 6084749 Magnesium Phenols Flavanoids Nonflavanoids 1 -0 07526047 -0 97657548 -1 21182921 0 72402116 2 -0 49032869 -0 07576891 0 02075402 -0 03343924 3 0 57596208 0 88274724 0 97506900 -0 56050853 Proanthocyanins Color Hue Dilution 1 -0 77751312 0 9388902 -1 1615122 -1 2887761 2 0 05810161 -0 8993770 0 4605046 0 2700025 3 0 57865427 0 1705823 0 4726504 0 7770551 Proline 1 -0 4059428 2 -0 7517257 3 1 1220202 Clustering vector 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 26 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 51 3 3 3 3 3 3 3 3 3 2 2 1 2 2 2 2 2 2 2 2 2 2 2 3 2 76 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 101 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 3 2 2 2 126 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 151 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 176 1 1 1 Within cluster sum squares cluster 1 326 3537 558 6971 385 6983 between_SS total_SS 44 8 Available components 1 cluster centers totss 4 withinss tot withinss betweenss 7 size iter ifault result shows information cluster means clustering vector sum square cluster available components Let's visualizations see data set clustered First use plotcluster function fpc package draw discriminant projection plot library fpc plotcluster data train fit km cluster see data clustered well collapse clusters Next draw parallel coordinates plot see variables contributed cluster library MASS parcoord data train fit km cluster extract insights graph suc black cluster contains wine low flavanoids value low proanthocyanins value low hue value green cluster contains wine dilution value higher wine red cluster Evaluation original data set wine also 3 classes reasonable compare classes 3 clusters fited K-Means confuseTable km - table wine Type fit km cluster confuseTable km 1 2 3 1 0 0 59 2 3 65 3 3 48 0 0 see 6 sample missed Let's use randIndex flexclust compare two parititions one data set one result clustering method library flexclust randIndex ct km ARI 0 897495 It's quite close 1 K-Means good model clustering wine data set References Choosing number cluster K-Means http stackoverflow com 15376462 1036500 K-means Clustering R Action http www r-statistics com 2013 08 k-means-clustering-from-r-in-action Color cluster output r http stackoverflow com questions 15386960 color-the-cluster-output-in-r Share TwitterFacebookGoogleLike Like Loading Related May 6 2015rain1024 Post navigation One thought Using K-Means cluster wine dataset Tuan Quyen Nguyen says Thanks nice post good demonstration K-means Imazine Type field dataset LikeLiked 1 person Reply May 26 2015 1 46 pm Leave Reply Cancel reply Enter comment Fill details click icon log Email required Address never made public Name required Website commenting using WordPress com account Log Change commenting using Twitter account Log Change commenting using Facebook account Log Change commenting using Google account Log Change Cancel Connecting Notify new comments via email Categories courses 6 r 6 way 2 researches 4 toolbox 7 Follow datayo WordPress com Recent Posts Mathematics Data Scientist Using C4 5 predict Diabetes Pima Indian Women RStudio IDE R Using K-Means cluster wine dataset Manipulate String Datetime R Recent Comments Tuan Quyen Nguyen Using K-Means cluster wine Read write data Parse JSON RI write blog frequen Quality postQuality post Data Mining ProcessQuality post write blog frequently Top Posts PagesUsing C4 5 predict Diabetes Pima Indian Women RStudio IDE R Using K-Means cluster wine dataset Mathematics Data Scientist Visulization clustering Facebook Ego Network R part 1 Read write data R Manipulate Data R Manipulate String Datetime R R Course Tagsbelow-standard social-mining visualizationArchives May 2015 5 April 2015 6 March 2015 8 May 2015 M W F Apr 123 45678910 11121314151617 18192021222324 25262728293031 Create free website blog WordPress com Sorbet Theme Follow Follow datayo Get every new post delivered Inbox Build website WordPress com d bloggers like"),
("Pushing the Frontiers of Computer Vision: A Q&A with Google's Christian Szegedy", "RE WORK Menu Home Us Events Workshops Contact Us Event Calendar RE WORK Blog Pushing Frontiers Computer Vision Q Google's Christian Szegedy Due inroads deep learning computer vision appears verge solved However current methods extremely data hungry getting high quality labelled data expensive cumbersome Instead letting humans hard work turn computers couch potatoes program figure visual world watching decades videos team Google set push frontiers computer vision giving affirmative answer question Christian Szegedy Senior Research Scientist Google working deep learning computer vision including image recognition object detection video analysis caught Christian ahead presentation Deep Learning Summit Boston month main types problems addressed deep learning space Deep learning applied successfully machine perception large scale data analysis Prime examples former kinds computer vision speech recognition music classification tasks major section computer vision literature last two years dedicated utilization learned deep convolutional network features large variety computer vision problems huge success Recurrent neural networks started revolutionize field machine translation text understanding well practical applications work sectors likely affected recent work focused various fundamental computer vision tasks image annotation object detection segmentation pose estimation laid ground-work lot computer vision systems used Google products example Inception network architectures core several vision-heavy Google services personal photo search image content face tagging social photos business detection recognition StreetView imagery Advances deep learning pave way future utilization visual signals easy efficient ubiquitous textual processing computers today developments expect see deep learning next 5 years Current deep learning algorithms neural networks far theoretically possible performance Today design vision networks 5-10 times cheaper use 15 times less parameters outperforming much expensive counterparts one year ago solely virtue improved network architectures better training methodologies convinced start deep learning algorithms become efficient able run cheap mobile devices even without extra hardware support prohibitive memory overhead advancements excite field inroads machine learning transform information technologies prominently way program computers slowly shift prescribing solve problems specifying let machines learn cope could even distill solution formal procedures akin current programs order truly get exciting developments come synergy currently disjoint areas marriage formal discrete methods fuzzy probabilistic approaches like deep neural networks Deep Learning Summit taking place Boston 26-27 May information register please visit event website Join conversation event hashtag reworkDL Big Data Neural Networks Machine Learning Deep Learning Algorithms Image Retrieval Pattern Recognition Deep Learning Hardware Computer Vision Latest Posts Using Spacesuit Tech Subtle Smart Clothing Deep Learning Data Analytics Actionable Intelligence Massive Scale Keeping Information Secure Importance IoT Infrastructure Detect Serious Heart Condition Using Smartphone Soofa Creating Smart Sustainable Social Spaces Connect Latest Blog Using Spacesuit Tech Subtle Smart Clothing Deep Learning Data Analytics Actionable Intelligence Massive Scale Keeping Information Secure Importance IoT Infrastructure Detect Serious Heart Condition Using Smartphone Soofa Creating Smart Sustainable Social Spaces RE WORK Twitter Starting afternoon Katie Rae ktrae Milek Smyk blstream fireside chat reworkIoT http co I6XuHokPgD RT arghonaie ReWorkiot sharing good news Arghon AIEngine Catch Collie's speech tomorrow 9 40 http co 27HcyqZ1hF RT seanlorenz Great talks robots IoT ReworkIoT Don't think 2 things belong together Read https co NIP9glQV Sociable Images Dan Taylor Heisenberg Media Jessica Bernard Craig Robinson Photography 2015 RE WORK X LTD 8160750 Website Digital Reflow"),
('The tensor renaissance in data science', "Menu Home Shop Video Training Books Radar Safari Books Online Conferences Courses Certificates oreilly com O'Reilly Radar RSS Feed Twitter Facebook Google Youtube Home Shop Video Training Books Radar Radar Animals Safari Books Online Conferences Courses Certificates Data Topics DataDesignEmerging TechIoTProgrammingWeb Ops PerformanceWeb Platform Help us test new look O Reilly Visit beta site Print Listen tensor renaissance data science O'Reilly Data Show Podcast Anima Anandkumar tensor decomposition techniques machine learning Ben Lorica bigdata Ben Lorica Comment May 7 2015 Comment Flash Player upgrade required may also download file Running time sitting UC Irvine Professor Anima Anandkumar Strata Hadoop World 2015 San Jose presentation wrote post urging data community build tensor decomposition libraries data science feedback ve gotten readers extremely positive latest episode O Reilly Data Show Podcast sat Anandkumar talk tensor decomposition machine learning data science program UC Irvine Modeling higher-order relationships natural question use tensors large matrices already challenging work Proponents quick point tensors model complex relationships Anandkumar explains Tensors higher order generalizations matrices matrices two-dimensional arrays consisting rows columns tensors multi-dimensional arrays instance picture tensors three-dimensional cube fact desk Rubik Cube sometimes use get better understanding think tensors One biggest use tensors representing higher order relationships want represent pair-wise relationships say co-occurrence every pair words set documents matrix suffices hand want learn probability range triplets words need tensor record relationships kinds higher order relationships important text also say social network analysis want learn immediate friends say friends friends friends someone Tensors whole represent much richer data structures matrices Subscribe O Reilly Data Show Podcast TuneIn iTunes SoundCloud RSS Recent progress tensor decomposition computation first encountered tensors math physics courses last years ve hearing machine learning circles past tensor computations deemed computationally expensive practical applications Anandkumar points better hardware recent breakthroughs ushered applications machine learning think first use tensors way back 1940s psychometrics journal Since diverse work tensors physics numerical analysis signal processing theoretical computer science However opinion one reasons think tensors perhaps fell little fashion fact processing tensors expensive much expensive matrices fact processing complexities grow exponentially order tensor one main reasons computers yet powerful tensors could handled efficiently However think seeing renaissance tensors explosion computational capabilities tensor operations highly parallelizable run cloud perfectly suited running GPUs since simple operations parallelized many cores thinking tensors theoretical computer science viewpoint many tensor problems NP-hard another reason tensors seen exotic objects hard analyze compared matrices people restricted matrices able prove lot nice properties hand research collaborators researchers field shown lot tensor-related problems machine learning hard encounter worst-case hard tensors machine learning applications would say main breakthrough makes analysis well manipulation tensors tractable many applications Feature learning deep neural networks Feature learning image courtesy Anima Anandkumar used permission course working analytic projects data scientists learn appreciate importance feature engineering machine learning pipelines recent trend use techniques like deep learning automatically learn good features applications span many domains Anandkumar highlights recent contributions tensor methods feature learning latest set results looking use tensors feature learning general concept idea feature learning look transformations input data classified accurately using simpler classifiers emerging area machine learning seen lot interest latest analysis ask tensors employed feature learning established learn recursively better features employing tensor decompositions repeatedly mimicking deep learning seen presented one early works deep learning workshop got lot interest useful feedback certainly want unify different kinds techniques instance another application ve looking use hierarchical graphical models also deep learning framework features extracted deep learning better detection multiple objects image deep learning currently focused benchmark data sets mostly one image one object whereas looking lot objects image efficiently learn also using fact objects tend co-occur images learn co-occurrences time exploit features extracted deep learning overall much better classification multiple objects listen entire interview SoundCloud player subscribe SoundCloud TuneIn iTunes See Anima Anandkumar presentations Strata Hadoop World 2015 San Jose complete video compilation Cropped image article category pages nosha Flickr used Creative Commons license tags Big Data Big Data Tools Pipelines data science data scientists datashow modeling O'Reilly Data Show Podcast tensor decomposition tensor libraries tensors Comment Get O Reilly Data Newsletter Stay informed Receive weekly insight industry insiders Featured Video Data science going - DJ Patil U government's first Chief Data Scientist looks future data science Strata Hadoop World 2015 San Jose Get Data Newsletter Stay informed Receive weekly insight industry insiders Featured Download Download free report free reports Recent Posts Real-world interfaces awkward playful stage finance disrupted Validating data models Kafka-based pipelines Four short links 28 May 2015 Protecting health open data management principles Recently Discussed Archives Archives Month May 2015 April 2015 March 2015 February 2015 January 2015 December 2014 November 2014 October 2014 September 2014 August 2014 July 2014 June 2014 May 2014 April 2014 March 2014 February 2014 January 2014 December 2013 November 2013 October 2013 September 2013 August 2013 July 2013 June 2013 May 2013 April 2013 March 2013 February 2013 January 2013 December 2012 November 2012 October 2012 September 2012 August 2012 July 2012 June 2012 May 2012 April 2012 March 2012 February 2012 January 2012 December 2011 November 2011 October 2011 September 2011 August 2011 July 2011 June 2011 May 2011 April 2011 March 2011 February 2011 January 2011 December 2010 November 2010 October 2010 September 2010 August 2010 July 2010 June 2010 May 2010 April 2010 March 2010 February 2010 January 2010 December 2009 November 2009 October 2009 September 2009 August 2009 July 2009 June 2009 May 2009 April 2009 March 2009 February 2009 January 2009 December 2008 November 2008 October 2008 September 2008 August 2008 July 2008 June 2008 May 2008 April 2008 March 2008 February 2008 January 2008 December 2007 November 2007 October 2007 September 2007 August 2007 July 2007 June 2007 May 2007 April 2007 March 2007 February 2007 January 2007 December 2006 November 2006 October 2006 September 2006 August 2006 July 2006 June 2006 May 2006 April 2006 March 2006 February 2006 January 2006 December 2005 November 2005 October 2005 September 2005 August 2005 July 2005 June 2005 May 2005 April 2005 March 2005 Archives Topic Data Design Emerging Tech IoT Programming Web Ops Performance Web Platform Archives Author Sinan Unur Aaron Sumner Adam DuVander Adam Flaherty Adam Messinger Adam Witwer Adrian Mendoza Alasdair Allan Alex Bordei Alex Bowyer Alex Iskold Alexander Macgillivray Alice Zheng Alistair Croll Allen Downey Allen Noren Allison Randal Ally MacDonald Alois Reitbauer Alysa Hutnik Amelia Bellamy-Royds Amr Awadallah Amy Heineike Amy Jollymore Amy Unruh Anant Jhingran Andreas Antonopoulos Andrew Collette Andrew Odewahn Andrew Savikas Andrew Shafer Andrew Baker Andy Fitzgerald Andy Kirk Andy Konwinski Andy Oram Angela Rufino Ann Spencer Ann Waldo Anna Smith Anne Gentle Anni Ylagan Ari Gesher Aria Haghighi Ariya Hidayat Arnold Robbins Artur Bergman Arun Gupta Audrey Watters Avi Bryant Barb Edson Barbara Bermes Baron Schwartz Barry Devlin Barry O'Reilly Beau Cronin Ben Christensen Ben Evans Ben Henick Ben Lorica Benjamin Hindman Bill Higgins Bill Lubanovic Bill McCoy Bonnie Feldman Bradley Voytek Brady Forrest Brandon Satrom Brett McLaughlin Brett Sandusky Brett Sheppard Brian Ahier Brian Anderson Brian Boyer brian d foy Brian d'Alessandro Brian Foster Brian Jepson Brian Kardell Brian MacDonald Brian O'Leary Brian Sawyer Brigitte Piniewski Bruce Stewart Carin Meier Carl Hewitt Carl Malamud Cathy O'Neil Chao Ray Feng Chiu-ki Chan Chris Cornutt Chris Meade Chris Vander Mey Chris Wiggins Christine Perey Ciara Byrne Cliff Miller Colt McAnlis Cornelia L vy-Bencheton Cory Doctorow Courtney Nash Dale Dougherty Dan Saffer Danese Cooper Darren Barefoot Dave Himrod Dave McClure Dave Zwieback David Beyer David Cranor David Elfi David Leinweber David Recordon David Sims David Stephenson DC Denison Deni Auclair Derek Jacoby Dinesh Subhraveti Dino Esposito DJ Patil Doug Finke Doug Hill Dr Venkat Subramaniam Drew Dara-Abrams Duncan Ross Dusty Phillips DW Wheeler Dylan Field E Vander Veer Edd Dumbill Edie Freedman Eli Goodman Elisabeth Robson Elizabeth Corcoran Ellen Friedman Elliott Hauser Elliotte Rusty Harold Emma Jane Westby Eoin Purcell Eric Redmond Eric Ries Ezra Haber Glenn Faye Williams Federico Castanedo Federico Lucifredi Fred Trotter Fred van den Bosch Gabe Zichermann Gavin Starks George Reese Gilad Rosner Glen Martin Greg Whisenant Gretchen Giles Gustavo Franco Gwen Shapira Hadley Wickham Hari K Gottipati Heather McCormack Helen Papagiannis Hew Wolff Howard Wen Hugh McGuire Ilya Grigorik Imran Ali J Paul Reed James Bridle James Turner Janaya Williams Jane Sarasohn-Kahn Jason Grigsby Jason Strimpel Jay Kreps Jay McGavren Jayant Shekar Jeevan Padiyar Jeff Gothelf Jeff Needham Jeffrey Carr Jeffrey Carr Jenn Webb Jennifer Pahlka Jeremy Freeman Jeremy Howard Jesper Andersen Jesse Anderson Jesse Robbins Jessica McKellar Jesus M Gonzalez-Barahona Jez Humble Jim Scott Jim Stogdill Jimmy Guterman Jo Prichard Joanne Molesky Jodee Rich Joe Procopio Johan Bergstr m John Adams John Allspaw John Battelle John Boxall John Feland John Foreman John Geraci John Graham-Cumming John King John Labovitz John Lindquist John Myles White John Piekos John Russell John Warren John Wilbanks Jon Bruner Jon Callas Jon Cowie Jon Roberts Jon Spinney Jon Udell Jonas Luster Jonathan Alexander Jonathan Reichental Ph D Jonathon Thurman Jono Bacon Joseph Hellerstein Joseph J Esposito Josh Lockhart Josh Simmons Joshua-Mich le Ross Joy Beatty Jud Valeski Julie Steele Justin Dombrowski Justin Hall Justo Hidalgo Karl Fogel Kassia Krozser Kat Meyer Kate Eltham Kate Pullinger Kathryn Barrett Kathy Sierra Kathy Walrath Katie Cunningham Katie Miller Keith Comito Keith Fahlgren Ken Yarmosh Kevin Shockey Kevin Sitto Kevin Smokler Khaled El Emam Kieren James-Lubin Kipp Bradford Kit Seeborg Kiyoto Tamura kmatsudaira Kurt Cagle Lara Swanson Laura Dawson Laura Klein Laurel Ruma Laurie Petrycki Leigh Dodds Liliana Bounegru Linda Stone Lisa Mann Liza Daly Lorna Jane Mitchell Lorne Lantz Luciano Ramalho Lucy Gray Lukas Biewald Mac Slocum Madhusudhan Konda Mandi Walls Manish Lachwani Marc Goodman Marc Hedlund Marie Beaugureau Marie Bjerede Mark Drapeau Mark Grover Mark Jeftovic Mark Lutz Mark Nelson Mark Pacelle Mark Sigal Marko Gargenta Martin Kalin Martin Kleppmann Mary Treseler Matt Garrish Matt Makai Matt Neuburg Matt Wood Matthew Burton Matthew Gast Matthew McCullough Matthew Russell Matthew Russell Max Kanat-Alexander Max Meyers Max Shron Meghan Athavale Meghan Blanchette Mehdi Daoudi Michael DeHaan Michael Driscoll Michael Ferrari Michael Freeman Michael Gold Michael Hunger Michael Jon Jensen Michael Lopp Michael McMillan Michael Scroggins Mike Amundsen Mike Barlow Mike Hendrickson Mike Honda Mike Loukides Mike Petrovich Mike Shatzkin Mitchell Hashimoto Naomi Robbins Nat Torkington Nate Osit Nathan Jepson Neal Ford Nicholas Tollervey Nick Bilton Nick Farina Nick Kolegraff Nick Lombardi Nick Ruffilo Nicolas Garcia Belmonte Nikolaj Nyholm O'Reilly Radar O'Reilly Strata Ohad Samet Osman Rashid Pablo Francisco Arrieta Gomez Paco Nathan Pamela Samuelson Paris Buttfield-Addison Patrick Mulder Patrick Reynolds Paul Kedrofsky Paul Spinrad Pete Hodgson Pete Warden Peter Arijs Peter Bennett Peter Cooper Peter Krautzberger Peter Laflin Peter Lewis Peter Meyers Philip Guo Philipp Janert Q Ethan McCallum Quinn Norton Rachel Roumeliotis Rael Dornfest Raffael Marty Rajat Bhargava Ramez Naam Randy Bias Raven Zachary Ray DiGiacomo Jr Renee DiResta Reynold Xin Richard Cook Richard Dallaway Richard Reese Richard Warburton Rob Tucker Robbie Allen Robert Kaye Robert Passarella Roberta Cairney Roger Chen Roger Magoulas Rogier DocWilco Mulhuijzen Ron Miller Roseanne Fallin Rune Madsen Russell J Dyer Ryan Bethencourt Ryan Neufeld Ryan Stewart Sam Newman Samuel Mullen Sanders Kleinfeld Sara Peyton Sara Winge Sarah Milstein Sarah Novotny Scott Jenson Scott Murray Scott Rich Scott Ruthfield Sean McGregor Sean O Sullivan bastien Pierre Semmy Purewal Seth Ladd Shahid Shah Shahin Farshchi Shai Almog Shannon Cutt Shyam Seshadri Silona Bonewald Simon Chan Simon Phipps Simon St Laurent Simon Wardley Spencer Critchley Stefan Thies Stephen Elston Stephen O'Grady Stephen O'Grady Steve Souders Steven Citron-Pousty Steven Shorrock Stoyan Stefanov Suzanne Axtell Tara Hunt Terrence Dorsey Terry Jones Tim Anderson Tim Busbice Tim Darling Tim O'Reilly Timothy M O'Brien Timothy McGovern Tish Shute Toby Inkster Todd Sattersten Tom Eisenmann Tom Steinberg Tony Quartarolo Trisha Gee Troy Topnik Tyler Bell Valeri Karpov Vandad Nahvandipoor Vanessa Fox Varun Nagaraj Cukierski William Mougayar William O'Connor Zigurd Mednieks CONTACT US Radar managing editor Jenn Webb Sign today receive special discounts product alerts news O'Reilly Privacy Policy View Sample Newsletter Twitter YouTube Slideshare Facebook Google RSS View RSS Feeds 2015 O'Reilly Media Inc 707 827-7019 800 889-8969 trademarks registered trademarks appearing oreilly com property respective owners O'Reilly O Reilly Radar Radar Contributors Academic Solutions Jobs Contacts Corporate Information Press Room Privacy Policy Terms Service Writing O Reilly Editorial Independence Community Authors Community Featured Users Forums Membership Newsletters O Reilly Answers RSS Feeds O Reilly Chimera beta Partner Sites makezine com makerfaire com craftzine com igniteshow com PayPal Developer Zone O Reilly Insights Forbes com Shop O'Reilly Customer Service Contact Us Shipping Information Ordering Payment Affiliate Program O Reilly Guarantee close Get O Reilly Data Newsletter Stay informed Receive weekly insight industry insiders"),
('Exposure to Diverse Information on Facebook', "FacebookResearchOur Research Academic Programs PublicationsEventsBlogSearchBlog PostExposure Diverse Information Facebookby Eytan BakshySolomon MessingLada Adamic 3 people increasingly turn social networks news civic information questions raised whether practice leads creation echo chambers people exposed information like-minded individuals 2 speculation focused whether algorithms used rank search results social media posts could create filter bubbles ideologically appealing content surfaced 3 Research conducted date however runs counter picture previous 2012 research paper concluded much information exposed share comes weak ties friends interact less often likely dissimilar us close friends 4 Separate research suggests individuals likely engage content contrary views presented along social information 5 latest research released today Science quantifies first time exactly much individuals could exposed ideologically diverse news information social media 1 found people friends claim opposing political ideology content peoples' News Feeds reflect diverse views News Feed surfaces content slightly aligned individual's ideology based person's actions Facebook friend content click consequential News Feed ranking terms much diverse content encounter Specifically find among self-report liberal conservative affiliation average 23 percent people's friends claim opposing political ideology hard news content people's friends share 29 5 percent cuts across ideological lines comes people see News Feed 28 5 percent hard news encountered cuts across ideological lines average 24 9 percent hard news content people actually clicked cross-cutting Sharing news Facebook six months July 2014 January 2015 7 million distinct Web links URLs shared people Facebook United States interested learning much people exposed hard news stories politics world affairs economy rather soft news stories entertainment celebrities sports whether information aligned primarily liberal conservative audiences trained support vector machine classifier uses first words articles linked URL shared Facebook allowed us identify 226 000 unique hard news articles shared least 100 times Next characterized content either conservative liberal Nine percent Facebook users United States self-report political affiliation profiles mapped common affiliations five-point scale ranging -2 liberal 2 conservative averaging affiliations shared article could measure ideological alignment story clear score measure ideological alignment audience shares article measure political bias slant article calculation described illustration Illustration ideological alignment content measured shared item average political affiliation individuals share example left example article shared five people three identified liberals one moderate one conservative producing average -2 5 average measure every story particular website domain see key differences well-known ideologically-aligned media sources FoxNews com aligned conservatives 80 HuffingtonPost com aligned liberals - 65 substantial polarization among hard news shared Facebook frequently shared links clearly aligned largely liberal conservative populations shown figure shows links particular hard news articles shared either primarily liberals alignment score close -1 conservatives alignment score close 1 rarely equally Using methods described turned toward measuring extent people could exposed ideologically diverse information Facebook Network structure ideology Homophily tendency similar individuals associate birds feather flock together robust social phenomenon Friends likely similar age educational attainment occupation geography surprising find holds true political affiliation Facebook see liberals conservatives tend connect people similar political affiliations based sample ego-networks depicted visualizations Example social networks liberal moderate conservative Points individuals' friends lines designate friendships However among report ideology average 23 percent friends report affiliation opposite side ideological spectrum figure see wide range network diversity Half users 9 33 percent friends opposing ideologies 25 percent less 9 percent remaining 25 percent 33 percent Percent friends opposing ideologies among liberals conservatives flow information Facebook diversity content people encounter depends friends also information friends share interaction people Facebook's News Feed News Feed shows content shared friends relevant content shown first Exactly stories people click depends often use Facebook far scroll News Feed choices make read Illustration exposure process consists three phases 1 news friends share Potential network 2 ranking time individuals take scroll governs see News Feeds Exposed 3 clicking actual article Selected much cross-cutting content people encounter depends friends information friends share people acquire information random others approximately 45 percent content liberals would exposed would cross cutting compared 40 percent conservatives course individuals encounter information random offline environments Internet much cross-cutting content shared friends appears News Feed People eligible see content shared friends News Feed since people don't enough time day see everything sort content show people relevant found 23 percent news shared liberals friends cross-cutting whereas seen News Feed 22 percent corresponds risk ratio 8 percent meaning people 8 percent less likely see countervailing articles shared friends compared likelihood seeing ideologically consistent articles shared friends hand 34 percent content shared conservatives ideologically cross-cutting versus 33 percent actually seen News Feed corresponding risk ratio 5 percent much cross-cutting content appears News Feed people actually click 22 percent content seen liberals cross-cutting found 20 percent content actually clicked cross-cutting meaning people 6 percent less likely click countervailing articles appeared News Feed compared likelihood clicking ideologically consistent articles appeared News Feed Conservatives saw 33 percent cross-cutting content News Feed actually clicked 29 percent corresponding risk ratio 17 percent diversity content 1 shared random others random 2 shared friends potential network 3 actually appearing peoples News Feeds exposed 4 clicked selected look people margin encountering hard news Facebook see evidence important role individual choice plays Take people whose friends shared least one consistent one cross-cutting story --- 99 percent exposed least one ideologically aligned item 96 percent encountered least one ideologically cross-cutting item News Feed looked clicked hard news content found 54 percent---more half---clicked ideologically cross-cutting content although less 87 percent clicked ideologically aligned content Proportion individuals least one cross-cutting aligned story 1 shared friends potential 2 actually appearing peoples News Feeds exposed 3 clicked selected Discussion showing people exposed substantial amount content friends opposing viewpoints findings contrast concerns people might list speak like-minded online 2 composition social networks important factor affecting mix content encountered social media individual choice also playing large role News Feed ranking smaller impact diversity information see side believe work beginning long line research people exposed consume media online information see paper available open access ScienceExpress Exposure Ideologically Diverse News Opinion E Bakshy Messing L Adamic Science References 1 Exposure Ideologically Diverse News Opinion E Bakshy Messing L Adamic Science 2015 2 C R Sunstein Republic com 2 0 Princeton University Press 2007 3 E Pariser Filter Bubble Internet Hiding Penguin Press London 2011 4 E Bakshy Rosenn C Marlow L Adamic Role Social Networks Information Diffusion Proceedings 21st international conference World Wide Web Pages 2012 5 Messing J Westwood Selective Exposure Age Social Media Endorsements Trump Partisan Source Affiliation Selecting News Online Communication Research 2012 Join UsDo want help billion people world connect share View Open PositionsCodeLearn open source tools technologies challenging scaling experiences Go Facebook CodeFacebook 2015AboutCareersPrivacyCookiesTermsHelp"),
('Interdisciplinary Data and Helping Humans Be Creative', 'Talking Machines Hello Media Episodes Contact Ways Listen Hello MediaEpisodesContactAboutWays Listen human conversation machine learning Episodes Hello MediaEpisodesContactAboutWays Listen May 07 2015 Interdisciplinary Data Helping Humans Creative May 07 2015 katherine gorman Episode 10 talk David Blei Columbia University talk work latent dirichlet allocation topic models PhD program data helping create Columbia exploring data inherently multidisciplinary learn Markov Chain Monte Carlo take listener question machine learning make humans creative May 07 2015 katherine gorman katherine gorman Think Privacy Starting Simple Machine EPISODES CONTACT WAYS LISTEN NEWSLETTER MEDIA PARTNERS'),
('Ask r/machinelearning: Is this CloudSight API just smoke and mirrors (humans) as opposed to deep learning as implied in their PR? Try it out yourself here.', "Menu Home CloudSight API Team Press Kit Log Revolutionizing way interact world around CloudSight Works CloudSight simple web API Send HTTP request image you'll receive description contents Check API docs detail Try Simply upload image type image URL see CloudSight API works Drag image hereor enter URL click Upload Payment Plans Pay go long-term contracts required Basic 49 Sign Image Requests 800 0 061 per request Pro 149 Sign Image Requests 3 000 0 050 per request Ultra 399 Sign Image Requests 10 000 0 040 per request Mega 1499 Sign Image Requests 50 000 0 030 per request Sign Free Log 2015 CloudSight Rights Reserved Terms Service Privacy Policy"),
('Neon, an open-source, Python-based, deep learning framework from Nervana Systems', "Skip content Sign Sign repository Explore Features Enterprise Blog Watch 120 Star 772 Fork 120 NervanaSystems neon Code Issues Pull requests Wiki Pulse Graphs HTTPS clone URL Subversion checkout URL clone HTTPS Subversion Download ZIP Nervana's python based Deep Learning Framework http neon nervanasys com 64 commits 1 branch 1 release 7 contributors Python 98 5 1 5 Python branch master Switch branches tags Branches Tags master Nothing show v0 8 1 Nothing show neon Ensure pip utilizes newest cudanet version latest commit 30a24370bb scttl authored May 25 2015 Permalink Failed load latest commit information bin Allow reuse example yamls integration testing May 12 2015 doc Expanded docs May 19 2015 examples Expanded docs May 19 2015 neon Improved handling tensor allocations using views May 25 2015 gitchangelog rc Initial public release neon May 3 2015 gitignore Initial public release neon May 4 2015 CONTRIBUTING rst Initial public release neon May 4 2015 ChangeLog Initial public release neon May 4 2015 LICENSE Initial public release neon May 4 2015 MANIFEST Initial public release neon May 4 2015 Makefile Improved handling tensor allocations using views May 25 2015 README md Move docker image links source install docs May 17 2015 requirements txt Initial public release neon May 4 2015 setup cfg Initial public release neon May 4 2015 setup py Ensure pip utilizes newest cudanet version May 25 2015 tox ini Initial public release neon May 4 2015 README md neon neon Nervana's Python based Deep Learning framework designed following functionality mind YAML easy model specification inspired pylearn2 Python easily adding models support many data formats Support commonly used models convnets MLPs RNNs LSTMs autoencoders RBMs Support common learning rules activation functions cost functions Comparing performance alternate numeric representations 32-bit floating point fp32 Deep Learning Support using spearmint hyperparameter optimization Swappable hardware backends write code deploy CPUs GPUs Nervana hardware Features unique neon include Tight integration nervanagpu kernels fp16 fp32 benchmarks Maxwell GPUs fastest implementations benchmark deep networks 4 3s macrobatch AlexNet Titan X Full run 1 GPU 45 hrs box fp16 AlexNet model accuracy fp32 Integration fork cudanet Alex Krizhevsky's cuda-convnet2 library Kepler GPU support Support distributed processor Nervana Engine deep learning use neon internally Nervana solve customers' problems across many domains hiring across several roles Apply Getting started Basic information get started Please consult full documentation information Installation Local install dependencies Cloud-based access email us account Docker images community provided Quick Install Mac OSX Linux machine enter following download install neon use train first multi-layer perceptron convolutional neural networks git clone https github com NervanaSystems neon git cd neon sudo make install install neon system-wide don't sufficient privileges would prefer isolated installation see either virtualenv based install take look community provided docker images several examples built-in neon examples directory user get started YAML format plain-text edited change various aspects model See ANNOTATED_EXAMPLE yaml definitions possible choices Running simple MNIST model CPU neon examples mlp mnist-small yaml Running Alexnet model GPU fp32 nervangpu requires Maxwell GPUs neon --gpu nervanagpu examples convnet i1k-alexnet-fp32 yaml cudanet works Kepler Maxwell GPUs neon --gpu cudanet examples convnet i1k-alexnet-fp32 yaml fp16 neon --gpu nervanagpu examples convnet i1k-alexnet-fp16 yaml Code organization backends --- implementation different hardware backends datasets --- support common datasets CIFAR-10 ImageNet MNIST etc diagnostics --- hooks measure timing numeric ranges hyperopt --- hooks hyperparameter optimization layers --- layer code models --- model code optimizers --- learning rules transforms --- activation cost functions metrics --- performance evaluation metrics Documentation complete documentation neon available useful starting points Using neon API Developing neon Issues bugs feature requests please Search open closed issues list see we're already working uncovered Check issue request already addressed Frequently Asked Questions FAQ File new issue submit new pull request code you'd like contribute Machine learning OPerations MOP Layer MOP abstraction layer Nervana's system software hardware includes Nervana Engine custom distributed processor deep learning MOP consists linear algebra operations required deep learning MOP operations currently exposed neon others distributed primitives exposed later versions well forthcoming Nervana libraries Defining models MOP-compliant manner guarantees run provided backends also provides way existing Deep Learning frameworks theano torch caffe interface Nervana Engine Upcoming libraries separate upcoming efforts following fronts Distributed models Automatic differentiation Integration Nervana Cloud License releasing neon nervanagpu open source Apache 2 0 License welcome contact us use cases Status API Training Shop Blog 2015 GitHub Inc Terms Privacy Security Contact Something went wrong request Please try"),
("ICML '15 Accepted Papers", 'International Conference Machine Learning 06 - 11 July 2015Lille Grand Palais Conference Accepted papers Invited Speakers Tutorials Complete Schedule Workshops Past Conferences Program Committee Info Sponsors Sponsors Authors Call Papers Call Workshops Style author instructions Submissions Reviewer Guidelines Awards Participants Registration Infos Book Room Venues Traveling Lille Visa information Call volunteers Scholarship applications Register Papers Workshops Book Room Accepted papers Stochastic Optimization Importance Sampling Regularized Loss Minimization Peilin Zhao Tong Zhang Approval Voting Incentives Crowdsourcing Nihar Shah Dengyong Zhou Yuval Peres low variance consistent test relative dependency Wacha Bounliphone Arthur Gretton Arthur Tenenhaus Aligned Subtree Kernel Weighted Graphs Lu Bai Luca Rossi Zhihong Zhang Edwin Hancock Spectral Clustering via Power Method Provably Christos Boutsidis Prabhanjan Kambadur Alex Gittens Information Geometry Minimum Description Length Networks Ke Sun Jun Wang Alexandros Kalousis Stephan Marchand-Maillet Efficient Training LDA GPU Mean-for-Mode Estimation Jean-Baptiste Tristan Joseph Tassarotti Guy Steele Adaptive Stochastic Alternating Direction Method Multipliers Peilin Zhao Jinwei Yang Tong Zhang Ping Li Lower Bound Optimization Finite Sums Alekh Agarwal Leon Bottou Learning Word Representations Hierarchical Sparse Coding Dani Yogatama Manaal Faruqui Chris Dyer Noah Smith Learning Transferable Features Deep Adaptation Networks Mingsheng Long Yue Cao Jianmin Wang Michael Jordan Robust partially observable Markov decision process Takayuki Osogami Relationship Sum-Product Networks Bayesian Networks Han Zhao Mazen Melibari Pascal Poupart Learning Corrupted Binary Labels via Class-Probability Estimation Aditya Menon Brendan Van Rooyen Cheng Soon Ong Bob Williamson Explicit Sampling Dependent Spectral Error Bound Column Subset Selection Tianbao Yang Lijun Zhang Rong Jin Shenghuo Zhu Stochastic PCA SVD Algorithm Exponential Convergence Rate Ohad Shamir Attribute Efficient Linear Regression Distribution-Dependent Sampling Doron Kukliansky Learning Local Invariant Mahalanobis Distances Ethan Fetaya Shimon Ullman Finding Linear Structure Large Datasets Scalable Canonical Correlation Analysis Zhuang Ma Yichao Lu Abstraction Selection Model-based Reinforcement Learning Nan Jiang Alex Kulesza Satinder Singh Surrogate Functions Maximizing Precision Top Purushottam Kar Harikrishna Narasimhan Prateek Jain Optimizing Non-decomposable Performance Measures Tale Two Classes Harikrishna Narasimhan Purushottam Kar Prateek Jain Coresets Nonparametric Estimation Case DP-Means Olivier Bachem Mario Lucic Andreas Krause Relative Exponential Weighing Algorithm Adversarial Utility-based Dueling Bandits Pratik Gajane Tanguy Urvoy Fabrice Cl rot Functional Subspace Clustering Application Time Series Mohammad Taha Bahadori David Kale Yingying Fan Yan Liu Accelerated Online Low Rank Tensor Learning Multivariate Spatiotemporal Streams Rose Yu Dehua Cheng Yan Liu Atomic Spatial Processes Sean Jewell Neil Spencer Alexandre Bouchard-C Classification Low Rank Missing Data Elad Hazan Roi Livni Yishay Mansour Dynamic Sensing Better Classification Acquisition Constraints Oran Richman Shie Mannor Modified Orthant-Wise Limited Memory Quasi-Newton Method Convergence Analysis Pinghua Gong Jieping Ye Telling cause effect deterministic linear dynamical systems Naji Shajarisales Dominik Janzing Bernhard Schoelkopf High Dimensional Bayesian Optimisation Bandits via Additive Models Kirthevasan Kandasamy Jeff Schneider Barnabas Poczos Theory Dual-sparse Regularized Randomized Reduction Tianbao Yang Lijun Zhang Rong Jin Shenghuo Zhu Generalization error bounds learning rank length document lists matter Ambuj Tewari Sougata Chaudhuri PeakSeg constrained optimal segmentation supervised penalty learning peak detection count data Toby Hocking Guillem Rigaill Mind duality gap safer rules Lasso Olivier Fercoq Alexandre Gramfort Joseph Salmon General Analysis Convergence ADMM Robert Nishihara Ben Recht Andrew Packard Michael Jordan Stochastic Primal-Dual Coordinate Method Regularized Empirical Risk Minimization Yuchen Zhang Xiao Lin DiSCO Distributed Optimization Self-Concordant Empirical Loss Yuchen Zhang Xiao Lin Spectral MLE Top-K Rank Aggregation Pairwise Comparisons Yuxin Chen Changho Suh Paired-Dual Learning Fast Training Latent Variable Hinge-Loss MRFs Stephen Bach Bert Huang Jordan Boyd-Graber Structural Maxent Models Corinna Cortes Vitaly Kuznetsov Mehryar Mohri Umar Syed Provable Generalized Tensor Spectral Method Uniform Hypergraph Partitioning Debarghya Ghoshdastidar Ambedkar Dukkipati Benefits Learning Strongly Convex Approximate Inference Ben London Bert Huang Lise Getoor Pushing Limits Affine Rank Minimization Adapting Probabilistic PCA Bo Xin David Wipf Budget Allocation Problem Multiple Advertisers Game Theoretic View Takanori Maehara Akihiro Yabe Ken-ichi Kawarabayashi Tracking Approximate Solutions Parameterized Optimization Problems Multi-Dimensional Parameter Domains Katharina Blechschmidt Joachim Giesen Soeren Laue Batch Normalization Accelerating Deep Network Training Reducing Internal Covariate Shift Sergey Ioffe Christian Szegedy Distributed Estimation Generalized Matrix Rank Efficient Algorithms Lower Bounds Yuchen Zhang Martin Wainwright Michael Jordan Landmarking Manifolds Gaussian Processes Dawen Liang John Paisley Markov Mixed Membership Models Aonan Zhang John Paisley Unified Framework Outlier-Robust PCA-like Algorithms Wenzhuo Yang Huan Xu Streaming Sparse Principal Component Analysis Wenzhuo Yang Huan Xu Divide Conquer Framework Distributed Graph Clustering Wenzhuo Yang Huan Xu Deep Rectifier Networks Achieve Linear Separability Preserve Distances Senjian Farid Boussaid Improved Regret Bounds Undiscounted Continuous Reinforcement Learning K Lakshmanan Ronald Ortner Daniil Ryabko Fundamental Incompatibility Scalable Hamiltonian Monte Carlo Naive Data Subsampling Michael Betancourt Faster Rates Frank-Wolfe Method Strongly-Convex Sets Dan Garber Elad Hazan Ordered Stick-Breaking Prior Sequential MCMC Inference Bayesian Nonparametric Models Mrinal Das Trapit Bansal Chiranjib Bhattacharyya Online Learning Eigenvectors Dan Garber Elad Hazan Tengyu Ma Unifying Framework Anytime Sparse Gaussian Process Regression Models Stochastic Variational Inference Big Data Trong Nghia Hoang Quang Minh Hoang Bryan Kian Hsiang Low Yinyang K-Means Drop-In Replacement Classic K-Means Consistent Speedup Yufei Ding Yue Zhao Xipeng Shen Madanlal Musuvathi Todd Mytkowicz Ordinal Mixed Membership Models Seppo Virtanen Mark Girolami Online Tracking Learning Discriminative Saliency Map Convolutional Neural Network Seunghoon Hong Tackgeun Suha Kwak Bohyung Han Fast Kronecker Inference Gaussian Processes non-Gaussian Likelihoods Seth Flaxman Andrew Wilson Daniel Neill Hannes Nickisch Alex Smola Statistical Algorithmic Perspectives Randomized Sketching Ordinary Least-Squares Garvesh Raskutti Michael Mahoney TD 0 function approximation Concentration bounds centered variant exponential convergence Nathaniel Korda Prashanth La Learning Parametric-Output HMMs Two Aliased States Roi Weiss Boaz Nadler Latent Gaussian Processes Distribution Estimation Multivariate Categorical Data Yarin Gal Yutian Chen Zoubin Ghahramani Improving Gaussian Process Sparse Spectrum Approximation Representing Uncertainty Frequency Inputs Yarin Gal Richard Turner Ranking Stochastic Pairwise Preferences Recovering Condorcet Winners Tournament Solution Sets Top Arun Rajkumar Lek-Heng Lim Stochastic Dual Coordinate Ascent Adaptive Probabilities Dominik Csiba Zheng Qu Peter Richtarik Vector-Space Markov Random Fields via Exponential Families Wesley Tansey OSCAR HERNAN MADRID PADILLA Arun Sai Suggala Pradeep Ravikumar JUMP-Means Small-Variance Asymptotics Markov Jump Processes Jonathan Huggins Karthik Narasimhan Ardavan Saeedi Vikash Mansinghka Low Rank Approximation using Error Correcting Coding Matrices Shashanka Ubaru Arya Mazumdar Yousef Saad Off-policy Model-based Learning Unknown Factored Dynamics Assaf Hallak Francois Schnitzler Timothy Mann Shie Mannor Log-Euclidean Metric Learning Symmetric Positive Definite Manifold Application Image Set Classification Zhiwu Huang Ruiping Wang Shiguang Shan Xianqiu Li Xilin Chen Asymmetric Transfer Learning Deep Gaussian Processes Melih Kandemir Towards Lower Sample Complexity Robust One-bit Compressed Sensing Rongda Zhu Quanquan Gu BilBOWA Fast Bilingual Distributed Representations without Word Alignments Stephan Gouws Yoshua Bengio Greg Corrado Multi-view Sparse Co-clustering via Proximal Alternating Linearized Minimization Jiangwen Sun Jin Lu Tingyang Xu Jinbo Bi Cascading Bandits Learning Rank Cascade Model Branislav Kveton Csaba Szepesvari Zheng Wen Azin Ashkan Latent Topic Networks Versatile Probabilistic Programming Framework Topic Models James Foulds Shachi Kumar Lise Getoor Random Coordinate Descent Methods Minimizing Decomposable Submodular Functions Alina Ene Huy Nguyen Alpha-Beta Divergences Discover Micro Macro Structures Data Karthik Narayan Ali Punjani Pieter Abbeel Fictitious Self-Play Extensive-Form Games Johannes Heinrich Marc Lanctot Counterfactual Risk Minimization Learning Logged Bandit Feedback Adith Swaminathan Thorsten Joachims Hedge Algorithm Continuum Walid Krichene Maximilian Balandat Claire Tomlin Linear Dynamical System Model Text David Belanger Sham Kakade Unsupervised Learning Video Representations using LSTMs Nitish Srivastava Elman Mansimov Ruslan Salakhudinov Message Passing Collective Graphical Models TAO SUN Dan Sheldon Akshat Kumar DP-space Bayesian Nonparametric Subspace Clustering Small-variance Asymptotics Yining Wang Jun Zhu HawkesTopic Joint Model Network Inference Topic Modeling Text-Based Cascades Xinran Theodoros Rekatsinas James Foulds Lise Getoor Yan Liu MADE Masked Autoencoder Distribution Estimation Mathieu Germain Karol Gregor Iain Murray Hugo Larochelle Online Learning Algorithm Bilinear Models Yuanbin Wu Shiliang Sun Adaptive Belief Propagation Georgios Papachristoudis John Fisher Large-scale log-determinant computation stochastic Chebyshev expansions Insu Han Dmitry Malioutov Jinwoo Shin Differentially Private Bayesian Optimization Matt Kusner Jacob Gardner Roman Garnett Kilian Weinberger Nearly-Linear Time Framework Graph-Structured Sparsity Chinmay Hegde Piotr Indyk Ludwig Schmidt Support Matrix Machines Luo Luo Yubo Xie Zhihua Zhang Wu-Jun Li Rademacher Observations Private Data Boosting Richard Nock Giorgio Patrini Arik Friedman Word Embeddings Document Distances Matt Kusner Yu Sun Nicholas Kolkin Kilian Weinberger Bayesian Empirical Bayesian Forests Taddy Matthew Chun-Sheng Chen Mitch Wyle Inferring Graphs Cascades Sparse Recovery Framework Jean Pouget-Abadie Thibaut Horel Distributed Box-Constrained Quadratic Optimization Dual Linear SVM Ching-Pei Lee Dan Roth Safe Exploration Optimization Gaussian Processes Yanan Sui Alkis Gotovos Joel Burdick Andreas Krause Ladder Reliable Leaderboard Machine Learning Competitions Avrim Blum Moritz Hardt Enabling scalable stochastic gradient-based inference Gaussian processes employing Unbiased LInear System SolvEr Maurizio Filippone Raphael Engler Finding Galaxies Shadows Quasars Gaussian Processes Roman Garnett Shirley Ho Jeff Schneider Following Perturbed Leader Online Structured Learning Alon Cohen Tamir Hazan Reified Context Models Jacob Steinhardt percy Liang Large-Scale Markov Decision Problems KL Control Cost Application Crowdsourcing Yasin Abbasi-Yadkori Peter Bartlett Xi Chen Alan Malek Learning Fast-Mixing Models Structured Prediction Jacob Steinhardt percy Liang Probabilistic Model Dirty Multi-task Feature Selection Daniel Hernandez-Lobato Jose Miguel Hernandez-Lobato Zoubin Ghahramani Deep Multi-View Representation Learning Weiran Wang Raman Arora Karen Livescu Jeff Bilmes Learning Program Embeddings Propagate Feedback Student Code Chris Piech Jonathan Huang Andy Nguyen Mike Phulsuksombati Mehran Sahami Leonidas Guibas Safe Subspace Screening Nuclear Norm Regularized Least Squares Problems Qiang Zhou Qi Zhao Efficient Learning Large-Scale Combinatorial Semi-Bandits Zheng Wen Branislav Kveton Azin Ashkan Swept Approximate Message Passing Sparse Estimation Andre Manoel Florent Krzakala Eric Tramel Lenka Zdeborov Simple regret infinitely many armed bandits Alexandra Carpentier Michal Valko Exponential Integration Hamiltonian Monte Carlo Wei-Lun Chao Justin Solomon Dominik Michels Fei Sha Optimal Regret Analysis Thompson Sampling Stochastic Multi-armed Bandit Problem Multiple Plays Junpei Komiyama Junya Honda Hiroshi Nakagawa Faster cover trees Mike Izbicki Christian Shelton Blitz Principled Meta-Algorithm Scaling Sparse Optimization Tyler Johnson Carlos Guestrin Unsupervised Domain Adaptation Backpropagation Yaroslav Ganin Victor Lempitsky Non-Linear Cross-Domain Collaborative Filtering via Hyper-Structure Transfer Yan-Fu Liu Shan-Hung Wu Manifold-valued Dirichlet Processes Hyunwoo Kim Jia Xu Baba Vemuri Vikas Singh Multi-Task Learning Subspace Segmentation Yu Wang David Wipf Qing Ling Wei Chen Ian Wassell Markov Chain Monte Carlo Variational Inference Bridging Gap Tim Salimans Diederik Kingma Max Welling Scalable Model Selection Large-Scale Factorial Relational Models Chunchen Liu Ryohei Fujimaki Power Randomization Distributed Submodular Maximization Massive Datasets Rafael Barbosa Alina Ene Huy Nguyen Justin Ward Dealing small data generalization context trees Ralf Eggeling Mikko Koivisto Ivo Grosse Non-Gaussian Discriminative Factor Models via Max-Margin Rank-Likelihood Xin Yuan Ricardo Henao Ephraim Tsalik Raymond Langley Lawrence Carin Bayesian nonparametric procedure comparing algorithms Alessio Benavoli Giorgio Corani Francesca Mangili Marco Zaffalon Convergence rate Bayesian tensor estimator minimax optimality Taiji Suzuki Identifying Good Options Combinatorially Structured Feedback Finite Noisy Environments Yifan Wu Andras Gyorgy Csaba Szepesvari Nested Sequential Monte Carlo Methods Christian Naesseth Fredrik Lindsten Thomas Schon Sparse Variational Inference Generalized GP Models Rishit Sheth Yuyang Wang Roni Khardon Universal Value Function Approximators Tom Schaul Daniel Horgan Karol Gregor David Silver Approximate Dynamic Programming Two-Player Zero-Sum Markov Games Julien Perolat Bruno Scherrer Bilal Piot Olivier Pietquin Greedy Maximization Entropy Dravyansh Sharma Ashish Kapoor Amit Deshpande Metadata Dependent Mondrian Processes Yi Wang Bin Li Yang Wang Fang Chen Complex Event Detection using Semantic Saliency Nearly-Isotonic SVM XIAOJUN CHANG YI YANG Eric Xing Yaoliang Yu Rebuilding Factorized Information Criterion Asymptotically Accurate Marginal Likelihood Kohei Hayashi Shin-ichi Maeda Ryohei Fujimaki Double Nystr om Method Efficient Accurate Nystr om Scheme Large-Scale Data Sets Woosang Lim Minhwan Kim Haesun Park Kyomin Jung Composition Theorem Differential Privacy Peter Kairouz Sewoong Oh Pramod Viswanath Convex Formulation Learning Positive Unlabeled Data Marthinus Du Plessis Gang Niu Threshold Influence Model Allocating Advertising Budgets Atsushi Miyauchi Yuni Iwamasa Takuro Fukunaga Naonori Kakimura Strongly Adaptive Online Learning Amit Daniely Alon Gonen Shai Shalev-Shwartz CUR Algorithm Partially Observed Matrices Miao Xu Rong Jin Zhi-Hua Zhou Deterministic Analysis Noisy Sparse Subspace Clustering Dimensionality-reduced Data Yining Wang Yu-Xiang Wang Aarti Singh MRA-based Statistical Learning Incomplete Rankings Eric Sibony St phan Clemen J r mie Jakubowicz Risk Regret Hierarchical Bayesian Learners Jonathan Huggins Josh Tenenbaum Towards Learning Theory Cause-Effect Inference David Lopez-Paz Krikamol Muandet Bernhard Sch lkopf Iliya Tolstikhin DRAW Recurrent Neural Network Image Generation Karol Gregor Ivo Danihelka Alex Graves Danilo Rezende Daan Wierstra Multiview Triplet Embedding Learning Attributes Multiple Maps Ehsan Amid Antti Ukkonen Distributed Gaussian Processes Marc Deisenroth Jun Wei Ng Guaranteed Tensor Decomposition Moment Approach Gongguo Tang Parikshit Shah ell_ 1 p -Norm Regularization Error Bounds Convergence Rate Analysis First-Order Methods Zirui Zhou Qi Zhang Anthony Man-Cho Consistent estimation dynamic multi-layer block models Qiuyi Han Kevin Xu Edoardo Airoldi Rate Convergence Error Bounds LSTD lambda Manel Tagorti Bruno Scherrer Variational Inference Normalizing Flows Danilo Rezende Shakir Mohamed Controversy mechanistic modelling Gaussian processes Benn Macdonald Catherine Higham Dirk Husmeier Convex Learning Multiple Tasks Structure Carlo Ciliberto Youssef Mroueh Tomaso Poggio Lorenzo Rosasco K-hyperplane Hinge-Minimax Classifier Margarita Osadchy Tamir Hazan Daniel Keren Non-Stationary Approximate Modified Policy Iteration Boris Lesner Entropy evaluation based confidence intervals frequency estimates Application learning decision trees Mathieu Serrurier Henri Prade Geometric Conditions Subspace-Sparse Recovery Chong Rene Vidal Empirical Study Stochastic Variational Inference Algorithms Beta Bernoulli Process Amar Shah David Knowles Zoubin Ghahramani Long Short-Term Memory Recursive Structures Xiaodan Zhu Parinaz Sobihani Hongyu Guo Weight Uncertainty Neural Network Charles Blundell Julien Cornebise Daan Wierstra Learning Submodular Losses Lovasz Hinge Jiaqian Yu Matthew Blaschko Coordinate Descent Converges Faster Gauss-Southwell Rule Random Selection Julie Nutini Mark Schmidt Issam Laradji Hoyt Koepke Hashing Distributed Data Cong Leng Jiaxiang Wu Jian Cheng Xi Zhang Hanqing Lu Large-scale Distributed Dependent Nonparametric Trees Zhiting Hu Ho Qirong Avinava Dubey Eric Xing Qualitative Multi-Armed Bandits Quantile-Based Approach Balazs Szorenyi Robert Busa-Fekete Paul Weng Eyke H llermeier Deep Edge-Aware Filters Li Xu Jimmy Ren Qiong Yan Renjie Liao Jiaya Jia Convex Optimization Framework Bi-Clustering Shiau Hong Lim Yudong Chen Huan Xu Feature Selection Secure Training Data Poisoning Huang Xiao Battista Biggio Gavin Brown Giorgio Fumera Claudia Eckert Fabio Roli Predictive Entropy Search Bayesian Optimization Unknown Constraints Jose Miguel Hernandez-Lobato Michael Gelbart Matthew Hoffman Ryan Adams Zoubin Ghahramani Theoretical Analysis Metric Hypothesis Transfer Learning Micha l Perrot Amaury Habrard Generative Moment Matching Networks Yujia Li Kevin Swersky Rich Zemel Stay path PCA along graph paths Megasthenis Asteris Anastasios Kyrillidis Alex Dimakis Han-Gyol Yi bharath Chandrasekaran Deep Learning Limited Numerical Precision Suyog Gupta Ankur Agrawal Pritish Narayanan Safe Screening Multi-Task Feature Learning Multiple Data Matrices Jie Wang Jieping Ye Harmonic Exponential Families Manifolds Taco Cohen Max Welling Training Deep Convolutional Neural Networks Play Go Christopher Clark Amos Storkey Kernel Interpolation Scalable Structured Gaussian Processes Andrew Wilson Hannes Nickisch Learning Deep Structured Models Liang-Chieh Chen Alexander Schwing Alan Yuille Raquel Urtasun Community Detection Using Time-Dependent Personalized PageRank Haim Avron Lior Horesh Scalable Variational Inference Log-supermodular Models Josip Djolonga Andreas Krause Variational Inference Gaussian Process Modulated Poisson Processes Chris Lloyd Tom Gunter Michael Osborne Stephen Roberts Scalable Deep Poisson Factor Analysis Topic Modeling Zhe Gan Changyou Chen Ricardo Henao David Carlson Lawrence Carin Hidden Markov Anomaly Detection Nico Goernitz Mikio Braun Marius Kloft Robust Estimation Transition Matrices High Dimensional Heavy-tailed Vector Autoregressive Processes Huitong Qiu Sheng Xu Fang Han Han Liu Brian Caffo Convex Calibrated Surrogates Hierarchical Classification Harish Ramaswamy Ambuj Tewari Shivani Agarwal Probabilistic Backpropagation Scalable Learning Bayesian Neural Networks Jose Miguel Hernandez-Lobato Ryan Adams Active Nearest Neighbors Changing Environments Christopher Berlind Ruth Urner Bipartite Edge Prediction via Transductive Learning Product Graphs Hanxiao Liu Yiming Yang Trust Region Policy Optimization John Schulman Sergey Levine Pieter Abbeel Michael Jordan Philipp Moritz Discovering Temporal Causal Relations Subsampled Data Mingming Gong Kun Zhang Bernhard Schoelkopf Philipp Geiger Preference Completion Large-scale Collaborative Ranking Pairwise Comparisons Dohyung Park Joe Neeman Jin Zhang Sujay Sanghavi Inderjit Dhillon Causal Inference Identification Vector Autoregressive Processes Hidden Components Philipp Geiger Kun Zhang Bernhard Schoelkopf Dominik Janzing Symmetric Asymmetric LSHs Inner Product Search Behnam Neyshabur Nathan Srebro Kendall Mallows Kernels Permutations Yunlong Jiao Jean-Philippe Vert Bayesian Multiple Target Localization Purnima Rajan Weidong Han Peter Frazier Bruno Jedynak Submodularity Data Subset Selection Active Learning Kai Wei Rishabh Iyer Jeff Bilmes Variational Generative Stochastic Networks Collaborative Shaping Philip Bachman Doina Precup Adding vs Averaging Distributed Primal-Dual Optimization Chenxin Ma Virginia Smith Martin Jaggi Michael Jordan Peter Richtarik Martin Takac Feature-Budgeted Random Forest Feng Nan Joseph Wang Entropic Graph-based Posterior Regularization Maxwell Libbrecht Michael Hoffman Jeff Bilmes William Noble Unsupervised Riemannian Metric Learning Histograms Using Aitchison Transformations Tam Le Marco Cuturi Low-Rank Matrix Recovery Row-and-Column Affine Measurements Zuk Avishai Wagner Algorithms Hard Pre-Image Problem String Kernels General Problem String Prediction bastien Gigu re lie Rolland Mario Marchand Multitask Point Process Predictive Model Wenzhao Lian Ricardo Henao Vinayak Rao Joseph Lucas Lawrence Carin Hybrid Approach Probabilistic Inference using Random Projections Michael Zhu Stefano Ermon Show Attend Tell Neural Image Caption Generation Visual Attention Kelvin Xu Jimmy Ba Ryan Kiros Kyunghyun Cho Aaron Courville Ruslan Salakhudinov Rich Zemel Yoshua Bengio Learning Search Better Teacher Kai-Wei Chang Akshay Krishnamurthy Alekh Agarwal Hal Daume John Langford Gated Feedback Recurrent Neural Networks Junyoung Chung Caglar Gulcehre Kyunghyun Cho Yoshua Bengio Context-based Unsupervised Data Fusion Decision Making Erfan Soltanmohammadi Mort Naraghi-Pour Mihaela van der Schaar Phrase-based Image Captioning Remi Lebret Pedro Pinheiro Ronan Collobert Celeste Variational inference generative model astronomical images Jeffrey Regier Andrew Miller Jon McAuliffe Ryan Adams Matt Hoffman Dustin Lang David Schlegel Prabhat Distributional Rank Aggregation Axiomatic Analysis Adarsh Prasad Harsh Pareek Pradeep Ravikumar Gradient-based Hyperparameter Optimization Reversible Learning Dougal Maclaurin David Duvenaud Ryan Adams Bimodal Modelling Source Code Natural Language Miltos Allamanis Daniel Tarlow Andrew Gordon Yi Wei Cheap Bandits Manjesh Hanawal Venkatesh Saligrama Remi Munos Subsampling Methods Persistent Homology Frederic Chazal Fabrizio Lecci Bertrand Michel Larry Wasserman embarrassingly simple approach zero-shot learning Bernardino Romera-Paredes Philip Torr Binary Embedding Fundamental Limits Fast Algorithm Xinyang Yi Constantine Caramanis Eric Price Scalable Bayesian Optimization Using Deep Neural Networks Jasper Snoek Oren Rippel Kevin Swersky Ryan Kiros Nadathur Satish Narayanan Sundaram Mostofa Patwary Mr Prabhat Ryan Adams Hard Inference Structured Prediction Amir Globerson Tim Roughgarden David Sontag Cafer Yildirim Online Time Series Prediction Missing Data Oren Anava Elad Hazan Assaf Zeevi Proteins Particles Pseudo-Max-Marginals Submodular Approach Jason Pacheco Erik Sudderth Fast Variational Approach Learning Markov Random Field Language Models Yacine Jernite Alexander Rush David Sontag Removing systematic errors exoplanet search via latent causes Bernhard Sch lkopf David Hogg Dun Wang Dan Foreman-Mackey Dominik Janzing Carl-Johann Simon-Gabriel Jonas Peters Scalable Nonparametric Bayesian Inference Point Processes Gaussian Processes Yves-Laurent KOM SAMO Stephen Roberts Correlation Clustering Data Streams KookJin Ahn Graham Cormode Sudipto Guha Andrew McGregor Anthony Wirth Learning Scale-Free Networks Dynamic Node Specific Degree Prior Qingming Tang Siqi Sun Jinbo Xu Deep Unsupervised Learning using Nonequilibrium Thermodynamics Jascha Sohl-Dickstein Eric Weiss Niru Maheswaranathan Surya Ganguli Modeling Order Neural Word Embeddings Scale Andrew Trask David Gilmore Matthew Russell Distributed Inference Dirichlet Process Mixture Models Hong Ge Yutian Chen Moquan Wan Zoubin Ghahramani Compressing Neural Networks Hashing Trick Wenlin Chen James Wilson Stephen Tyree Kilian Weinberger Yixin Chen Intersecting Faces Non-negative Matrix Factorization New Guarantees Rong Ge James Zou Scaling Natural Gradient Sparsely Factorizing Inverse Fisher Matrix Roger Grosse Ruslan Salakhudinov Deeper Look Planning Learning Replay Harm Vanseijen Rich Sutton Optimal Adaptive Algorithms Online Boosting Alina Beygelzimer Satyen Kale Haipeng Luo Global Convergence Stochastic Gradient Descent Non-convex Matrix Problems Christopher De Sa Christopher Re Kunle Olukotun Empirical Exploration Recurrent Network Architectures Rafal Jozefowicz Wojciech Zaremba Ilya Sutskever Complete Dictionary Recovery Using Nonconvex Optimization Ju Sun Qing Qu John Wright Safe Policy Search Lifelong Reinforcement Learning Sublinear Regret Haitham Bou Ammar Rasul Tutunov PASSCoDe Parallel ASynchronous Stochastic dual Co-ordinate Descent Cho-Jui Hsieh Hsiang-Fu Yu Inderjit Dhillon High Confidence Policy Improvement Philip Thomas Georgios Theocharous Mohammad Ghavamzadeh Fixed-point algorithms learning determinantal point processes Zelda Mariet Suvrit Sra Consistent Multiclass Algorithms Complex Performance Measures Harikrishna Narasimhan Harish Ramaswamy Aadirupa Saha Optimizing Neural Networks Kronecker-factored Approximate Curvature James Martens Roger Grosse Convex Exemplar-based Approach MAD-Bayes Dirichlet Process Mixture Models En-Hsu Yen XIN LIN Kai Zhong Pradeep Ravikumar Inderjit Dhillon Multi-instance multi-label learning presence novel class instances Anh Pham Raviv Raich Xiaoli Fern Jes P rez Arriaga Entropy-Based Concentration Inequalities Dependent Variables Liva Ralaivola Massih-Reza Amini PU Learning Matrix Completion Cho-Jui Hsieh Nagarajan Natarajan Inderjit Dhillon Asynchronous Distributed Proximal Gradient Method Composite Convex Optimization Necdet Aybat Zi Wang Garud Iyengar Sparse Subspace Clustering Missing Entries CONGYUAN YANG Daniel Robinson Rene Vidal Moderated Drifting Linear Dynamical Systems Jinyan Guan Kyle Simek Ernesto Brau Clayton Morrison Emily Butler Kobus Barnard Boosted Categorical Restricted Boltzmann Machine Computational Prediction Splice Junctions Taehoon Lee Sungroh Yoon Privacy Free Posterior Sampling Stochastic Gradient Monte Carlo Yu-Xiang Wang Stephen Fienberg Alex Smola trust-region method stochastic variational inference applications streaming data Lucas Theis Matt Hoffman Inference Partially Observed Queuing Model Applications Ecology Kevin Winner Garrett Bernstein Dan Sheldon Deterministic Independent Component Analysis Ruitong Huang Andras Gyorgy Csaba Szepesv ri Optimality Multi-Label Classification Subset Zero-One Loss Distributions Satisfying Composition Property Maxime Gasse Alexandre Aussem Haytham Elghazel Un-regularizing approximate proximal point faster stochastic algorithms empirical risk minimization Roy Frostig Rong Ge Sham Kakade Aaron Sidford New Generalized Error Path Algorithm Model Selection Bin Gu Charles Ling 2014-2015 ICML International Conference Machine Learning'),
('Free Open Source Bayesian Network Software', 'CLOSE Analytics Business Intelligence Machine Learning Statistics Data Database Analytics Business Intelligence Machine Learning Statistics Data Database Home Analytics Machine Learning 10 Free Open Source Bayesian Network Software 10 Free Open Source Bayesian Network Software AnalyticsMachine Learning May 6 2015 0 83 Banjo Bayesian Network Inference Java Objects static dynamic Bayesian networks Bayesian Network Tools Java BNJ research development using graphical models probability implemented 100 pure Java BUGS Bayesian Inference using Gibbs Sampling Bayesian analysis complex statistical models using Markov chain Monte Carlo methods Dlib C Library extensive Bayesian Network support Dynamic Bayesian Network Simulator FBN Free Bayesian Network constraint based learning Bayesian networks JavaBayes system calculates marginal probabilities expectations produces explanations performs robustness analysis allows user import create modify export networks jBNC Java toolkit training testing applying Bayesian Network Classifiers JNCC2 Java implementation Naive Credal Classifier 2 MSBNx component-based Windows application creating assessing evaluating Bayesian Networks SMILE Structural Modeling Inference Learning Engine fully portable library C classes implementing graphical decision-theoretic methods Bayesian networks influence diagrams UnBBayes framework GUI Bayes Nets probabilistic models TAGSBayesian NetworksBUGSFreeMachine LearningMSBNxOpen Source Next article10 Free Deep Learning Tools teklis SIMILAR ARTICLES 10 Free Deep Learning Tools May 6 2015 0 35 COMMENTS Leave Reply Cancel reply Categories Business Intelligence Database Machine Learning Statistics Recent Posts 10 Free Open Source Bayesian Network Software May 6 2015 10 Free Deep Learning Tools May 6 2015 10 Excel Monte Carlo Simulation Add-Ins May 11 2015 18 In-Memory Databases May 9 2015 5 Embeddable In-Memory Databases May 9 2015 teklis'),
('ICLR 2015 Conference Schedule/Publication overview', "ICLR ICLR Home ICLR 2015 News ICLR 2015 Call Papers Style files Template ICLR Facebook Page Publication model Previous ICLRs ICLR 2014 ICLR 2013 CBLS Table Contents Basic Information Registration Important Dates Committee Discussion Forum Pictures ICLR Facebook Page Sponsors Conference Wireless Access Conference Schedule Keynote Talks Conference Oral Presentations May 9 Conference Poster Session May 7 Workshop Poster Session May 8 Workshop Poster Session Presentation Guidelines Basic Information May 7 - 9 2015 Hilton San Diego Resort Spa negotiated room rate ICLR 2015 Please use link reservations difficulty booking site please call Hilton San Diego's in-house reservation team directly 1-619-276-4010 ext 1 Registration Anyone registering April 29 2015 need see Karen Smith registration desk badge Late registration regular 800 Late registration student 600 Note registration fee includes breakfast coffee breaks dinner joint ICLR AISTATS reception See conference schedule timing events Online Registration Form Important Dates 19 Dec 2014 Authors submit papers ICLR 2015 via CMT 11 59 pm PST 26 Dec 2014 Authors update submissions arXiv number URL available 19 Dec 2014 02 Jan 2015 Reviewers receive assignments 09 Feb 2015 Reviewers submit reviews 27 Feb 2015 Authors post initial responses reviews 09 Mar 2015 End discussion period papers 20 Mar 2015 Decisions sent authors 06 Apr 2015 Deadline early registration register hotel conference rate Committee General Chairs Yoshua Bengio Universit de Montreal Yann LeCun New York University Facebook Program Chairs Brian Kingsbury IBM Research Samy Bengio Google Nando de Freitas University Oxford Hugo Larochelle Universit de Sherbrooke Contact iclr2015 programchairs gmail com Discussion Forum Pictures ICLR Facebook Page https www facebook com iclr cc Sponsors ICLR 2015 gratefully acknowledges support sponsors Gold Silver Bronze Conference Wireless Access network Hilton Resort username iclr2015 password deeplearning Conference Schedule Date Start End Event Details May 7 0730 0900 breakfast South Poolside Sponsored Baidu 0900 1230 Oral Session International Ballroom 0900 0940 keynote Antoine Bordes Facebook Artificial Tasks Artificial Intelligence slides Video1 Video2 0940 1000 oral Word Representations via Gaussian Embedding Luke Vilnis Andrew McCallum Brown University slides Video 1000 1020 oral Deep Captioning Multimodal Recurrent Neural Networks m-RNN Junhua Mao Wei Xu Yi Yang Jiang Wang Zhiheng Huang Alan Yuille Baidu UCLA slides Video 1020 1050 coffee break 1050 1130 keynote David Silver Google DeepMind Deep Reinforcement Learning slides Video1 Video2 1130 1150 oral Deep Structured Output Learning Unconstrained Text Recognition Text Recognition Max Jaderberg Karen Simonyan Andrea Vedaldi Andrew Zisserman Oxford University Google DeepMind slides Video 1150 1210 oral Deep Convolutional Networks Large-Scale Image Recognition Karen Simonyan Andrew Zisserman Oxford slides Video 1210 1230 oral Fast Convolutional Nets fbfft GPU Performance Evaluation Nicolas Vasilache Jeff Johnson Michael Mathieu Soumith Chintala Serkan Piantino Yann LeCun Facebook AI Research slides Video 1230 1400 lunch 1400 1700 posters Workshop Poster Session 1 Pavilion 1730 1900 dinner South Poolside Sponsored Google May 8 0730 0900 breakfast South Poolside Sponsored Facebook 0900 1230 Oral Session International Ballroom 0900 0940 keynote Terrence Sejnowski Salk Institute Beyond Representation Learning Video1 Video2 0940 1000 oral Reweighted Wake-Sleep slides Video 1000 1020 oral local low-dimensionality natural images slides Video 1020 1050 coffee break 1050 1130 keynote Percy Liang Stanford Learning Latent Programs Question Answering slides Video1 Video2 1130 1150 oral Memory Networks slides Video 1150 1210 oral Object detectors emerge Deep Scene CNNs slides Video 1210 1230 oral Qualitatively characterizing neural network optimization problems slides Video 1230 1400 lunch 1400 1700 posters Workshop Poster Session 2 Pavilion 1730 1900 dinner South Poolside Sponsored IBM Watson May 9 0730 0900 breakfast South Poolside Sponsored Qualcomm 0900 0940 keynote Hal Daum III U Maryland Algorithms Learn Think Feet slides Video 0940 1000 oral Neural Machine Translation Jointly Learning Align Translate slides Video 1000 1030 coffee break 1030 1330 posters Conference Poster Session Pavilion AISTATS attendees invited poster session 1330 1700 lunch break 1700 1800 ICLR AISTATS Oral Session International Ballroom 1700 1800 keynote Pierre Baldi UC Irvine Ebb Flow Deep Learning Theory Local Learning Video 1800 2000 ICLR AISTATS reception Fresco's near pool Keynote Talks Antoine Bordes Artificial Tasks Artificial Intelligence Despite great recent advances road towards intelligent machines able reason adapt real-time multimodal environments remains long uncertain final goal complex away impossible perform experiments research directly desired final conditions one use intermediate proxy tasks midway goals tasks like object detection computer vision machine translation natural language processing useful fuel many applications However intermediate tasks already difficult obvious suited testbeds designing intelligent systems inherent complexity makes hard precisely interpret behavior true capabilities algorithms particular regarding key sophisticated capabilities like reasoning planning Hence talk advocate use controlled artificial environments developing research AI environments one precisely study behavior algorithms unambiguously assess abilities talk follows joint work discussions Jason Weston Sumit Chopra Tomas Mikolov Leon Bottou among others David Silver Deep Reinforcement Learning talk discuss reinforcement learning RL combined deep learning DL several ways combine DL RL together including value-based policy-based model-based approaches planning Several approaches well-known divergence issues present simple methods addressing instabilities methods achieved notable success Atari 2600 domain present recent selection recent results improve published state-of-the-art Atari challenging domains Finally discuss RL used improve DL even native problem supervised unsupervised learning Terrence Sejnowski Beyond Representation Learning build ever deeper networks ever sophisticated representations good time pause ask end Building ever taller skyscrapers gets heads clouds get us moon good place look answers nature lecture start look hierarchy cortical areas much intuition deep learning came explore essential brain regions cortical areas communicate give rise intelligent behavior Percy Liang Learning Latent Programs Question Answering first Summer Olympics least 20 nations took place city tackle problem building system answering questions involve computing answer propose methodology based semantic parsing map question onto latent program logical form whose execution yields answer denotation obtain depth complexity program breadth diversity questions domains define new task answering complex question semi-structured tables web show promising results new dataset invite community take challenge Hal Daum III Algorithms Learn Think Feet classic framework machine learning example prediction great examples fully available different humans reason get information may make prediction may decide get information us it's worth spending effort making hard important decisions e g foreign policy easy low-cost decisions e g afternoon snacks I'll describe recent work focuses information cost value time I'll show examples three settings natural language processing syntactic parsing question answering competitions simultaneous machine translation last problem incrementally producing translation foreign sentence entire sentence heard challenging even well-trained humans joint work number fantastic collaborators Jordan Boyd-Graber Leonardo Claudino Jason Eisner Lise Getoor Alvin Grissom II Mohit Iyyer John Morgan Jay Pujara Richard Socher Pierre Baldi Ebb Flow Deep Learning Theory Local Learning physical neural system storage processing intertwined learning rules adjusting synaptic weights depend local variables activity pre- post-synaptic neurons Thus learning models must specify two things 1 variables considered local 2 kind function combines local variables learning rule consider polynomial learning rules analyze behavior capabilities linear non-linear networks byproduct framework enables discovery new learning rules important relationships learning rules group symmetries Stacking local learning rules deep feedforward networks leads deep local learning deep local learning learn interesting representations cannot learn complex input-output functions even targets available top layer Learning complex input-output functions requires instead local deep learning target information transmitted deep layers thereby raising two fundamental issues 1 nature transmission channel 2 nature amount information transmitted channel leads class deep targets learning algorithms provide targets deep layers stratification along information spectrum illuminating remarkable power uniqueness backpropation algorithm theory clarifies concept Hebbian learning learnable Hebbian learning explains sparsity space learning rules discovered far unique role backpropagation plays space Conference Oral Presentations Word Representations via Gaussian Embedding Luke Vilnis Andrew McCallum Deep Captioning Multimodal Recurrent Neural Networks m-RNN Junhua Mao Wei Xu Yi Yang Jiang Wang Alan Yuille Deep Structured Output Learning Unconstrained Text Recognition Max Jaderberg Karen Simonyan Andrea Vedaldi Andrew Zisserman Deep Convolutional Networks Large-Scale Image Recognition Karen Simonyan Andrew Zisserman Fast Convolutional Nets fbfft GPU Performance Evaluation Nicolas Vasilache Jeff Johnson Michael Mathieu Soumith Chintala Serkan Piantino Yann LeCun Reweighted Wake-Sleep Jorg Bornschein Yoshua Bengio local low-dimensionality natural images Olivier Henaff Johannes Balle Neil Rabinowitz Eero Simoncelli Memory Networks Jason Weston Sumit Chopra Antoine Bordes Object detectors emerge Deep Scene CNNs Bolei Zhou Aditya Khosla Agata Lapedriza Aude Oliva Antonio Torralba Qualitatively characterizing neural network optimization problems Ian Goodfellow Oriol Vinyals Neural Machine Translation Jointly Learning Align Translate Dzmitry Bahdanau Kyunghyun Cho Yoshua Bengio May 9 Conference Poster Session Board Presentation 2 FitNets Hints Thin Deep Nets Adriana Romero Nicolas Ballas Samira Ebrahimi Kahou Antoine Chassang Carlo Gatta Yoshua Bengio 3 Techniques Learning Binary Stochastic Feedforward Neural Networks Tapani Raiko Mathias Berglund Guillaume Alain Laurent Dinh 4 Reweighted Wake-Sleep Jorg Bornschein Yoshua Bengio 5 Semantic Image Segmentation Deep Convolutional Nets Fully Connected CRFs Liang-Chieh Chen George Papandreou Iasonas Kokkinos Kevin Murphy Alan Yuille 7 Multiple Object Recognition Visual Attention Jimmy Ba Volodymyr Mnih Koray Kavukcuoglu 8 Deep Narrow Boltzmann Machines Universal Approximators Guido Montufar 9 Transformation Properties Learned Visual Representations Taco Cohen Max Welling 10 Joint RNN-Based Greedy Parsing Word Composition Jo l Legrand Ronan Collobert 11 Adam Method Stochastic Optimization Jimmy Ba Diederik Kingma 13 Neural Machine Translation Jointly Learning Align Translate Dzmitry Bahdanau Kyunghyun Cho Yoshua Bengio 15 Scheduled denoising autoencoders Krzysztof Geras Charles Sutton 16 Embedding Entities Relations Learning Inference Knowledge Bases Bishan Yang Scott Yih Xiaodong Jianfeng Gao Li Deng 18 local low-dimensionality natural images Olivier Henaff Johannes Balle Neil Rabinowitz Eero Simoncelli 20 Explaining Harnessing Adversarial Examples Ian Goodfellow Jon Shlens Christian Szegedy 22 Modeling Compositionality Multiplicative Recurrent Neural Networks Ozan Irsoy Claire Cardie 24 Deep Convolutional Networks Large-Scale Image Recognition Karen Simonyan Andrew Zisserman 25 Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition Vadim Lebedev Yaroslav Ganin Victor Lempitsky Maksim Rakhuba Ivan Oseledets 27 Deep Captioning Multimodal Recurrent Neural Networks m-RNN Junhua Mao Wei Xu Yi Yang Jiang Wang Alan Yuille 28 Deep Structured Output Learning Unconstrained Text Recognition Max Jaderberg Karen Simonyan Andrea Vedaldi Andrew Zisserman 30 Zero-bias autoencoders benefits co-adapting features Kishore Konda Roland Memisevic David Krueger 31 Automatic Discovery Optimization Parts Image Classification Sobhan Naderi Parizi Andrea Vedaldi Andrew Zisserman Pedro Felzenszwalb 33 Understanding Locally Competitive Networks Rupesh Srivastava Jonathan Masci Faustino Gomez Juergen Schmidhuber 35 Leveraging Monolingual Data Crosslingual Compositional Word Representations Hubert Soyer Pontus Stenetorp Akiko Aizawa 36 Move Evaluation Go Using Deep Convolutional Neural Networks Chris Maddison Aja Huang Ilya Sutskever David Silver 38 Fast Convolutional Nets fbfft GPU Performance Evaluation Nicolas Vasilache Jeff Johnson Michael Mathieu Soumith Chintala Serkan Piantino Yann LeCun 40 Word Representations via Gaussian Embedding Luke Vilnis Andrew McCallum 41 Qualitatively characterizing neural network optimization problems Ian Goodfellow Oriol Vinyals 42 Memory Networks Jason Weston Sumit Chopra Antoine Bordes 43 Generative Modeling Convolutional Neural Networks Jifeng Dai Yang Lu Ying-Nian Wu 44 Unified Perspective Multi-Domain Multi-Task Learning Yongxin Yang Timothy Hospedales 45 Object detectors emerge Deep Scene CNNs Bolei Zhou Aditya Khosla Agata Lapedriza Aude Oliva Antonio Torralba May 7 Workshop Poster Session Board Presentation 2 Learning Non-deterministic Representations Energy-based Ensembles Maruan Al-Shedivat Emre Neftci Gert Cauwenberghs 3 Diverse Embedding Neural Network Language Models Kartik Audhkhasi Abhinav Sethy Bhuvana Ramabhadran 4 Hot Swapping Online Adaptation Optimization Hyperparameters Kevin Bache Dennis Decoste Padhraic Smyth 5 Representation Learning cold-start recommendation Gabriella Contardo Ludovic Denoyer Thierry Artieres 6 Training Convolutional Networks Noisy Labels Sainbayar Sukhbaatar Joan Bruna Manohar Paluri Lubomir Bourdev Rob Fergus 7 Striving Simplicity Convolutional Net Alexey Dosovitskiy Jost Tobias Springenberg Thomas Brox Martin Riedmiller 8 Learning linearly separable features speech recognition using convolutional neural networks Dimitri Palaz Mathew Magimai Doss Ronan Collobert 9 Training Deep Neural Networks Noisy Labels Bootstrapping Scott Reed Honglak Lee Dragomir Anguelov Christian Szegedy Dumitru Erhan Andrew Rabinovich 10 Stability Deep Networks Raja Giryes Guillermo Sapiro Alex Bronstein 11 Audio source separation Discriminative Scattering Networks Joan Bruna Yann LeCun Pablo Sprechmann 13 Simple Image Description Generator via Linear Phrase-Based Model Pedro Pinheiro R mi Lebret Ronan Collobert 15 Stochastic Descent Analysis Representation Learning Algorithms Richard Golden 16 Distinguishability Criteria Estimating Generative Models Ian Goodfellow 18 Embedding Word Similarity Neural Machine Translation Felix Hill Kyunghyun Cho Sebastien Jean Coline Devin Yoshua Bengio 20 Deep metric learning using Triplet network Elad Hoffer Nir Ailon 22 Understanding Minimum Probability Flow RBMs Various Kinds Dynamics Daniel Jiwoong Im Ethan Buchman Graham Taylor 23 Group Theoretic Perspective Unsupervised Deep Learning Arnab Paul Suresh Venkatasubramanian 24 Learning Longer Memory Recurrent Neural Networks Tomas Mikolov Armand Joulin Sumit Chopra Michael Mathieu Marc'Aurelio Ranzato 25 Inducing Semantic Representation Text Jointly Predicting Factorizing Relations Ivan Titov Ehsan Khoddam 27 NICE Non-linear Independent Components Estimation Laurent Dinh David Krueger Yoshua Bengio 28 Discovering Hidden Factors Variation Deep Networks Brian Cheung Jesse Livezey Arjun Bansal Bruno Olshausen 29 Tailoring Word Embeddings Bilexical Predictions Experimental Comparison Pranava Swaroop Madhyastha Xavier Carreras Ariadna Quattoni 30 Learning Vector Representations Hierarchical Label Spaces Jinseok Nam Johannes F rnkranz 31 Search Real Inductive Bias Role Implicit Regularization Deep Learning Behnam Neyshabur Ryota Tomioka Nathan Srebro 33 Algorithmic Robustness Semi-Supervised -Good Metric Learning Maria-Irina Nicolae Marc Sebban Amaury Habrard ric Gaussier Massih-Reza Amini 35 Real-World Font Recognition Using Deep Network Domain Adaptation Zhangyang Wang Jianchao Yang Hailin Jin Eli Shechtman Aseem Agarwala Jon Brandt Thomas Huang 36 Score Function Features Discriminative Learning Majid Janzamin Hanie Sedghi Anima Anandkumar 38 Parallel training DNNs Natural Gradient Parameter Averaging Daniel Povey Xioahui Zhang Sanjeev Khudanpur 40 Generative Model Deep Convolutional Learning Yunchen Pu Xin Yuan Lawrence Carin 41 Random Forests Hash Qiang Qiu Guillermo Sapiro Alex Bronstein 42 Provable Methods Training Neural Networks Sparse Connectivity Hanie Sedghi Anima Anandkumar 43 Visual Scene Representations sufficiency minimality invariance approximation deep convolutional networks Stefano Soatto Alessandro Chiuso 44 Deep learning Elastic Averaging SGD Sixin Zhang Anna Choromanska Yann LeCun 45 Example Selection Dictionary Learning Tomoki Tsuchida Garrison Cottrell 46 Permutohedral Lattice CNNs Martin Kiefel Varun Jampani Peter Gehler 47 Unsupervised Domain Adaptation Feature Embeddings Yi Yang Jacob Eisenstein 49 Weakly Supervised Multi-embeddings Learning Acoustic Models Gabriel Synnaeve Emmanuel Dupoux May 8 Workshop Poster Session Board Presentation 2 Learning Activation Functions Improve Deep Neural Networks Forest Agostinelli Matthew Hoffman Peter Sadowski Pierre Baldi 3 Restricted Boltzmann Machine Classification Hierarchical Correlated Prior Gang Chen Sargur Srihari 4 Learning Deep Structured Models Liang-Chieh Chen Alexander Schwing Alan Yuille Raquel Urtasun 5 N-gram-Based Low-Dimensional Representation Document Classification R mi Lebret Ronan Collobert 6 Low precision arithmetic deep learning Matthieu Courbariaux Yoshua Bengio Jean-Pierre David 7 Theano-based Large-Scale Visual Recognition Multiple GPUs Weiguang Ding Ruoyan Wang Fei Mao Graham Taylor 8 Improving zero-shot learning mitigating hubness problem Georgiana Dinu Marco Baroni 9 Incorporating Distributional Relational Semantics Word Representations Daniel Fried Kevin Duh 10 Variational Recurrent Auto-Encoders Otto Fabius Joost van Amersfoort 11 Learning Compact Convolutional Neural Networks Nested Dropout Chelsea Finn Lisa Anne Hendricks Trevor Darrell 13 Compact Part-Based Image Representations Extremal Competition Overgeneralization Marc Goessling Yali Amit 15 Unsupervised Feature Learning Temporal Data Ross Goroshin Joan Bruna Jonathan Tompson David Eigen Yann LeCun 16 Classifier Hierarchical Topographical Maps Internal Representation Pitoyo Hartono Paul Hollensen Thomas Trappenberg 18 Entity-Augmented Distributional Semantics Discourse Relations Yangfeng Ji Jacob Eisenstein 20 Flattened Convolutional Neural Networks Feedforward Acceleration Jonghoon Jin Aysegul Dundar Eugenio Culurciello 22 Gradual Training Method Denoising Auto Encoders Alexander Kalmanovich Gal Chechik 23 Deep Gaze Boosting Saliency Prediction Feature Maps Trained ImageNet Matthias K mmerer Lucas Theis Matthias Bethge 24 Difference Target Propagation Dong-Hyun Lee Saizheng Zhang Asja Fischer Antoine Biard Yoshua Bengio 25 Predictive encoding contextual relationships perceptual inference interpolation prediction Mingmin Zhao Chengxu Zhuang Yizhou Wang Tai Sing Lee 27 Purine Bi-Graph based deep learning framework Min Lin Shuo Li Xuan Luo Shuicheng Yan 28 Pixel-wise Deep Learning Contour Detection Jyh-Jing Hwang Tyng-Luh Liu 29 Ensemble Generative Discriminative Techniques Sentiment Analysis Movie Reviews Gr goire Mesnil Tomas Mikolov Marc'Aurelio Ranzato Yoshua Bengio 30 Fast Label Embeddings Extremely Large Output Spaces Paul Mineiro Nikos Karampatziakis 31 Analysis Unsupervised Pre-training Light Recent Advances Tom Paine Pooya Khorrami Wei Han Thomas Huang 33 Fully Convolutional Multi-Class Multiple Instance Learning Deepak Pathak Evan Shelhamer Jonathan Long Trevor Darrell 35 Deep CNNs Learn Objects Xingchao Peng Baochen Sun Karim Ali Kate Saenko 36 Representation using Weyl Transform Qiang Qiu Andrew Thompson Robert Calderbank Guillermo Sapiro 38 Denoising autoencoder modulated lateral connections learns invariant representations natural images Antti Rasmus Harri Valpola Tapani Raiko 40 Towards Deep Neural Network Architectures Robust Adversarial Examples Shixiang Gu Luca Rigazio 41 Explorations high dimensional landscapes Levent Sagun Ugur Guney Yann LeCun 42 Generative Class-conditional Autoencoders Jan Rudy Graham Taylor 43 Attention Fine-Grained Categorization Pierre Sermanet Andrea Frome Esteban Real 44 Baseline Visual Instance Retrieval Deep Convolutional Networks Ali Sharif Razavian Josephine Sullivan Atsuto Maki Stefan Carlsson 45 Visual Scene Representation Scaling Occlusion Stefano Soatto Jingming Dong Nikolaos Karianakis 46 Deep networks large output spaces Sudheendra Vijayanarasimhan Jon Shlens Jay Yagnik Rajat Monga 47 Efficient Exact Gradient Update training Deep Networks Large Sparse Targets Pascal Vincent 49 Self-informed neural network structure learning David Warde-Farley Andrew Rabinovich Dragomir Anguelov Presentation Guidelines Conference Orals oral 20-minute time slot Please prepare 15 minutes material plan use last 5 minutes questions switching speakers Poster Presentations poster boards 4' high x 8' wide 120 cm high X 240 cm wide iclr2015 main txt Last modified 2015 05 18 20 57 admin Drupal Garland Theme Dokuwiki"),
('Image Scaling using Deep Convolutional Neural Networks', 'Image Scaling using Deep Convolutional Neural Networks Norman Tasfi May 06 2015 past summer interned Flipboard Palo Alto California worked machine learning based problems one Image Upscaling post show preliminary results discuss model possible applications Flipboard products High quality print-like finish play key role Flipboard design language want users enjoy consistent beautiful experience throughout Flipboard content custom print magazine hand Providing experience consistently difficult Different factors image quality deeply affect overall quality presented content Image quality varies greatly depending image source varying image quality especially apparent magazines display images across whole page full bleed format display images either web mobile devices must certain threshold display well receive large image web product create breathtaking full bleed sections Full bleed High Quality Image Lower resolution images introduce pixelation smoothing artifacts scaled 100 especially apparent full bleed presentation seen severely reduces quality presentation products Full bleed Low Quality Image cause general need image size X required size Y must run scaling algorithm algorithm performs mathematical operation scale image pixels desired size Y possible algorithms bicubic bilinear nearest-neighbor interpolation Many algorithms listed perform interpolation pixel values create transition algorithms use surrounding pixels guess missing values new image problem case scaling image larger size many new values filled image algorithms try make guesses new values introduces errors process leads noise haloing artifacts go would like give high level introduction traditional convolutional flavours Neural Networks good grasp feel free skip ahead next section Following introduction Neural Networks preliminary results section discussion model architectures design decisions applications Note Smaller nuances Neural Networks covered introduction Neural Networks Neural Networks amazing type model able learn given data large breadth applications enjoyed recent resurgence popularity many domains computer vision audio recognition natural language processing recent feats include captioning images playing Atari aiding self-driving cars language translation Neural Networks exist different configurations convolutional recurrent good different types tasks Learning modes also exist supervised unsupervised focus supervised learning Supervised learning described network trained input output mode used predict new outputs given input example would training network thousands pictures cats dogs manually labelled asking new picture cat dog structural level Neural Network feedforward graph nodes known units performs nonlinear operation incoming inputs inputs weight network able learn algorithm known backpropagation Basic Neural Network Source Wikipedia structure Neural Network flexible based task hand free customize network selecting attributes number hidden layers blue nodes number units per layer number connections per unit etc attributes known hyperparameters describe model structure behaviour Selecting parameters correctly critical achieving high performance Hyperparameters usually chosen random grid search optimization algorithms bayesian gradient based simple trial error mentioned previously units within network perform mathematical operation inputs take closer look calculating simple numerical example involving single unit handful inputs simple model unit takes three inputs scalar bias term shown inputs weightings referred weights express importance incoming input bias term allows us offset weighted value left right example use Rectified Linear ReL function units mathematical operation formally known activation function expressed gist activation function input value zero considered zero anything remains output range activation functions also used sigmoid hyperbolic tangent maxout calculate output value need compute first step compute vector product transpose defined bias value added result finally passed activation function make example little concrete use random numbers Say following vectors corresponding weights inputs scalar bias value Going step step calculate result bias value added take quantity pass activation function per function definition see x greater zero get value x case 3 54 exciting Well terms single unit exciting stands tweak weights bias value model basic functions little example lacks expressive power order increase expressive power chain link units together form larger networks seen bigger network equations network weight one unit another follows expression calculated earlier See chained together create complicated network applying calculation use output previous layer units compute next values propagated network arrive final layer call process forward propagation process little use us need change output network makes adjustments weights biases update weights biases network use algorithm known backpropagation focus supervised approach network given pairs data input x desired output y order use backpropagation supervised manner need quantify network performance must define error compares result forward propagation network estimated value desired value comparison values formally known cost function defined however want post use mean squared error function function gives us magnitude error network given randomly initialized weights biases output far desired y value causing large output Backpropagation takes magnitude propagates back network front adjusting weights biases along way amount adjustment weight bias thought contribution error calculated gradient descent algorithm seeks minimize error function changing weights biases general process train Neural Network given input vector x expected output y follows Perform step forward propagation network input vector x calculate output Calculate error error function Backpropagate errors network updating weights biases steps repeated different xand y pairs weights biases network give us minimum error possible minimizing function Convolutional Neural Networks paying attention recent tech articles likely heard Neural Networks breaking state-of-the-art several domains breakthroughs due small part convolutional Neural Networks Convolutional Neural Networks convnets slightly different flavour typical feed-forward Neural Network Convnets take biological inspiration visual cortex contains small regions cells sensitive subregions visual field referred receptive field mimic small subfield learning weights form matrices referred kernels like biologically inspired counterparts sensitive similar subregions image require way express similarity kernel subregion Since convolution operation essentially returns measure similarity two signals happily pass learned kernel along subregion image operation measure similarity returned back animation kernel yellow convolved image green result operation right red Animation Convolution Operation Source Feature extraction using convolution Stanford Deep Learning illustrate let run simple square kernel detects directional edges image weights kernel Applying convolutional operation using kernel several times left image get image right Animation Convolution Operation Source Edge Detection Matrix Convolution useful context Neural Networks several reasons exist allows us extract information right image kernels inform us presence location directional edges used Neural Network learn appropriate weight settings kernels extract basic features edges gradients blurs network deep enough convolutional layers start learning feature combinations previous layers simple building blocks edges gradients blurs become things like eyes noses hair later layers Kernels building high level representations earlier layers Source Yann Lecun ICML 2013 tutorial Deep Learning Traditionally wanted work image audio domains would perform feature generation using algorithms would preprocess data usable form would allow machine learning algorithm choice make sense incoming data process tedious fragile applying convolutional frontend networks able let algorithm minimal preprocessing data side create features work best specific domain situation network feature extraction learned features network work better algorithmically hand engineered features Image Scaling using Convolutional Neural Networks collection preliminary results produced model left image original high resolution ground truth would hope get perfect reconstruction scale original factor 2x send either bicubic scaling algorithm model results center right positions respectively Original Bicubic Model major differences seen hairline eyebrows skin cheek forehead Original Bicubic Model model good job along hard edges balcony sun tanning beds foreground Original Bicubic Model one difficult see first glance finer details hair along side ear inside ear present model Original Bicubic Model image related work complete unless Lenna included way Observe sharpness feathers noise lips eyes pattern hat also shows better model output Original Bicubic Model major differences versions tree leaves shadow shapes tree textures Architecture one architectures used primary goal double number pixels taken image architecture 8 layer Neural Network composed three convolutional layers shown stacked pinkish blocks four fully connected layers colored blue layer uses rectified linear activation function final dense layer linear Gaussian units shown small section input image ingested first convolutional layer image collected larger image using square sliding window first convolutional contains largest number filter maps outputs reprojected higher dimensionality first two dense fully-connected layers output processed next two convolutional layers Features two convolutional layers fed series fully-connected layers final output image calculated linear Gaussian layer pooling operations used convolutional layers pooling useful classification tasks invariance input important location features detected kernel important Pooling also discards much useful information opposite needed use case weights initialized using Xavier initialization suggested Glorot Benigo slightly tweaked hyperparameter optimzation defined sampled normal distribution biases initialized zero Dataset network trained large dataset approximately 3 million samples dataset images used natural images including animals outdoor scenes images needed filtered set included illustrations animals text image varying size quality constraint form pixel count added focus images contained total pixel count 640 000 sample within dataset low high resolution image pair low resolution image x input created downscaling high resolution image certain factor desired output y original high resolution image mild noise distortions added input data data normalized zero mean calculating global mean unit variance dividing standard deviation dataset dataset divided subsets training testing validation following 80 10 10 split respectively Regularization Max Norm Max norm constraints enforce absolute upper bound weight vectors magnitude every unit layer stops network weights exploding gradient update large Max norm constraints used layers except final linear Gaussian layer aggressive magnitude used convolutional layers layers magnitudes much lax L2 L2 regularization penalizes network using large weight vectors set parameter known regularization strength added cost function term optimization algorithm tries keep weights small minimizing cost function convolutional layers first two densely connected layers mild regularization applied densely connected layers use stronger value Dropout Dropout randomly drops units layer training step creating sub-architectures within model viewed type sampling smaller network within larger network weights included units updated makes sure network become reliant units process removing units happens runs units included change convolutional layers high inclusion probability almost 1 0 last two fully connected layers include half units Training model trained using Stochastic Gradient descent batch sizes 250 entire training set 250 epochs highish batchsize used smooth updates make better use GPUs still getting benefit perturbations smaller batches network trained minimize mean square error function learning rate scale used weights biases formula weights per layer current layer position total number layers scaled learning rate helped earlier layers converge biases learning rate multiplier 2 0 Nesterov momentum used initial value 0 15 increased 0 7 45 epochs Amazon g2 2xlarge EC2 instances used train network NVIDIA cuDNN library added speed training Training final model took approximately 19 hours Hyperparameters majority hyperparameters selected using inhouse hyperparameter optimization library works clusters Amazon g2 2xlarge instances performed using portion training dataset validation dataset process took roughly 4 weeks evaluated 500 different configurations Variations things work well working problem Used larger batch size 1000 worked well ran local minima quickly jitter provided smaller batch useful bounce minimas Used small convolutional network alright generalize well larger convolutional network Tried use weight initialization formula suggested et al Unfortunately caused network sputter around failed learn Might specific configuration many people successfully used Used amount L2 regularization layers worked much better vary L2 regularization based layers started saturating clamped max normal constraints Used pooling layers Lost much information layers images turned grainy poor looking biggest lesson learned dealing larger networks important getting weight initializations right feel aspect hyperparameters chosen largest impact well model train good idea spend time researching different initialization techniques understand impact model many papers machine learning libraries different initialization schemes easily learn Applications goal remove replace need upscaling algorithms bicubic upscaling try improve quality using different technology avenues primary use case scale lower resolution images higher resolution images available happens occasionally across platforms Besides primary use case still images technique applied different media formats GIFs GIF could split separate frames scaled repackaged final use case thought saving bandwidth smaller image could sent client would run client side version model gain larger image could accomplished using custom solution one javascript implementations neural networks available ConvNetJS steps feel problem space lot potential many things try including wild ideas Larger filter sizes convolutional layers Try layers data Try different color channel formats instead RGB Try using hundreds filters first convolutional layer sample using dropout small inclusion probability try tweaking learning rate layer Ditch fully connected layers try using convolutional layers Curious distillation would work problem Might help create lighter version run client devices easily Look small large make network quality starts degrading Conclusion Pursuing high fidelity presentation difficult endeavor takes exceeding amount effort squeeze final percentage points quality constantly reflecting product see percentage points come even seem obvious possible first wont place everywhere within product feel good cursory step forward improving quality hope enjoyed reading post taken something interesting away would like thank everyone Flipboard outstanding internship experience learnt lot met many awesome people gained invaluable experience process machine learning large datasets working great people interesting projects excites feel free apply hiring Feel free follow twitter normantasfi Special thanks Charles Emil Anh Mike Klaas Michael Johnston suggestions edits throughout process Shoutout Greg always making time help server setups questions Add Flipboard Magazine'),
("Have TEDs 2,000 Presenters Answer Your Deepest, Probing Questions With Help From IBM's Watson", "Fast Company Exist Design Create Video Features Emails Issues Subscribe Find Us Facebook Twitter Sign Co Exist 2015-05-05 Co Exist TED 2 000 Presenters Answer Deepest Probing Questions Help IBM's Watson new discovery tool creates super mashup mix tape TED speakers expounding term might want search relationship money happiness Emily McManus editor TED com tell answer age-old question long time based hundreds TED talks watched since 2007 could point relevant takes world top thinkers However 2 000 talks posted date even longer seen every single one one human McManus says help IBM Watson conversationally-conversant AI computer system famously won Jeopardy 2011 eight weeks IBM team created unique tool exploring TED body work demonstrate Watson developers conference it's holding New York City May 5 asked question program surfaces TED talks touch relevant concepts cuts helpfully exact section talk search result one cut played another different talks something like super mashup TED speakers taking whatever deep questions society universe strike fancy also timeline shows concepts come talk data visualization maps connectedness talks pick ideas emerge says Jeffrey Coveyduc IBM director advanced cognitive technology like rolling discovery tool concept-driven topics come never explicitly mentioned notes Another perhaps even interesting feature Watson also sentiment psycholinguistic analysis speaker talk talk given global health expert Hans Rosling ignorant world example summarizes presence heartfelt tranquil sounds right TED notes presentation relative small emotional range high degree openness feature could eventually analyze speaker traits across broad spectrum presenters surface unexpected connections researchers say Click enlarge TED program tested trained small group users could available publicly soon summer demo IBM Watson office posed software question relationship income inequality crime surfaced interesting talks TED library obvious others example pointed sections Rosling talk mentioned well Richard Wilkinson economic inequality harms societies Andrew McAfee future jobs looks like Loretta Napoleoni intricate economics terrorism latter talk gets terror networks target poor McManus says good pick wouldn thought Like Jeopardy matchup Watson's sometimes dubious attempts cooking TED program partly meant help IBM introduce Watson world non-threatening way Last year company launched 1 billion business unit dedicated commercializing software courting app developers hospitals universities retailers many sectors jump onto calls cognitive computing revolution announced number new apps partnerships World Watson event today says tens thousands developers platform already IBM's company developing kind emerging technology among pursuing aggressively search tool idea Watson would serve smart collaborator intelligent assistant help people make sense ever-growing amount information available people's brains possibly process Click enlarge TED demo also serves early foray Watson exploring video rather text images audio Back Watson played Jeopardy Alex Trebek refrain video clues software capable researchers say develop TED tool team processed TED existing transcripts talks used Watson speech-to-text technology add detailed information video audio become important web take MOOCs example kind work needed Watson wants valuable tool that's worth paying Sometimes challenging actually find ideas want within body information says Kai Young IBM program director led prototyping team proliferation video dominant form data web problem going grow Top Photo Steven Rosenbaum Getty Images Jessica Leber Jessica Leber Co Exist Assistant Editor Previously business reporter MIT Technology Review also staff writer ClimateWire Change org Continued Google Twitter May 5 2015 4 00 PM Add New Comment Sign Submit 0 Comments Trending Happening World Changing Ideas 2015 Universal Basic Income Bipartisan Solution Poverty We've Waiting Treating Low-Wage Workers Well Become Hot New Business Strategy Don't Relax Uncomfortability New Convenience Inside Satellite Detective Agencies Catch Companies Destroying Planet--From Space Advertise Privacy Policy Terms Us Fast Company Inc 2015 Mansueto Ventures LLC"),
('Parallel Machine Learning with Hogwild!', "Toggle navigation Products GraphLab Create Download Pricing Uses Industry Application Learn Gallery How-To Translator API Documentation User Guide C SDK beta Forum Events Upcoming Events Conference 2015 Company Press Team Investors Careers Contact Us Blog Download Menu Products GraphLab Create Download Pricing Uses Industry Application Learn Gallery How-To Translator API Documentation User Guide C SDK beta Forum Events Upcoming Events Conference 2015 Company Press Team Investors Careers Contact Us Blog Dato Blog Parallel Machine Learning Hogwild Posted Krishna Sridhar May 6 2015 7 00 00 Tweet Topics GraphLab Create Machine Learning Concepts Dato Technology Deep Dive Prior joining Dato fortunate actively involved University Wisconsin's numerical optimization group pushing envelope field machine learning collaboration database systems researchers group's research software asynchronous multi-core algorithms machine learning big data applications transformed way big data analytics done you've ever wondered meaning buzz words like Hogwild Chris rant omit DimmWitted ASYCD read blog post explain stochastic gradient descent SGD thread locking large effect performance attempt explain parallel algorithms machine learning Hogwild work transformed big data analytics GraphLab Create adopts techniques also actively pushes frontier parallel machine learning algorithms Learning Machine Learning Models Many machine learning problems logistic regression support vector machine matrix factorization formulated mathematical optimization problem problems given training data depicted blue figure goal learn parameters function depicted pink best fits data fitted learned function used make future predictions Figure 1 Learning prediction function using training data loss function used capture quality model based error predictions ground truth Typically away prediction ground truth larger penalty must pay goal model training minimize loss function e make sure total difference predictions ground truth measured loss function low possible process model fitting mathematically stated follows Gradient Descent Gradient descent simple popular algorithm used minimize common class loss functions following illustration explains intuition behind algorithm Figure 2 Performing gradient step minimizing function start random estimate solution typically random draw normal distribution make several iterations passes data pass update current estimate solution taking finite step along gradient slope function calculated point repeat process reach minimum e trough function length steps taken iteration small enough function minimized convex shaped like bowl gradient descent guaranteed minimize loss function finite number iterations One main disadvantages gradient descent requires many iterations reaching reasonably good solution Simple modifications gradient descent result algorithms L-BFGS acclerated gradient methods known accelerate model training reaching minimum using much fewer iterations However even accelerated versions gradient descent require several passes get good solution Stochastic Gradient Descent stochastic-gradient descent SGD algorithm variation gradient descent widely used applications like recommender systems deep learning well basic classification regression tasks algorithm described follows Algorithm 1 Model training using Stochastic Gradient Descent SGD iteration SGD draw random example training data perform gradient-descent-like update respect single example algorithm proposed way back 60 several decades rejected mainstream numerical optimization research simple requiring many iterations training data past years however resurgence usage SGD big data machine learning applications following reasons Small memory footprint Unlike many algorithms SGD doesn actually need data memory algorithm requires single training data point memory downside algorithm IO bound aggressive caching significantly speed algorithm Figure 3 Gradient Descent vs SGD Get reasonable solution quickly figure describes SGD popular algorithm machine learning SGD gets good solution quickly take forever get best solution turns problem machine learning applications getting best solution could fact result overfitting Although D SGD stands descent algorithm isn really descent algorithm theory guarantees loss function reduces makes iterations training data let us roll practice generally provides D however interesting theoretical results show caveats expectation algorithm converges minimum good true Well yes statements SGD true theory practice two key issues algorithm First inherently serial parameters theta must updated seeing every example next example needs wait update done calculate gradient Hence believed SGD difficult parallelize recently Second notoriously hard tune joke SGD mood swings would burst anger time Dato spent many months building auto-tuned SGD converges good solutions wide range datasets We'll write full technical report describing got teaser SGD Multiple Threads described earlier SGD inherently serial algorithm believed thread must wait another thread update making next move proposals make parallel weren successful Updates locking familiar parallel programming obvious way make algorithm parallel lock update step thread acquires lock current estimate solution updating parameter unlock update done algorithm described Algorithm 2 Parallel stochastic gradient descent locks many problems update step typically takes order microseconds acquiring lock take milliseconds words takes 1000x longer lock make update Spin locks tend work better mutex locks SGD still fast enough give real life analogy algorithm like team spends time planning synchronizing actual work Comment locks Hogwild SGD simple enough explained post-it note done Hogwild sentence main idea Hogwild - Remove thread locks parallel SGD code Hogwild threads overwrite writing time compute gradients using stale version current solution algorithm described follows Algorithm 3 Asynchronous stochastic gradient descent Hogwild Wait works Yes works without negative effect mathematical efficiency algorithm provides benefits multiple threads plot things work source Niu et al Hogwild started trend asynchronous algorithms model training Figure 3 Parallel Performance SGD source Niu et al Funny true story Hogwild Feng first author original Hogwild paper working trying get SGD go fast decided comment locking mechanism code maybe curiosity maybe debugging algorithm worked also happened 100x faster fact worked coincidence co-authors able prove lock-free madness fast also mathematically efficient Parallel machine learning trends ideas Hogwild extended several machine learning algorithms pattern parallelism works algorithms like stochastic coordinate descent useful solving SVMs randomized Kaczmarz algorithm solving systems linear equations even paper exploiting non-uniform memory access patterns multi-core server machines work pushed limits single machine something suitable truly scalable machine learning Dato strongly believe asynchronous e lock free algorithms key ingredient scalable machine learning platform Pushing Envelope GraphLab Create GraphLab Create adopted techniques also actively pushes frontier parallel machine learning algorithms advances we've made Hybrid-Locking explained removing locks parallel SGD speed algorithm lot one small catch practice vanilla Hogwild may work well problems variables updated frequently includes regularization terms model intercept terms linear models bias terms SVMs weights popular items recommender system example everyone watched Godfather parameter Godfather updated much frequently others Dato use hybrid-locking scheme e lock variables occur frequently update remaining infrequent variables lock-free enables us balance fast ensuring convergence problems datasets wide range characteristics Figure 4 Getting best worlds hybrid algorithms Hybrid-algorithms Stochastic algorithms get good solution fast may get best solution Deterministic algorithms like L-BFGS work well good starting point Dato actively working hybrid algorithms start stochastic algorithms like SGD switch deterministic algorithms like L-BFGS illustration depicts one get best worlds using hybrid algorithms hybrid locking hybrid batch stochastic algorithms secret sauce behind scale speed capabilities GraphLab Create We're constantly improving optimization engine learning algorithms favorite parallel distributed optimization algorithms Leave us comment Comments Subscribe Dato Blog notifications Recent Posts Evaluate Machine Learning Models Part 4 Hyperparameter Tuning Eliminating data pipeline glue code GraphLab Create's SDK Dato Lunch Work Perk Works Introducing Dato Predictive Services Deploying scikit-learn Models Dato Predictive Services Posts Topic Data Science 11 GraphLab Create 10 company news 7 Data Science Tools 6 Machine Learning Primer 5 Predictive Services 4 Big Data Analytics 3 Guest Post 3 Machine Learning Models 3 Dato Technology Deep Dive 2 Distributed Systems 2 GraphLab Canvas 2 GraphLab Conference 2 Machine Learning Concepts 2 Text Classification 2 Use Cases 2 Dato culture 1 Deep Learning 1 Fraud Detection 1 Graph Analysis 1 Machine Learning Workshop 1 Predictive Applications 1 Regression Analysis 1 Sentiment Analysis 1 see Posts Author Alice Zheng 10 Carlos Guestrin 6 Chris DuBois 5 Rajat Arya 4 Krishna Sridhar 3 Johnnie Konstantas 2 Michael Fire 2 Shawn Scully 2 Yucheng Low 2 Brian Kent 1 Danny Bickson 1 Gerard Cunningham 1 Jay Haijie Gu 1 Martin Kircher 1 Nick McClure 1 Piotr Teterwak 1 Robert Voyer 1 Sylvie Liberman 1 Timmy Wilson 1 Trey Causey 1 Zach Nation 1 see Contact Team Blog Careers Press Investors Connect Us Subscribe Machine Learning Blog Get data science insights best practices Dato team sent right inbox 2015 Dato Rights Reserved part website may reproduced without Dato's expressed consent Dato GraphLab GraphLab Create logos property Dato Privacy Policy Terms Use"),
('Your Prediction Gets As Good As Your Data [AI Optify]', "Home Services Contact Optimization Advanced Machine Learning Data Mining Online Advertising Services Prediction Gets Good Data May 5 2015 Kazem past seen software engineers data scientists assume keep increasing prediction accuracy improving machine learning algorithm want approach classification problem different angle recommend data scientists analyze distribution data first measure information level data approach givesus upper bound far one improve accuracy predictive algorithm make sure optimization efforts wasted Entropy Information information theory mathematician developed useful techniques entropy measure information level data process Let's think random coin head probability 1 one filps coin get information see head event since it's rare event compared tail likely happen formualte amount information random variable negative logarithm event probability captures described intuition Mathmatician also formulated another measure called entropy capture average information random process bits shown entropy formula discrete random variable first example let's assume coin P H 0 P 100 compute entropy coin follows second example let's consider coin P H 1 P 1-P H 99 Plugging numbers one find entropy coin Finally coin P H P 0 5 e fair coin entropy calculated follows Entropy Predictability examples tell us coin head probability zero coin's entropy zero meaning average information coin zero makes sense flipping coin always comes tail Thus prediction accuracy 100 words entropy zero maximum predictibility second example head probability zero still close zero makes coin predictable low entropy Finally last example 50 50 chance seeing head tail events maximizes entropy consequently minimizes predictability words one show fair coin meaximum entropy 1 bit making prediction good random guess Kullback Leibler Divergence last example show borrow ideas information theory measure distance two probability distributions Let's assume modeling two random processes pmf's P Q One employ entropy measure compute distance two pmf's follows distance function known KL Divergence measures distance Q distribution P's KL Divergence useful various applications NLP problems want measure distance distributions two documents e g modelled bag words Wrap-up post showed entropy information theory provides way measure much information exists given dataset also highlighted inverse relationship entropy predictability shows one use entropy calculate upper bound accuracy prediction problem hand Feel free share comments questions comment section also reach us info AIOptify com Previous Next Please enable JavaScript view comments powered Disqus Optify Optify Ad-Tech company specialized applying large scale machine learning algorithms programmatic ad buying selling B2B B2C clients Free Monthly Newsletter Featuring Articles Related Data Science Subscribe One email every month Easy unsubscribe spam Viewed Articles Crawling World Wild Web Online Advertising Algorithms Latent Dirichlet Allocation Entropy Predictability Google AdWords Optimization Predicting US Election Flu Predictor Social Media Twitter LinkedIn Back top"),
("IBM's supercomputer Watson will be used to make decisions about cancer care in 14 hospitals in the US and Canada, it has been announced.", "Accessibility links Skip content Accessibility Help BBC iD BBC navigation News News Sport Weather Shop Earth Travel Capital iPlayer Culture Autos Future TV Radio CBBC CBeebies Arts WW1 Food iWonder Bitesize Music Nature Earth Local Travel Menu Search BBC News navigation Sections Home Video World Asia UK Business Tech selected Science Magazine Entertainment Arts Health World News TV Pictures Also News Special Reports Explainers Reporters Say Technology Technology IBM's Watson supercomputer speed cancer care 6 May 2015 section Technology Watson sift medical data minutes compared weeks would take human IBM's supercomputer Watson used make decisions cancer care 14 hospitals US Canada announced Using computers trawl vast amounts medical data speeds diagnosis process system help assess individual tumours suggest drug used target Doctors welcomed new computer learn case examines dealing cancer always race said Dr Lukas Wartman assistant director cancer genomics McDonnell Genome Institute Washington University St Louis one signed use Watson system cancer patient know important genomic information Unfortunately translating cancer-sequencing results potential treatment options often takes weeks team experts study one patient's tumour provide results guide treatment decisions Watson appears help dramatically reduce timeline explained Pressing issue could alternatives standard treatments cancer people currently diagnosed cancer receive surgery chemotherapy radiation treatment genetic sequencing becomes increasingly accessible affordable patients starting benefit treatments target specific cancer-causing genetic mutations However process time-consuming - single patient's genome represents 100 gigabytes data - needs combined medical records journal studies information clinical trials would take clinician weeks analyse completed Watson minutes technology we're applying challenge brings power cognitive computing bear one urgent pressing issues time - fight cancer - way never possible explained Steve Harvey vice president IBM Watson Health According Mr Harvey Watson look actionable targets although acknowledged institutions genetic sequencing half cases come back something actionable Sometimes impossible identify main mutation cases targeted therapy currently exists collaborating IBM include Cleveland Clinic Fred Pamela Buffett Cancer Centre Omaha Yale Cancer Centre Eleven others join programme end 2015 pay undisclosed subscription fee IBM Corporate medicineThe link-up part increasingly close relationship medical community technology corporations Apple revealed week plans develop apps iPhone allow users take DNA tests may reveal diseases health conditions likely developIt also recently teamed IBM allow software helps gather health data iPhones used Watson IBM convinced Watson help change face healthcare even bigger ambitions cognitive computing platform Speaking IBM event week firm's chief executive Ginni Rometty made bold prediction technology saying future every decision mankind makes every decision going informed cognitive system like Watson result lives world going better Share story sharing Email Facebook Twitter WhatsApp Linkedin story IBM Apple want share others 14 April 2015 Video Milestone study probes cancer origin 14 August 2013 Related Internet links IBM Watson BBC responsible content external Internet sites Technology stories Top Stories Blatter denies scandal responsibility Fifa president Sepp Blatter says cannot held responsible current corruption scandal vows work earn back trust 28 May 2015 Putin makes troop deaths state secret 28 May 2015 Rinehart loses family trust battle 28 May 2015 Features Analysis Shockwaves 'fake diploma' scandal shaking Pakistan Welcome Cambodia Australia isn't telling unwanted refugees pulled trigger search truth death top prosecutor Mystery journeys weird circular joyrides hijacked Uber account Red card Palestinians press Fifa suspend Israel football Cat Dad Move Tiger Mother - there's new feline parenting style To-do list Nigeria's new leader end corruption without horse whip Ministry love state-run dating site reduce young divorce Iran News navigation Sections Home Video World World Home Africa Australia Europe Latin America Middle East US Canada Asia Asia Home China India UK UK Home England N Ireland Scotland Wales Politics Business Business Home Market Data Markets Economy Companies Entrepreneurship Technology Business Business Sport Knowledge economy Tech selected Science Magazine Entertainment Arts Health World News TV Pictures Also News Special Reports Explainers Reporters Say BBC News Services mobile connected TV Get news alerts Contact BBC News Explore BBC News News Sport Weather Shop Earth Travel Capital iPlayer Culture Autos Future TV Radio CBBC CBeebies Arts WW1 Food iWonder Bitesize Music Nature Earth Local Travel Terms Use BBC Privacy Policy Cookies Accessibility Help Parental Guidance Contact BBC Copyright 2015 BBC BBC responsible content external sites Read approach external linking"),
('The ELM Scandal, a formal complaint launched against Extreme Learning Machines', 'Home Register Examples FAQs News Contact Login 128 579 Anonymous Messages Sent Thursday May 28 2015 View Message Message Subject ELM Scandal Message IEEE copyrights ieee org IEEE SMC Exco Members Hi feel case investigated IEEE SMC Society SMC Society technical co-sponsor ICARCV 2014 possibly co-sponsor ICARCV 2004 well SMC Society published journal article IEEE SMC-Part B 2012 believe IEEE must consider non-IEEE publications also investigation non-IEEE publications cited IEEE publications reinforce misleading materials published non-IEEE sources requesting IEEE obtain views following researchers whose works abused inventor ELM Dr Guangbin Huang Dr Dave Lowe d lowe aston ac uk Dr Dave Broomhead david broomhead manchester ac uk Dr CLP Chen philipchen umac mo Dr J Suykens Johan Suykens esat kuleuven Dr R Duin r duin ieee org addition Profs L Wang ELPWANG NTU EDU SG DH Wang dh wang latrobe edu au also much familiar depth ethical violations committed Dr GB Huang firmly believe strong evidences provided email take appropriate actions systematic violation publishing ethics document must made available IEEE Fellows committees case Dr GB Huang nominated elevated March 2015 require clarifications please feel free contact us Thank look forward hearing near future Dave Chen Pao ---------------------------- Case Creation Creator ELM ELM Scandal ELM created 2004 certainly controversial name field artificial intelligence document presents numerous reasons taking appropriate actions ELM creator order ensure face similar situation future 1 ELM-RBF Case order get first ELM-RBF paper accepted 1st author creator Guang-Bin Huang knowingly excluded closely related publications closely related publication 6 known Huang excluded reference lists 11 12 states abstract random selection RBF centers training data performs poorly compared intelligent selection methods 11 12 Huang proposed totally randomly selecting RBF centers widths feature space independent training data certain proper citation relevant references proper experimental comparisons would resulted rejection paper due two important reasons 1 primary concepts completely randomly selecting center points training data presented inventors RBF 1988 3 page 325 footnote 1 Footnote 1 3 particular necessarily require radial basis function centers correspond data points closed-form pseudo-inverse solution 2 generalization randomization center points RBF known perform poorly 6 center points may located according density class labels training data Hence apparent order publish first two ELM-RBF publications first author knowingly excluded almost identical concepts 6 excluded known results stating randomization concepts would worse stated 6 abstract Please also note article 6 cited Huang et al 13 reference number 4 follows common learning algorithm radial basis function networks based first choosing randomly data points radial basis function centers using singular value decomposition solve weights network procedure several drawbacks particular arbitrary selection centers clearly unsatisfactory also well known first author widely published field RBF several journal publications listed Appendix page 8 2003-2007 period two ELM-RBF papers published 2004-2005 Hence first author must make claim never seen original papers Broomhead et al 3 preparing 2 papers 11 12 Even first author makes claim article 6 listed reference 4 publication 13 re-stated facts abstract two papers published conference poor quality out-of-the-field journal may use general reviewers specialized domain Hence author selected venues publish unethical works knowingly suppressing almost identical works superior quality apparent Huang ELM-RBF papers would certainly rejected relevant facts presented first two publications 11 12 Based evidences stern action needed case 2 ELM-Sigmoidal Case authors publish articles genuinely unaware closely related work discontinue activities facts made known domain ELM-Sigmoidal articles 9 30 32 examples cases authors stopped pursuing work domain However Huang unethically promoting ELM initially publishing selecting fast review process conference without citing relevant literatures either knowingly unknowingly ELM 14 differs RVFL 4 5 18 21 23 direct links input output ELM 14 differs RNN 27 bias output node inspection references 14 reveals absence closely related publications 4 5 18 21 23 27 Reviewers would certainly accepted ELM 14 relevant articles published 5 years 5 8 years 4 10 years 23 12 years 27 16 years 3 cited exact differences pointed experimental comparisons presented must take consideration terminologies hidden neurons weights biases connections activations defined mid 1980s 26 Therefore certain relevant previous articles cited experimentally compared correctly 2004 article would accepted Among closely related works 4 5 18 23 published top journals well captured numerous databases 2004 first ELM papers published internet-era must tolerate independent invention previously published concepts well captured several publications databases safely assume RNN 27 RVFL 4 5 18 21 23 likely perform either better good ELM ELM simplified version much older methods Hence based justification presented section appropriate actions taken IJCNN 2004 publication editing PDF 14 highlight ELM variant RNN RVFL 3 ELM-Kernel Case LS-SVM b 0 Naming algorithms also ethical issue instance converting inequality condition SVM 7 equality condition least squares formulation obtained variant appropriately named LS-SVM 28 acknowledge inventors SVM completely new names given minor variants substantial conceptual similarities existing works inventors existing works acknowledged future Avoiding confusion another reason naming algorithms showing past connections variants SVM named without showing relationship SVM would 1000s unrelated names variants SVM easy imagine consequences Fortunately researchers name variants giving due credits original inventors ELM-Kernel version exception ethical tradition adopted researchers Hence PDF IEEE SMC-B 2012 15 article edited show new name LS-SVM without Bias order correct ethical violation committed ELM-kernel inventor GB Huang 4 Continued Exclusion Closely Related Works Even though comment published Wang Wan 29 highlighted exclusion closely related works Broomhead et al 3 Chen et al 4 5 Huang excluded recent view publication 17 also reason simple excluded works almost identical ELM RBF Sigmoidal also reasonably certain performances excluded works likely either similar superior ELM versions minor variations introduced Huang simplified reducing degrees freedom relation parameter tuning flexibilities 5 Making Incorrect Statements inventor ELM frequently makes incorrect statements reply 16 claims following Lowe work universal approximation capability moves one step towards ELM direction impact factor b selected heuristically hidden nodes Lowe network incorrect according proof 25 assuming proof discredited yet different impact factors used stated page 248 255 25 inventor must asked present solid evidences making wrong claim 16 Please insert Fig 4 17 Fig 4 17 Cognitive Computation J 2014 figure wrongly identifies RVFL 23 iterative method pointed correctly 29 page 1494 right column figure wrongly identifies reference 31 Quicknet closed-form method figure excludes works Broomhead et al 3 Chen et al 4 5 almost identical ELM-RBF ELM-Sigmoidal respectively cases found Fig 4 appears recent publication 17 author claims RVFL iterative method wrong Pao 23 Chen 4 5 presented closed form solutions Wang Wan 29 informed ELM inventor details 2008 However ELM inventor wanted forget facts instead wanted convey wrong message followers ELM page 383 left column 17 Huang states RVFL uses conventional gradient descent method ELM tries avoid hand 23 page 167 following stated means unique minimum found N d iterations learning procedure conjugate gradient CG approach 11 12 explicit matrix inversion needs avoided matrix inversion use pseudo-inverse feasible single step learning would suffice reason giving preference iterative method 23 early 1990s iterative solutions preferred matrix inversion Intel 287 387 computers time would struggle perform matrix inversions beyond 10x10 ELM inventor identifies reference 31 Quicknet 1989 closed form solution wrong Quicknet name closed form solution first published 2006 32 Identifying RVFL 23 iterative method 17 excluding almost identical works Chen et al RVFL 4 5 even Wang Wan 29 correctly identified RVFL 23 closed form method shows clearly Huang intentionally knowingly making wrong statements mislead researchers Hence appropriate actions must taken stop activities 6 Making Experimentally Unsubstantiated Claims Inventor ELM frequently makes claims without solid experimental results example texts copied 17 demonstrate unscientific publishing activities ELM-inventor Page 385 kernels used ELM SVM LS-SVM SVM LS-SVM naturally lead sub-optimal solutions Page 385 dilemma may existed random methods biases output nodes 40 structure risks considered order improve generalization performance case Schmidt et al 40 would provide suboptimal solutions Ref 40 17 ref 27 document Huang author 17 must asked show extensive simulation results support claims Proofs sometimes wrong insignificant due assumptions etc Hence experimental results important Recent extensive comparisons 8 using 120 datasets 179 classifiers support claims inventor ELM 7 Reinventing Wheel Reinventing wheel taken place numerous occasions past credit inventing shared among different inventors one condition earlier inventors published inventions widely circulated public domain materials several examples previous century internet digital databases available Artificial neural network researchers aware reinvention back-propagation learning method early 1980s PDP research group 26 whereas similar work done Paul Werbos early 1970s Another example Kuhn-Tucker KT condition becoming Karush-Kuhn-Tucker KKT condition 20 unpublished century older invention 19 located important issue consider whether approve independent invention old concepts century old concepts published top journals well captured numerous databases obvious answer permit independent invention old concepts widely available public domain databases junior research would feel would good idea conduct literature search thoroughly using means declare independent inventors existing concepts well captured several databases ELM certainly falls category first ELM papers published 2004 11 12 14 even though highly similar papers 4 5 6 18 23 available several years prior 2004 many databases similar superior solution procedures Therefore stern actions must taken ELM creator order convey clear message junior researchers cannot become independent inventor existing concepts widely available digital database literature search thoroughly pretending know literature even clearly known Hence following message must conveyed junior researchers without ambiguity responsibility conduct thorough literature search possible reinvent wheel getting work published conference journal told others highly similar work literature well captured databases search engines ground claim independent inventor concept must accept fact done literature search sufficiently using possible synonyms so-called novel work must regarded novel Another strange observation relation ELM inventor labeling older methods published 1990s variants ELM published first 2004 really strange phenomenon never heard scientific community atrocious action must stopped strong message heard newer algorithms identified variants older methods correct way state facts would ELM-Sigmoidal RVFL 4 5 18 23 without direct links input output RNN 27 without bias output neuron Therefore researchers must asked use RVFL RNN instead ELM-Sigmoidal future publications 8 Unscientifically Promoting ELM Name inventor organizes numerous journal special issues ELM conference series etc publish ELM related works Even though inventor closely involved review process many occasions citations relevant publications experimental comparisons relevant works mostly non-existent offering SCI indexed publication option ELM inventor multiplied ELM publications even though likely either worse good older methods 3 4 5 18 21 23 activities misguided far many junior researchers resulted wastage resources taking appropriate action GB Huang junior researchers informed excluding closely related works literature review experimental comparisons tolerated 9 ELM Mean ELM presented researchers method non-iterative solution randomization However ELM-kernel 15 LS-SVM 28 b 0 without randomization Hence identifying ELM-versions 11 14 15 method domain randomization closed form solution procedure justified due existence ELM-kernel without randomization ELM represent randomization would linguistic invention synonym Moor-Penrose pseudo inverse ELM move state-of-the-art beyond existed proposed 2004 without citing relevant literatures ELM certainly taken research-publishing ethics principles beyond acceptable extreme 10 Correct Time Act Reinventing wheel taken place many times recent literature identified plagiarisms sufficient evidences obvious duplication materials cases authors reviewers genuinely failed locate previous works latter case need take action provided authors realize unsatisfactory literature reviewing subsequently accept original works case ELM published intentionally suppressing closely related works without offering experimental comparisons works fact remains experimentally compared determine significance minor variations inventor followers ELM insignificant published conference journal articles tiny differences explicitly stated first submissions tiny differences highlighted Wang Wan 29 Unfortunately inventor ELM continues exclude closely related references 17 comparing ELM fairly unrelated methods incorrectly comparing inventor offered experimental comparisons closely related methods decade clearly show unethical behavior inventor Due special circumstances ELM indeed required much longer time arrive conclusion Hence correct time act unethical publishing activities around ELM 11 Summary Based evidences presented Sections 1-9 clear inventor ELM GB Huang violating ethical publishing principles numerous ways evidences also clearly show inventor ELM conducted unethical activities systematic manner follows 1 Preparing articles 11 12 14 minor variation previously published methods 2 Submitting conferences poor quality journal without citing related previous works Obviously references correctly cited described even poor quality journal would accept works 3 publishing work inventor claims certainly true ELM-RBF case inventor knew relevant works know literature pointing minor variation inventor introduced first place 4 Instead accepting work almost literature reverting back older names ethically correct step taken almost researchers situation inventor promotes name 1 repeating tiny variations unethical manner 2 excluding identical works 3 excluding thorough experimental comparisons near identical works literature 4 making negative statements methods without solid experimental results 5 comparing apparently different methods 1 31 5 theories proofs derived neural network models universal approximation 10 wavelet 33 trigonometric 22 etc copied applied ELM claim ELM got lot theories RVFL RNN fact ELM tiny variant RVFL RNN trivial theories RVFL RNN well 6 Along way inventor makes either false claims claims without experimental support extensive datasets ELM superior others reason 7 Organize special issues conferences etc promote unethical research practices among junior researchers would bother read original works published 1988-1996 periods observations well supported evidences presented Sections 1-9 Hence correct time act elaborate ELM Scandal comparable scandal discussed 2 12 References 1 E Baum capabilities multilayer perceptrons Journal Complexity vol 4 193 215 1988 2 P K Bondyopadhyay Sir JC Bose diode detector received Marconi first transatlantic wireless signal December 1901 Italian Navy Coherer Scandal Revisited Proc IEEE Vol 86 1 259 285 1998 3 D Broomhead D Lowe Multivariable functional interpolation adaptive networks Complex Systems vol 2 321 355 1988 4 C L P Chen rapid supervised learning neural network function interpolation approximation IEEE Trans Neural Net 7 5 1220 1230 1996 5 C L P Chen J Z Wan rapid learning dynamic stepwise updating algorithm flat neural networks application time-series prediction IEEE Trans Systems Man Cybernetics Part B 29 1 62 72 1999 6 Chen C F Cowan P M Grant Orthogonal least squares learning algorithm radial basis function networks IEEE Trans Neural Networks 2 2 302 309 1991 7 C Cortes V Vapnik Support vector networks Machine Learning Vol 20 3 273 297 1995 8 M Fernandez-Delgado E Cernadas Barro D Amorim need hundreds classifiers solve real world classification problems Journal Machine Learning Research vol 15 1 3133 3181 2014 9 Ferrari R F Stengel Smooth function approximation using neural networks IEEE Trans Neural Networks Vol 16 1 24 38 2005 10 K Hornik Approximation capabilities multilayer feedforward networks Neural Networks vol 4 pp 251 257 1991 11 G -B Huang C -K Siew Extreme learning machine RBF network case Proc ICARCV 2004 pp 1029 1036 Int Conf Control Automation robotics Vision 12 G B Huang C K Siew Extreme learning machine randomly assigned RBF Kernels Int J Information Technology 11 1 16 24 2005 13 G -B Huang P Saratchandran N Sundararajan generalized growing pruning RBF GGAP-RBF neural network function approximation IEEE Trans Neural Networks 16 1 57 67 2005 14 G -B Huang Q -Y Zhu C -K Siew Extreme learning machine new learning scheme feedforward neural networks Proc IEEE Int Joint Conf Neural Networks Vol 2 2004 pp 985 990 15 G -B Huang H Zhou X Ding R Zhang Extreme learning machine regression multiclass classification IEEE Trans Systems Man Cybernetics Part B Cybernetics Vol 42 2 513 529 2012 16 G -B Huang Reply comments extreme learning machine Trans Neur Netw vol 19 8 pp 1495 1496 Aug 2008 17 G -B Huang insight extreme learning machines Random neurons random features kernels Cognitive Computation Vol 6 376 390 2014 18 B Igelnik Y H Pao Stochastic choice basis functions adaptive function approximation functional-link net IEEE Trans Neural Networks 6 6 1320 1329 1995 19 W Karush Minima functions several variables inequalities side constraints MS thesis Department Mathematics U Chicago 1939 20 H Kjeldsen contextualized historical analysis Kuhn Tucker theorem nonlinear programming impact World War II Historia Mathematica 27 4 331 361 2000 21 H Li C L P Chen H -P Huang Fuzzy neural intelligent systems Mathematical foundation applications engineering CRC Press 2000 22 N Y Nikolaev H Iba Polynomial harmonic GMDH learning networks time series modeling Neural Networks 16 2003 1527 1540 23 Y H Pao G H Park D J Sobajic Learning generalization characteristics random vector functional-link net Neurocomputing 6 2 163 180 1994 24 Y H Pao Y Takefuji Functional-link net computing IEEE Computer 25 5 76 79 1992 25 J Park W Sandberg Universal approximation using radial-basis function networks Neural Comput vol 3 2 pp 246 257 June 1991 26 D E Rumelhart J L McClelland PDP Research Group Eds Parallel Distributed Processing Explorations Microstructure Cognition Vol 1 Foundations MIT Press 1986 27 W F Schmidt M Kraaijveld R P W Duin Feedforward neural networks random weights Proc 11th IAPR Int Conf Pattern Recog Conf B Pattern Recognition Methodology Systems Vol 2 1992 pp 1 4 28 J Suykens J Vandewalle Least squares support vector machine classifiers Neural Processing Letters vol 9 3 293 300 1999 29 L P Wang C R Wan Comments extreme learning machine IEEE Trans Neural Networks Vol 19 8 1494 1495 2008 30 B Widrow Greenblatt Y Kim D Park no-prop algorithm new learning algorithm multilayer neural networks Neural Netw vol 37 182 188 Jan 2013 31 H White additional hidden unit test neglected nonlinearity multilayer feedforward networks Proc Int conf Neural Networks 1989 pp 451 455 32 H White Approximate nonlinear forecasting methods Handbook Economic Forecasting Elsevier 2006 pp 460 512 33 Q Zhang Benveniste Wavelet networks IEEE Trans Neural Networks vol 3 889 898 1992 13 Appendix appendix lists RBF journal papers co-authored ELM inventor period publishing ELM-RBF without citing relevant papers journal articles ELM inventor must working RBF starting 2002 earlier ELM-RBF concepts known since 1988 repeated several times RBF related papers simplest accurate RBF solution efficient sequential learning algorithm growing pruning RBF GAP-RBF networks Huang GB Saratchandran P Sundararajan N IEEE TRANSACTIONS SYSTEMS MAN CYBERNETICS PART B-CYBERNETICS Volume 34 Issue 6 Pages 2284-2292 Published DEC 2004 generalized growing pruning RBF GGAP-RBF neural network function approximation Huang GB Saratchandran P Sundararajan N IEEE TRANSACTIONS NEURAL NETWORKS Volume 16 Issue 1 Pages 57-67 Published JAN 2005 Performance evaluation GAP-RBF network channel equalization Li MB Huang GB Saratchandran P et al NEURAL PROCESSING LETTERS Volume 22 Issue 2 Pages 223-233 Published OCT 2005 Neuron selection RBF neural network classifier based data structure preserving criterion Mao KZ Huang GB IEEE TRANS NEURAL NETWORKS Vol 16 Issue 6 Pages 1531-1540 Published NOV 2005 Complex-valued growing pruning RBF neural networks communication channel equalisation Li M -B Huang G -B Saratchandran P et al IEE PROC-VISION IMAGE SIGNAL PROCESSING Vol 153 4 pp 411-418 AUG 2006 Improved GAP-RBF network classification problems Zhang Runxuan Huang Guang-Bin Sundararajan N et al NEUROCOMPUTING Volume 70 Issue 16-18 Pages 3011-3018 Published OCT 2007 Learn TheAnonymousEmail com Message TheAnonymousEmail com tell world feel anonymously www DumpOnYou com Register free today Copyright 2015 TheAnonymousEmail com Privacy Policy'),
("Published ELM overview, 'Trends in extreme learning machines: A review'. Make your own opinion! :)", 'PDF-1 7 2590 0 obj endobj 2751 0 obj Filter FlateDecode ID Index 2590 566 Info 2589 0 R Length 413 Prev 770111 Root 2591 0 R Size 3156 Type XRef W 1 3 1 stream h KA g'),
('Reinforcement Learning Neural Turing Machines', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1505 00521 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF PostScript formats Current browse context cs LG prev next new recent 1505Change browse cs References CitationsNASA ADS 1 blog link Bookmark Computer Science Learning Title Reinforcement Learning Neural Turing Machines Authors Wojciech Zaremba Ilya Sutskever Submitted 4 May 2015 Abstract expressive power machine learning model closely related number sequential computational steps learn example Deep Neural Networks successful shallow networks perform greater number sequential computational steps highly parallel Neural Turing Machine NTM model compactly express even greater number sequential computational steps even powerful DNN memory addressing operations designed differentiable thus NTM trained backpropagation differentiable memory relatively easy implement train necessitates accessing entire memory content computational step makes difficult implement fast NTM work use Reinforce algorithm learn access memory using backpropagation learn write memory call model RL-NTM Reinforce allows model access constant number memory cells computational step implementation faster RL-NTM first model principle learn programs unbounded running time successfully trained RL-NTM solve number algorithmic tasks simpler ones solvable fully differentiable NTM RL-NTM fairly intricate model needed method verifying correctness implementation developed simple technique numerically checking arbitrary implementations models use Reinforce may independent interest Subjects Learning cs LG Cite arXiv 1505 00521 cs LG arXiv 1505 00521v1 cs LG version Submission history Ilya Sutskever view email v1 Mon 4 May 2015 04 14 54 GMT 40kb authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('Recursive Neural Networks Can Learn Logical Semantics', "Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1406 1827 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context cs CL prev next new recent 1406Change browse cs cs LG cs NE References CitationsNASA ADS DBLP - CS Bibliography listing bibtex Samuel R Bowman Christopher Potts Christopher D Manning Bookmark Computer Science Computation Language Title Recursive Neural Networks Learn Logical Semantics Authors Samuel R Bowman Christopher Potts Christopher D Manning Submitted 6 Jun 2014 v1 last revised 14 May 2015 version v4 Abstract Tree-structured recursive neural networks TreeRNNs sentence meaning successful many applications remains open question whether fixed-length representations learn support tasks demanding logical deduction pursue question evaluating whether two models---plain TreeRNNs tree-structured neural tensor networks TreeRNTNs ---can correctly learn identify logical relationships entailment contradiction using representations first set experiments generate artificial data logical grammar use evaluate models' ability learn handle basic relational reasoning recursive structures quantification evaluate models natural SICK challenge data models perform competitively SICK data generalize well three experiments simulated data suggesting learn suitable representations logical inference natural language Subjects Computation Language cs CL Learning cs LG Neural Evolutionary Computing cs NE Cite arXiv 1406 1827 cs CL arXiv 1406 1827v4 cs CL version Submission history Samuel Bowman view email v1 Fri 6 Jun 2014 22 09 27 GMT 112kb v2 Sun 14 Dec 2014 20 37 33 GMT 153kb v3 Tue 3 Mar 2015 19 48 45 GMT 102kb D v4 Thu 14 May 2015 19 37 38 GMT 119kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact"),
('Deep Learning for Decision Making and Control', "Skip navigation UploadSign inSearch Loading Close Yeah keep Undo Close video unavailable Watch QueueTV QueueWatch QueueTV Queue Remove allDisconnect Loading Watch Queue TV Queue __count__ __total__ Deep Learning Decision Making Control UWTV SubscribeSubscribedUnsubscribe8 598 Subscription preferences Loading Loading Working Add Want watch later Sign add video playlist Sign Share Report Need report video Sign report inappropriate content Sign Statistics 9 126 104 Like video Sign make opinion count Sign 105 1 Don't like video Sign make opinion count Sign 2 Loading Loading Loading Rating available video rented feature available right Please try later Published Mar 18 2015A remarkable feature human animal intelligence ability autonomously acquire new behaviors research concerned designing algorithms aim bring ability robots simulated characters Levine describe class guided policy search algorithms tackle challenge transforming task learning control policies supervised learning problem supervision provided simple efficient trajectory-centric methods Sergey Levine postdoctoral researcher working Professor Pieter Abbeel UC Berkeley03 10 2015https www cs washington edu htbin-p http uwtv org Category Education License Standard YouTube License Show Show less Loading Autoplay autoplay enabled suggested video automatically play next Next wonderful terrifying implications computers learn Jeremy Howard TEDxBrussels - Duration 19 47 TEDx Talks 66 647 views 19 47 Play nextPlay Large-Scale Deep Learning Building Intelligent Computer Systems - Duration 59 40 UWTV 961 views 59 40 Play nextPlay Shooting Hoops Saving Lives - Duration 4 24 UWTV 515 views 4 24 Play nextPlay UW 360 Season 5 Episode 6 - Duration 22 39 UWTV 222 views 22 39 Play nextPlay New Tennis Coach - Duration 3 09 UWTV 79 views 3 09 Play nextPlay Nobel Laureate Daniel Kahneman Rational Decision Making Society - Duration 28 56 ImagE Native 4 421 views 28 56 Play nextPlay Jeremy Howard wonderful terrifying implications computers learn - Duration 19 50 TED 134 321 views 19 50 Play nextPlay Deep Learning - GMIC SV 2014 Day 2 - Duration 18 19 GMIConference 4 990 views 18 19 Play nextPlay Confidence-driven decision-making Peter Atwater TEDxWilmington - Duration 14 39 TEDx Talks 11 014 views 14 39 Play nextPlay 007 Machine learning best practices we've learned hundreds competitions - Ben Hamner - Duration 51 08 11 793 views 51 08 Play nextPlay Andrew Ng Deep Learning Self-Taught Learning Unsupervised Feature Learning - Duration 45 47 133 063 views 45 47 Play nextPlay Andrew Ng Baidu Stanford Coursera advances deep learning - Duration 36 56 gigaom 5 053 views 36 56 Play nextPlay Xavi Hernandez Analyzing Xavi's Game Control - Duration 15 44 FadilFcbChannel 782 946 views 15 44 Play nextPlay Andrew Ng Challenges Deep Learning - Duration 6 39 Milan Lajto 1 063 views 6 39 Play nextPlay Deep Learning AI Use Today - Duration 50 46 AlchemyAPI 3 634 views 50 46 Play nextPlay Geoff Hinton - Recent Developments Deep Learning - Duration 1 05 18 UBCCPSC 47 605 views 1 05 18 Play nextPlay Google Deep Learning - Duration 2 44 MIT Technology Review 15 562 views 2 44 Play nextPlay NIPS 2014 Workshop - Hinton Deep Learning Representation Learning - Duration 27 36 NeuralInformationPro 458 views 27 36 Play nextPlay Nature Human-level control deep reinforcement learning - Duration 2 35 KeSimpulan 3 204 views 2 35 Play nextPlay Bliss 29 - Strategy Successful Decision Making - BK Shivani Hindi - Duration 31 12 bkshivani 60 692 views 31 12 Play nextPlay Loading suggestions Show Language English Country India Safety History Help Loading Loading Loading Press Copyright Creators Advertise Developers YouTube Terms Privacy Policy Safety Send feedback Try something new Loading Working Sign add Watch Later Add Loading playlists"),
('A Recurrent Neural Network Based Alternative to Convolutional Networks', 'Cornell University Library gratefully acknowledge support fromthe Simons Foundation member institutions arXiv org cs arXiv 1505 00393 Search Article-id Help Advanced search papers Titles Authors Abstracts Full text Help pages Full-text links Download PDF formats Current browse context cs CV prev next new recent 1505Change browse cs References CitationsNASA ADS Bookmark Computer Science Computer Vision Pattern Recognition Title ReNet Recurrent Neural Network Based Alternative Convolutional Networks Authors Francesco Visin Kyle Kastner Kyunghyun Cho Matteo Matteucci Aaron Courville Yoshua Bengio Submitted 3 May 2015 Abstract paper propose deep neural network architecture object recognition based recurrent neural networks proposed network called ReNet replaces ubiquitous convolution pooling layer deep convolutional neural network four recurrent neural networks sweep horizontally vertically directions across image evaluate proposed ReNet three widely-used benchmark datasets MNIST CIFAR-10 SVHN result suggests ReNet viable alternative deep convolutional neural network investigation needed Subjects Computer Vision Pattern Recognition cs CV Cite arXiv 1505 00393 cs CV arXiv 1505 00393v1 cs CV version Submission history KyungHyun Cho view email v1 Sun 3 May 2015 04 58 53 GMT 797kb D authors paper endorsers Disable MathJax MathJax Link back arXiv form interface contact'),
('A Statistical View of Deep Learning (IV): Recurrent Nets and Dynamical Systems', "Spectator Shakir's Machine Learning Blog Machine Learning StatisticsA Statistical View Deep Learning IV Recurrent Nets Dynamical Systems Statistical View Deep Learning III Memory Kernels Statistical View Deep Learning V Generalisation Regularisation Statistical View Deep Learning IV Recurrent Nets Dynamical Systems 2 3 May 2015 Machine Learning Statistics Tags deep learning dynamical systems latent variable models Recurrent nets time-series Recurrent neural networks RNNs established one key tools machine learning toolbox handling large-scale sequence data ability specify highly powerful models advances stochastic gradient descent availability large volumes data large-scale computing infrastructure allows us apply RNNs creative ways handwriting generation image captioning language translation voice recognition RNNs routinely find part large-scale consumer products first encounter mystery surrounding models refer many different names recurrent networks deep learning state space models probabilistic modelling dynamical systems signal processing autonomous non-autonomous systems mathematics Since attempt solve problem descriptions inherently bound together many lessons exchanged particular lessons large-scale training deployment big data problems deep learning even powerful sequential models changepoint factorial switching state-space models post initial exploration connections Equivalent models recurrent networks state-space models Recurrent Neural Networks Recurrent networks 1 take functional viewpoint sequence modelling describe sequence data using function built using recursive components use feedback hidden units time points past inform computations sequence present obtain neural network activations one hidden layers feeds back network along input see figures recursive description unbounded practically use model unfold network time explicitly represent fixed number recurrent connections transforms model feedforward network familiar techniques applied consider observed sequence x describe loss function RNNs unfolded steps model corresponding loss function feedforward network d appropriate distance function data predicted squared loss difference standard feedforward networks parameters recursive function f time points e shared across model perform parameter estimation averaging mini-batch sequences using stochastic gradient descent application backpropagation algorithm recurrent networks combination unfolding time backpropagation referred backpropagation time BPTT 2 Since simplified task always considering learning algorithm application SGD backprop free focus energy creative specifications recursive function simplest common recurrent networks use feedback one past hidden layer earlier examples include Elman Jordan networks true workhorse current recurrent deep learning Long Short-Term Memory LSTM network 3 transition function LSTM produces two hidden vectors hidden layer h memory cell c applies function f composed soft-gating using sigmoid functions number weights biases e g B b Probabilistic dynamical systems also view recurrent network construction using probabilistic framework relying reasoning used part series Instead viewing recurrent network recursive function followed unfolding time steps directly model sequence length latent hidden dynamics specify probabilistic graphical model latent states h observed data x assumed probabilistic transition probability time equivalent assuming parameters transition function shared could refer models stochastic recurrent networks established convention refer dynamical systems state-space models probabilistic modelling core quantity interest probability observed sequence x computed follows Using maximum likelihood estimation obtain loss function based log marginal likelihood Since recurrent networks transition dynamics assumed deterministic easily recover RNN loss function recovers original loss function distance function given log chosen likelihood function surprise RNN loss corresponds maximum likelihood estimation deterministic dynamics machine learners never really trust data cases wish consider noisy observations stochastic transitions may also wish explore estimation beyond maximum likelihood great deal power obtained considering stochastic transitions transform recurrent networks probabilistic generative temporal models 4 5 models account missing data allow denoising built-in regularisation model sequence density gain new avenues creativity transitions consider states jump random times different operational modes might reset base state interact multiple sequences simultaneously hidden states h random faced problem inference certain assumptions discrete Gaussian transitions algorithms hidden Markov models Kalman filters respectively demonstrate ways done recent approaches use variational inference particle MCMC 4 general efficient inference large-scale state-space models remains active research area Prediction Filtering Smoothing Dynamical systems often described make three different types inference problems explicit prediction filtering smoothing 5 Prediction inferring future first use machine learning models seen training data asked forecast behaviour sequence point k time-steps future compute predictive distribution hidden state since knowing allows us predict generate would observed Filtering inferring present task computing marginal distribution hidden state given past states observations Smoothing inferring past task computing marginal distribution hidden state given knowledge past future observations operations neatly separate different types computations must performed correctly reason sequence random hidden states RNNs due deterministic nature computing predictive distributions filtering realised feedforward operations unfolded network Smoothing operation counterpart architectures bi-directional recurrent nets attempt fill role Summary Recurrent networks state space models attempt solve problem best reason sequential data continue research area intersection deterministic probabilistic approaches allow us exploit power temporal models Recurrent networks shown powerful scalable applicable incredibly diverse set problems also much teach terms initialisation stability issues gradient management implementation large-scale temporal models Probabilistic approaches much offer terms better regularisation different types sequences model wide range probabilistic queries make models sequence data much said initial connections make clear way forward References 1 Yoshua Bengio Ian Goodfellow Aaron Courville Deep Learning 2015 2 Paul J Werbos Backpropagation time Proceedings IEEE 1990 3 Felix Gers Long short-term memory recurrent neural networks 2011 4 David Barber Taylan Cemgil Silvia Chiappa Bayesian time series models 2011 5 Simo arkk Bayesian filtering smoothing 2013 Share Twitter 2 thoughts Statistical View Deep Learning IV Recurrent Nets Dynamical Systems Reply Roger May 5 2015 1 13 Really enjoyed reading post Thanks totally share view recurrent networks huge source inspiration make probabilistic models time series scalable useful interesting see come approximate inference methods don't penalise us much carrying extra burden probabilistic machinery particular quite worried learn model often probabilistic inference filtering smoothing large state spaces nonlinear dynamics step computationally expensive also hard know priori approximate inference method efficient robust problem hand Shameless plug recent paper Yutian Chen Carl Rasmussen learn nonlinear state-space models put Gaussian process prior state transition function Since using variational inference sparse GPs la Titsias model becomes parametric dissimilar recurrent neural network I'm currently exploring initialisation techniques borrowed RNNs useful think Achilles heel approach need perform smoothing part learning algorithm Reply Justin Bayer May 5 2015 8 11 actually know application bidirectional RNNs filtering smoothing probabilistic sense problem typical generative approach decompose p x_1 product p x_t x_1 t-1 already makes impossible incorporate information future could imagine alternating cascade forward backward passes time achieve know RNN simultaneously allows efficient exact filtering smoothing architecture parameters used bidirectional RNN recent Nips VI Workshop paper inference model SGVB actual graphical model Leave Reply Cancel Reply Author required Email published required Website Notify follow-up comments email Notify new posts email Statistical View Deep Learning III Memory Kernels Statistical View Deep Learning V Generalisation Regularisation Follow TwitterMy Tweets Recent Posts Chinese Edition Statistical View Deep Learning II Chinese Edition statistical View Deep Learning Statistical View Deep Learning V Generalisation Regularisation Statistical View Deep Learning IV Recurrent Nets Dynamical Systems Statistical View Deep Learning III Memory Kernels TagsAI auto-encoders bayes factor Bayesian brain deep learning density estimation dynamical systems GLM inference latent variable models marginal likelihood mcmc model selection NIPS papers Philosophy priors Recurrent nets regression regularisation sparsity statistics time-series variational inference WBIC Archives May 2015 April 2015 March 2015 January 2015 August 2013 April 2013 March 2013 Meta Log Entries RSS Comments RSS WordPress org 2015 Spectator Rights Reserved"),
('Simple and common clustering Problem. Need an answer', 'current community chat blog Cross Validated Cross Validated Meta communities Sign log customize list stack exchange communities Stack Exchange sign log tour help Tour Start quick overview site Help Center Detailed answers questions might Meta Discuss workings policies site Cross Validated Questions Tags Users Badges Unanswered Ask Question Page Found question removed Cross Validated reasons moderation Please refer help center possible explanations question might removed similar questions might relevant hierarchical clustering algorithm distance metric hierarchical clustering valid Hierarchical clustering possible combine single-linkage clustering average linkage clustering sub-optimality various hierarchical clustering methods assessed ranked Choosing number clusters hierarchical agglomerative clustering Agglomerative Hierarchical Clustering complete linkage opposed single linkage dendrogram Bisecting K-means using Dynamic Time Warping Hierarchical clustering methods using similarity metric d x x 0 possibly assymmetric Clustering reinforcement learning approach perform divisive hierarchical clustering Try Google search Try searching similar questions Browse recent questions Browse popular tags feel something missing contact us tour help blog chat data legal privacy policy work advertising info mobile contact us feedback Technology Life Arts Culture Recreation Science Stack Overflow Server Fault Super User Web Applications Ask Ubuntu Webmasters Game Development TeX - LaTeX Programmers Unix Linux Ask Different Apple WordPress Development Geographic Information Systems Electrical Engineering Android Enthusiasts Information Security Database Administrators Drupal Answers SharePoint User Experience Mathematica Salesforce 14 Photography Science Fiction Fantasy Graphic Design Seasoned Advice cooking Home Improvement Personal Finance Money Academia 10 English Language Usage Skeptics Mi Yodeya Judaism Travel Christianity Arqade gaming Bicycles Role-playing Games 21 Mathematics Cross Validated stats Theoretical Computer Science Physics MathOverflow 7 Stack Apps Meta Stack Exchange Area 51 Stack Overflow Careers site design logo 2015 stack exchange inc user contributions licensed cc by-sa 3 0 attribution required rev 2015 5 28 2614 Cross Validated works best JavaScript enabled'),
('The Great Convergence: Deep Learning, Compressive Sensing, Advanced Matrix Factorization (x-post: r/CompressiveSensing )', "Nuit Blanche name Igor Carron homepage Page Views Nuit Blanche since July 2010 Follow IgorCarron Cite Nuit Blanche related pages recent Compressive Sensing article Scientific Reports Attendant Project Page Please join comment Google Community 1502 CompressiveSensing subreddit 811 Facebook page LinkedIn Compressive Sensing group 3293 Advanced Matrix Factorization Group 1017 Reference pages include Big Picture Compressive Sensing Learning Compressive Sensing Advanced Matrix Factorization Jungle Page Highly Technical Reference Pages - Aggregators Technologies Exist CAI Cable Igor's Adventures Matrix Factorization search Reproducible Research page Paris Machine Learning Meetup Archives Meetup com register 2222 members LinkedIn post jobs 721 Google 233 Facebook follow-on discussions Twitter Monday May 04 2015 Great Convergence Deep Learning Compressive Sensing Advanced Matrix Factorization Email ThisBlogThis Share TwitterShare FacebookShare Pinterest Initially wondering linear sampling mechanism could twisted would simple solvers reconstruct initial set data looked complex reconstruction solvers heard deep learning using nonlinear sampling mechanism could fact well reconstructing initial sets data see Sunday Morning Insight Quick Panorama Sensing Direct Imaging Machine Learning want real sense great convergence currently happenning Science Engineering really want pay attention folks compressive sensing advanced matrix factorization fora little trying make sense neural network architectures Recently Rich Baraniuk told us Probabilistic Theory Deep Learning today people like Guillermo Sapiro Larry Carin looking architectures significant Deep Neural Networks Random Gaussian Weights Universal Classification Strategy Raja Giryes Guillermo Sapiro Alex M Bronstein Two important properties classification machinery system preserves important information input data ii training examples convey information unseen data iii system able treat differently points different classes work show fundamental properties inherited architecture deep neural networks formally prove networks random Gaussian weights perform distance-preserving embedding data special treatment in-class out-of-class data Similar points input network likely theoretical analysis deep networks presented exploits tools used compressed sensing dictionary learning literature thereby making formal connection important topics derived results allow drawing conclusions metric learning properties network relation structure provide bounds required size training set training examples would represent faithfully unseen data results validated state-of-the-art trained networks Generative Model Deep Convolutional Learning Yunchen Pu Xin Yuan Lawrence Carin generative model developed deep multi-layered convolutional dictionary learning novel probabilistic pooling operation integrated deep model yielding efficient bottom-up pretraining top-down refinement probabilistic learning Experimental results demonstrate powerful capabilities model learn multi-layer features images excellent classification results obtained MNIST Caltech 101 datasets Credit Photo Cropped processed single frame NAVCAM image Comet 67P C-G taken 15 April 2015 distance 165 km comet centre Credits ESA Rosetta NAVCAM CC BY-SA IGO 3 0 Join CompressiveSensing subreddit Google Community post Liked entry subscribe Nuit Blanche's feed there's came also subscribe Nuit Blanche Email explore Big Picture Compressive Sensing Matrix Factorization Jungle join conversations compressive sensing advanced matrix factorization calibration issues Linkedin labels CS MF ML TheGreatConvergence Igor 5 04 2015 10 30 00 comments Post Comment Newer Post Older Post Home Subscribe Post Comments Atom Printfriendly Nuit Blanche Stemming Data Tsunami One Algorithm Time Tweet Igor Search Nuit Blanche Loading Subscribe E-MAIL Nuit Blanche Get new entries directly mailbox enter email address Nuit Blanche Nuit Blanche blog focuses Compressive Sensing Advanced Matrix Factorization Techniques Machine Learning well many engaging ideas techniques needed handle make sense high dimensional data also known Big Data Nuit Blanche french expression translates nighter restless night Contact Cassini MSL Oppy HiRISE SOHO SDO Rosetta-Philae Rosetta Contact igorcarron gmail com Webon LinkedIn Twitter Pages Home Reproducible Research implementations Randomized Numerical Linear Algebra RandNLA Advanced Matrix Factorization Learning Compressed Sensing It's CAI Cable Igor's Adventures Matrix Factorization Machine Learning Meetups Around World Compressed Sensing Pages Focused Interest Pages Datasets Challenges Nuit Blanche Conversations Linking Nuit Blanche blogs CS Meetings Real Time Experiments Highly Technical Reference Pages - Aggregators Recent Nuit Blanche entries Paris Machine Learning Meetup Archives Pinterest Boards Imaging Nature Technologies Exist Wondering Star Computational Photography Subscribe LinkedIn Matrix Factorization Group 1001 members right one Link stats Subscribe Nuit Blanche RSS Feed Posts Atom Posts Comments Atom Comments Subscribe LinkedIn Compressive Sensing group 3145 members right one Link stats Google Badge updated profile LinkedIn reflect activities Nuit Blanche means provide recommendations based experience reading blog Nuit Blanche QR code Search Nuit Blanche LoadingLatest news Compressive Compressed Sensing Arxiv Full Text Search Arxiv Google Compressive Sensing Compressed Sensing 24 hours week month Rice University Compressive Sensing repositoryLatest news Matrix Factorization Arxiv old Arxiv new Google 24 hours week month Readership Statistics another set watching blog feedreaders 740 readers receive every entries mailboxes 600 people come site directly everyday detailed information following blog entries far site seen 3 500 000 pageviews since counter installed 2007 Nuit Blanche Referenced Dead Tree World Big Picture Compressive Sensing mentioned article La Recherche french speaking equivalent competitor Science October 2010 issue page 20-21 Wired Magazine piece Compressed Sensing featuring links blog Big Picture March 1 2010 Emmanuel Candes Terry Tao wrote Nuit Blanche Dec '08 issue IEEE Information Theory Society Newsletter Xiaochuan Pan Emil Sidky Michael Vannier wrote Nuit Blanche commercial CT scanners still employ traditional filtered back-projection image reconstruction Check also acknowledgments Ghost Imaging paper one Like Link Xi'an's Og Toscana 3 - Filed Mountains pictures Running Travel Wines Tagged Chianti farmhouse Italia ruins sunset Tuscany 35 minutes ago Hack Day Eye-Controlled Wheelchair Advances Talented Teenage Hackers - Myrijam Stoetzer friend Paul Foltin 14 15 years old kids Duisburg Germany working eye movement controller wheel chair Th 53 minutes ago Terahertz Technology Abstract-Terahertz response patterned epitaxial graphene - Christian Sorger Sascha Preu1 2 Johannes Schmidt3 Stephan Winnerl3 Yuliy V Bludov4 Nuno M R Peres4 Mikhail Vasilevskiy4 Heiko B Weber1 http 1 hour ago Endeavour Data code regulation - Data code code data distinction software code input data blurry best arbitrary worst distinction 6 hours ago Another Word Cybersecurity Authoritative Reports Resources Topic Need Librarians - Cybersecurity Authoritative Reports Resources Topic Rita Tehan Information Specialist Congressional Research Service summary 18 hours ago Image Sensors World Sony 2015 IR Day - Sony held 2015 Investor Relations Day today Device Segment presentation Tomoyuki Suzuki Executive Deputy President Corporate Executive Offic 21 hours ago free hunch Interactive R Tutorial Machine Learning Titanic Competition - Always wanted compete Kaggle competition sure right skill set DataCamp created free interactive tutorial help 21 hours ago High Noon GMT Oh torn 'twixt love an' tenure Behold molten children Recurrent Neural Networks Generating Even Word God - Please note post taken seriously show artificial system learned enough lot text generate new text 2 days ago What's new Cancellation multilinear Hilbert transform - ve uploaded arXiv paper Cancellation multilinear Hilbert transform submitted Collectanea Mathematica paper uses method 2 days ago G del's Lost Letter P NP John Alicia Nash 1928 1933 2015 - condolences Awesome Stories source John Nash wife Alicia killed taxi accident New Jersey Turnpike Saturday afternoon wer 2 days ago Timothy Lottes OS Project 7 - PS 2 Misc - started back long break Finished PS 2 driver Simplified keyboard interface one 64-bit bit array memory simple p 3 days ago Ergodic Walk AISTATS 2015 talks one day - attended AISTATS day change year unfortunately due teaching missed poster Shuang Song presented work 3 days ago Herve La cuisine cr erait-elle son objet - La chimie cr e son objet la phrase est paradoxale dit qu'elle est du chimiste Marcellin Berthelot mais est-elle vraiment de lui Voir Marcellin 3 days ago ChapterZero quick thought Supernatural tv shows - finished season 9 Supernatural ve got give show credit one demands deus ex machina ending Anything less 3 days ago Mr Vacuum Tube Phased Array Radar Looking Walls - Phased Array Radar Looking Walls http blog array2016 org p 24 5 days ago Walking Randomly MATLAB Vectorisation double-edged sword - Imagine new MATLAB programmer create N x N matrix called j j first attempt solution might 6 days ago slice pizza Morning Madness Ode Mercedes - morning madness drive along skyline Cutting fog thick stew waves dew 70mph hugging memorized curves without 1 week ago Decision Science News Gelman sense dubious Science article - Statistician Andrew Gelman sense something dubious Science article soon published post Gelman sense 1 week ago 0xDE Graham Erd Egyptian fractions - recent paper Ron Graham surveys work Paul Erd Egyptian fractions know Erd s' second paper subject didn't p 1 week ago Geomblog ITA conference really enjoy - Continuing thoughts STOC 2017 reboot went back Boaz's original question would make likely go STOC thought I'd 1 week ago Pillow Lab Blog Fast Kronecker trick Gaussian Process regression expressive kernels - May 11th presented following paper lab meeting Fast Kernel Learning Multidimensional Pattern Extrapolation Andrew Gordon Wilson Elad Gil 2 weeks ago Machine Learning etc ICLR 2015 - ICLR posters caught eye larger image simple implement idea gives impressive results force two groups units un 2 weeks ago Libres pens es d'un math maticien ordinaire adventure Google search - interesting experience Google recently related Electronic Journal Probability EJP Electronic Communications Probability ECP 2 weeks ago tombone's blog Dyson 360 Eye Baidu Deep Learning Embedded Vision Summit Santa Clara - Bringing Computer Vision Consumer Mike Aldred Electronics Lead Dyson Ltd vision research priority decades results 3 weeks ago Secrets Consulting Requirements Hints Variations - volumes Exploring Requirements follow chapter section hints variations topic chapter Many readers tel 3 weeks ago Harvest Imaging Blog Third HARVEST IMAGING FORUM December 2015 - successful forums 2013 2014 third one organized December 2015 Voorburg Hague Netherlands basic inte 3 weeks ago m bandit COLT 2015 accepted papers cool videos - Like last year compiled list COLT 2015 accepted papers together links arxiv version whenever could find one papers 3 weeks ago Information Structuralist Counting bits Vapnik Chervonenkis - Machine learning enabling computers improve performance given task get data express intuition quantitative 4 weeks ago Le Petit Chercheur Illustr Quasi-isometric embeddings vector sets quantized sub-Gaussian projections - Last January honored invited RWTH Aachen University Holger Rauhut Sjoerd Dirksen give talk general topic quantized co 4 weeks ago Machine Learning Theory Randomized experimentation - One good thing machine learning present people actually use back-ends many systems interact daily basis 5 weeks ago La vertu d'un LA virtue - fortunate hive D dom nologie la science du traitement de donn es signal images etc - O l'on propose le n ologisme d dom nologie pour d signer la technique la pratique la science du traitement de signal et de l'analyse d'images au c 1 month ago robots net Robots Podcast Farewell robots net join us Robohub - Since May 2007 colleagues Robots Podcast Robohub working robots net bring latest news views robo 1 month ago Machine Learning Deep Learning Works II Renormalization Group - Deep Learning amazing Deep Learning successful Deep Learning old-school Neural Networks modern hardware w 1 month ago Follow Data Genomics Today Tomorrow presentation - Slideshare link widget presentation gave Genomics Today Tomorrow event Uppsala couple weeks ago March 19 2015 sp 1 month ago Adventures Signal Processing Open Science Open Access Journals Missing - end could value proposition future journals 1 month ago Thoughts Mysterious Universe State Probabilistic Programming - two weeks last July cocooned hotel Portland living breathing probabilistic programming student probabilistic p 1 month ago Epistasis Blog Biomedical Informatics Faculty Positions University Pennsylvania - recently moved research lab Perelman School Medicine University Pennsylvania serve Director Institute Biomed 2 months ago Petros Boufounos Internship Opening Sensor Fusion - new internship opening MERL area sensor fusion posting follows MM880 Sensor fusion MERL looking well qualified 2 months ago G-media Le blog Nouveaut d veloppeurs domotique et openpicus Prise Gigogne et Dolphin View - Dans cet article nous allons voir comment utiliser la prise gigogne avec mesure de comptage depuis le logiciel Dolphin View Mat riel n cessaire une c 2 months ago Neurevolution Neurevolution relaunch - hard believe started blog eight years ago way back grad students long way ve come Patryk Dir 2 months ago Inductio Ex Machina Upcoming Conference Sydney Predictive APIs Apps - big kick-off Barcelona last year 200 attendees second International Conference Predictive APIs Apps held Sydn 2 months ago Ars Mathematica Nine Chapters Semigroup Art - Googling something came across Nine Chapters Semigroup Art leisurely introduction theory semigroups 2 months ago yellow noise takis champs magnetiques palais de tokyo february 2015 - takis champs magnetiques palais de tokyo february 2015 3 months ago polylogblog CPM 2015 - Ely asked remind everyone deadline 26th Annual Symposium Combinatorial Pattern Matching fast approaching 2nd F 4 months ago Mirror Image Kinect depth sensor works stereo triangulation simple ways speed convnet little - quite complex methods making convolutional networks converge faster Natural gradient dual coordinate ascent second order hessian fre 5 months ago natural language processing blog myth strong baseline - probably count fingers number papers I've submitted reviewer hasn't complained baseline way don't mean 6 months ago Building Intelligent Probabilistic Systems Harvard Center Research Computation Society Call Fellows Visiting Scholars - Harvard Center Research Computation Society CRCS solicits applications Postdoctoral Fellows Visiting Scholars Programs 7 months ago Pixel shaker Pics Manipulated Photos Notable Historic Figures Digital Era Images - Manipulating photos happened way Photoshop around series shows afters famous notable figures digital era 7 months ago Victoria Stodden input OSTP RFI reproducibility - Sept 23 2014 US Office Science Technology Policy Whitehouse accepting comments Strategy American Innovation 8 months ago Ga l Varoquaux Hiring engineer mine large brain connectivity databases - Work us leverage leading-edge machine learning neuroimaging Parietal research team work improving way brain images analyz 8 months ago Computers don't see Compiling OpenCV 3 0 alpha CUDA support MacOS X - quick tip people troubles compiling OpenCV 3 0 alpha MacOS X variable called CUDA_TOOLKIT_DIR cmake configura 8 months ago Herr Strathmann - home Shogun NYC - late August invited NYC present Shogun open-source Machine Learning software workshop link organised John Langford Seeing Sh 8 months ago Lousodrome Vie au Japon Le certificat de r sidence j minhy - Le certificat de r sidence j minhy en japonais est un simple document d une page qui certifie votre adresse de r sidence et qui est demand pour 10 months ago trekkinglemon's fresh squeeze Data processing flashy yellow Peyresq 2014 Last day - 2014 edition Peyresq summer school finished coda let us summarize last talk Continue reading 10 months ago Camdp com updates DataOrigami Launch - I'm proud announce latest project dataorigami net still go check 11 months ago Neighborhood Infinity Cofree meets Free - - LANGUAGE RankNTypes MultiParamTypeClasses TypeOperators - Introduction spoke BayHac 2014 free monads asked co 1 year ago Statistical Trader Follow twitter StatTrader - Since working full time managing Data Sciences team Bloomberg Global Data haven't done sort long-form blogging used 1 year ago Ivan Oseledets homepage Introduction cross approximation - written short incomplete introductory page skeleton decomposition using maxvol algorithm update references 1 year ago MAKE Magazine New Project TV-B-Gone Kit - Tired LCD TVs everywhere Want break advertisements re trying eat Want zap screens across street TV-B-Go 1 year ago i2pi com focus - focus 1 year ago brain map statistics connection Linus Pauling fMRI - think Linus Pauling two things come mind work nature chemical bond awarded 1954 Nobel Prize Chem 1 year ago Computational Information Geometry Wonderland New blog address Moving Wordpress - use anymore blog system rather use Wordpress supports many goodies like latex Please update yo 1 year ago Martin Tall Gaze Interaction Introducing Eye Tribe Tracker - It's great pride today introduce Eye Tribe Tracker It's worlds smallest remote tracker first use USB3 0 one 100 1 year ago De Rerum Natura Functional programming - least interesting concept programming languages purity Java prime example Everything object much prefer Python wa 1 year ago Doyung Pig Hive Pig Hive - Hadoop ecosystem Data processing Pig Hive Pig Hive pig hive data 2 years ago BlackbordRMT Dyson Brownian motion thirty stationnary Brownian motions Ornstein Uhlenbeck - image Image Hosted ImageShack us Dyson Brownian motion thirty stationnary Brownian motions Ornstein Uhlenbeck processes constr 2 years ago Freakonometrics Mod lisation et pr vision cas d' cole - Quelques lignes de code que l'on reprendra au prochain cours avec une transformation en log et une tendance lin aire Consid rons la recherche du mot c 2 years ago Brain Windows WordPress accepts Bitcoin - WordPress blog platform accepts bitcoin hosting costs via BitPay WordPress hosts Brain Windows many world biggest best blogs 2 years ago Science Sands Science Sands moved - Dear readers migrated blog along things new site http davidketcheson info New posts longer appear blo 2 years ago Journey Randomness SMC sampler - monte carlo sequential monte carlo SMC sampler Jasra Doucet Del Moral 2 years ago Hao's TechBlog Sampling Rate Pixels compare sampling rate camera single-pixel camera - beginning I'd like make clear two terms Nyquist frequency Nyquist rate may take thing even text 2 years ago bpchesney org Nested Linear Programs Find Agreement among Sensors - Suppose 3 sensors B C set readings goes column matrix b matrix constructed bA bB bC repr 2 years ago Stupid Matlab Hacks busy recently hacks mean time go play - busy recently hacks mean time go play http www mathworks com matlabcentral fileexchange 37104-kappatau mak 2 years ago Becoming Astronaut Orbiters going - month NASA commenced delivery four Space Shuttle orbiters final destinations extensive decommissioning process 3 years ago FUTUREPICTURE Note comments - six months ago hit serious rash spam 20 000 comments posted span two weeks Unfortunately ti 3 years ago CyberGi Os ciborgues da Campus Party - Essa semana fui na quinta edi o da Campus Party e ontem dia 9 tive o prazer de conhecer dois ciborgues o Rob Spencer que fez o document rio Eyeborg q 3 years ago Collective Research Interaction Sound Signal Processing Sonification Handbook - yet heard Sonification Handbook edited Thomas Hermann Andy Hunt John G Neuhoff published even better freely avai 3 years ago Espace Vide PCA Compressive Measurements Video - I've little bit fun visualizations today outcomes pretty nice potentially artistic thought I'd share 3 years ago inspiration etc Session 4 5th Graders - Doodling 2 - Session 4 - Doodling Approximately one half hours Starting one element Adding proximity expanding drawing Tool 3 years ago Marcio Marim Welcome new website - time work second version didn publish happy release new website share creations interests whi 3 years ago Cognitive Radio Blog G Vazquez-Vilar PhD Thesis Interference Management Cognitive Radio - image Thesis Cognitive radio dissertation examination took place couple weeks ago Happily passed say one unexpect 3 years ago OISblog Liquid Crystal Eyeglasses - Pixel Optics introduced Empower eyeglasses use liquid crystal lenses actively adjust power 4 years ago Big Numbers Going commission - m going focus completely school blog going hiatus months ll start ve passed hurd 4 years ago Another Dimension three musketeers - Given vectors three quantities interesting indeed fact concept inner product vector space revolves largely around thos 4 years ago Electrons holes Indefinite hiatus - blog put indefinite hiatus 4 years ago Arthur Charpentier Blog transfert - mentioned past weeks blog transfered please update links bookmarks redirected shortly http freako 4 years ago Chaotic Pearls Indonesian Contoh Program Phase Unwrapping - Ide dari phase-unwraping PU progresif ini muncul di suatu sore hari ketika saya sedang berjalan-jalan di sekitar kampus sekitar tahun 2002-an Saya ber 4 years ago YALL1 ALgorithms L1 Announcements - June 4 2010 Toeplitz circulant sampling demos released June 4 2010 YALL1 version 1 0 released open-source Download link YALL1 n 4 years ago Hashimoto Laboratory's Blog Personal Mobility Next Level - Improving concept self-balancing unicycle Honda introduced brand new U3-X new personal mobility platform regular large whee 5 years ago Lianlin Li's Compressive Sensing blog Chinese Igor's Blog today - Today Post updated Igor's blog following CS Matrix Completion via Thresholding Dick Gordon's Op-Ed Lianlin L 5 years ago Guan Gui's blog expoit channel structure - 6 years ago Three-Toed Sloth - ML Counterexamples Pt 2 - Regression Post-PCA camdp com blogs - Willow Garage Blog Willow Garage - Latest News - KinectHacks net - Little Knowledge - Blog Robotics - Show 25 Show Another Blog List Haldane's Sieve SWEEPFINDER2 Increased sensitivity robustness flexibility - SWEEPFINDER2 Increased sensitivity robustness flexibility Michael DeGiorgio Christian D Huber Melissa J Hubisz Ines Hellmann Rasmus Nielsen Su 5 hours ago Information Processing John Nash dead 86 - original title post won Nobel Memorial Prize see sad news bottom Beautiful Mind Nash went see von Neuman 4 days ago Scientific Clearing House Selection week - Violinist Hilary Hahn hails Baltimore pianist Valentina Lisitsa play first movement American composer Charles Ives Fourth Sonata 6 days ago leon bottou org news news graph_transducer_networks_explained - Graph Transducer Networks explained scavenging old emails couple weeks ago found copy early technical report describes 2 weeks ago tombone's blog Dyson 360 Eye Baidu Deep Learning Embedded Vision Summit Santa Clara - Bringing Computer Vision Consumer Mike Aldred Electronics Lead Dyson Ltd vision research priority decades results 3 weeks ago Property Testing Review News April 2015 - April busy time property testing 9 papers posted online month let jump right Testing properties graphs Approximately Co 3 weeks ago Relax Conquer Courant Institute Mathematical Sciences - Finally longer job market excited announce join Courant Institute Mathematical Sciences Assistant Profess 1 month ago Moody Rd Competing data science contest without reading data - Machine learning competitions become extremely popular format solving prediction classification problems sorts famous ex 2 months ago AK Tech Blog Neustar SIAM SODA 2015 - Author Note Hello readers m Sonya Berg first post Neustar research blog data scientist Neustar Research foc 4 months ago Mostly linguistically computational Adventure collaborative filtering information retrieval matrix factorization stuff Count-Min-Log Strange effect - followed previous steps mentionning previous post odd behaviour Count-Min-Log MAX sampling w 5 months ago Deep Learning Recent Reddit AMA Deep Learning - Recently Geoffrey Hinton Yann Lecun Yoshua Bengio reddit AMA subscribers r MachineLearning asked questions AMA contains 6 months ago jim learning choose mentor - http www cell com neuron fulltext S0896-6273 13 00907-0 https www cs princeton edu courses archive spring15 cos598D http www cs princeton edu cours 1 year ago Blog Interphase Transport Phenomena Laboratory Texas M University Fun Boiling - 1 year ago Math Good another WordPress site Linear Algebra good - Linear algebra branch mathematics concerned solving linear systems natural think linear algebra solving unkno 1 year ago BayesRules STAT 330 November 29 2012 - finished discussion model selection averaging went missing data models cases fact data observed h 2 years ago Comments Lecture Schedule Embryo Physics Course - Blog List FlowingData Compare curve reality income versus college attendance - image Draw line grow poorer families less likely go college grow richer families likely 1 hour ago SynBioFromLeukipposInstitute Scoop Three developments help synthetic biology live promise Genetic Literacy Project - Three developments help synthetic biology live promise Dominic Basulto May 28 2015 Washington Post See Scoop via 2 hours ago Retraction Watch chocolate-diet sting study retracted coverage doesn surprise news watchdog - Yesterday John Bohannon described i09 com successfully created health news conducted flawed trial health benefits chocolate 4 hours ago Statistical Modeling Causal Inference Social Science Cracked com Huffington Post Wall Street Journal New York Times - David Christopher Bell goes trouble link Palko explain Every Map Popular _________ State Bullshit long 4 hours ago olimex MOD-LCD3310 OSHW monochrome LCD 84 48 pixels board UEXT connector - MOD-LCD3310 Open Source Hardware board released Apache 2 0 Licensee low cost 84 48 pixels LCD connect development bo 6 hours ago Science-Based Medicine Florida strikes Brian Clement - Brian Clement charlatan Unfortunately doesn seem problem State Florida made two turned three attempts g 12 hours ago Sage Open Source Mathematics Software Guiding principles SageMath Inc - February year 2015 founded Delaware C Corporation called SageMath Inc first stab guiding principles compan 21 hours ago Quomodocumque evil impulse good - learned teaching Rabbi Rebecca Ben-Gideon last week turning mind Rabbi Nahman said Rabbi Samuel name Behold 1 day ago Machine Vision 4 Users re backlighting cylindrical parts - see backlighting used time machine vision training classes trade shows typically gauging locating shapes Look closely though 1 day ago Skulls Stars comics moment inspires Suicide Squad - Update Forgot say thank Dad mailing complete Suicide Squad collection made whole post possible suspect peop 1 day ago InnoCentive Challenges Bioabsorbable Elastomeric Film - Seeker looking bio-absorbable elastomeric film specific properties could existing product one adapted 1 day ago What's new Cancellation multilinear Hilbert transform - ve uploaded arXiv paper Cancellation multilinear Hilbert transform submitted Collectanea Mathematica paper uses method 2 days ago Ben Krasnow physics floating screwdrivers - explain jet air float common screwdriver Plans make fluid turbulence disc http makezine com projects rheoscopic-coffee-tab 2 days ago Short Fat Matrices Three Paper Announcements - ve pretty busy lately writing researching visitors announcements serve quick summary ve 1 Tables 1 week ago 2Physics Current Fluctuations - left right Pierre Pfe er Fabian Hartmann Sven H ing Martin Kamp Lukas Worschech Authors Pierre Pfe er1 Fabian Hartmann1 Sven H ing1 2 1 week ago Large Scale Machine Learning Animals Open Data Science Conference - May 30 Boston - 1 week ago Zhilin's Scientific Journey Yann LeCun's Comments Extreme Learning Machine ELM - Yann LeCun https www facebook com yann lecun posts 10152872571572143 Facebook commented ELM quoted What's great Ext 2 weeks ago Gowers's Weblog Nick Clegg Liberal Democrat - life found Liberal Democrat policies Liberal-SDP Alliance policies Liberal policies n 4 weeks ago JeremyBlum com Shapeoko2 CNC Mill Build Log Review - new Inventables Shapeoko2 CNC mill carving away Watch timelapse build-log thorough review do-it-yourself CNC milling machine Continue 4 weeks ago F Pedregosa IPython Jupyter notebook gallery - Draft - TL DR created gallery IPython Jupyter notebooks Check - image Notebook gallery couple months ago put online website d 5 weeks ago Richard Baraniuk Probabilistic Theory Deep Learning - Patel Nguyen R G Baraniuk Probabilistic Theory Deep Learning arXiv preprint arxiv org abs 1504 00641 2 April 2014 grand challeng 1 month ago Welcome Sparse Land Discreteness Sparsity - Discrete signals may sparse whereas sparse signals may discrete Let us consider following signal x 1 1 -1 1 -1 -1 1 1 2 months ago Various Consequences Reliability Growth Enhancing Defense System Reliability - report pdf National academies reliability growth interesting There's lot good stuff design reliability physics fai 2 months ago Inductio Ex Machina Upcoming Conference Sydney Predictive APIs Apps - big kick-off Barcelona last year 200 attendees second International Conference Predictive APIs Apps held Sydn 2 months ago Highly Scalable Blog Data Mining Problems Retail - Retail one important business domains data science data mining applications prolific data numerous optimization p 2 months ago regularize Farewell Ernie Esser - Today learned Ernie Esser passed away March 8th 2015 age 34 Ernie PostDoc UBC Vancouver PhD UCLA worked applied math 2 months ago Thingiverse Blog MakerBot PrintShop 1 4 3D Print iPad 3G 4G - Last week introduced exciting new feature MakerBot 3D Ecosystem ability start 3D print remotely using smartphone MakerBot P 3 months ago much little time Post-doc Molecular Informatics Opening NCATS - post-doc opening Informatics group NCATS work computational aspects high throughput combination screening topics includ 3 months ago next big thing syndrome Books read 2014 - disclaimer book cover images post Amazon Affiliate links click buy book receive cents form 5 months ago Pursuits Null Space Blog done moved - first hesitant handling latex Rolf Mathcination pointed excellent tool latex-to-wordpress wil 5 months ago Proof Pudding M571 Fall 2014 Lecture 5 - 1 Agenda QR Factorization Gram-Schmidt classical modified Householder QR reflectors ended discussion projectors several impo 6 months ago Tianyi Zhou's Research Blog Learn Low-rank Sparse Structures via Randomized Alternating Projections List Submodular Optimization Streaming Data Update - Coresets k-Segmentation Streaming Data NIPS 2014 Streaming Submodular Optimization Massive Data Summarization Fly KDD 2014 8 months ago Wondering Star Lunar Detection Ultra-High-Energy Cosmic Rays Neutrinos - Spotted ArXiv Physics blog using SKA array Moon collector would certainly qualify sensors size pl 8 months ago Dan's Blog make paper spherical panorama - image image Photos usually show rectangular fragment scene image taken Typical panoramic images display landscap 8 months ago Seth's blog Videos Seth Roberts Memorial Talks - Video recordings Ancestral Health Society public talks August 10 2014 honoring Seth life work posted http bit ly 1v33kbM Many 9 months ago Thoughts Artificial Intelligence - blog moved new post metaphor mathematics new blog 9 months ago Lupi Software thoughts Monitorama 2014 PDX - Monitorama fantastic conference came mixed feelings Great work done open source software however based con 1 year ago Andrej Karpathy Blog Interview Data Science Weekly Neural Nets ConvNetJS - Quick post thought mention ve given interview two months ago ConvNetJS background perspectives neural 1 year ago Ivan Oseledets homepage Introduction cross approximation - written short incomplete introductory page skeleton decomposition using maxvol algorithm update references 1 year ago Artificial Intelligence Blog John Dobson 1915 2014 - September 2008 met John Harrisburg slept overnight friend Zoungy place enjoyed beautiful drive Cherry Springs th 1 year ago Hey What's BIG idea 3D Printing Tangible Idea - heywhatsthebigidea net B J Rao write another article 3D printing internet already offers abundance information subject Moun 1 year ago Normal Deviate END - addition best comedy TV show ever Seinfeld great source wisdom one episode Jerry counsels George hit high 1 year ago Math Good another WordPress site Linear Algebra good - Linear algebra branch mathematics concerned solving linear systems natural think linear algebra solving unkno 1 year ago tcs math - mathematics theoretical computer science Kadison-Singer Quantum Mechanics - accounts Kadison-Singer problem mention arose considerations foundations quantum mechanics Specifically question abo 1 year ago programming machine learning blog BibTeX-powered publications list Pelican pelican-bibtex - Hook Wouldn like manage academic publications list easily within context static website Without resorting external services 2 years ago Mathblogging org -- Blog Mathematical Instruments Gianluigi Filippelli - post part series Mathematical Instruments introduce math bloggers listed site Today Gianluigi Filipp 2 years ago openPicus blog Better BBQ Flyport Cosm - knew Austria famous BBQ remember summer 2006 great barbeque austrian friends Klagenfurt fantastic 8 2 years ago Dirac Sea Survey Non Parametric Bayesian marginal applications - Survey Non Parametric Bayesian marginal applications mystery Zoubin one favorite researchers papers usually 2 years ago Biophotonics Review Biology Games Biomedical Research - Games become important part cultures bringing lots entertainment creativity societies Beyond traditional understanding o 2 years ago Clouds Fukushima 9 Months later - regards airborne radiation ground measurement reading NaI detector KEK Tsukuba showing steady decline past 3 years ago Gigapixel News Journal ipConfigure Presents First Gigapixel Wide Area Surveillance Platform - ipConfigure privately owned software research development company announces world first multi-Gigapixel surveillance platform designed 3 years ago Gustavo Tinkers Source Code GUI - Hosted https github com goretkin soundcard-radar 3 years ago Passive Vision Passive Vision - Welcome plan use blog ideas research teaching moment m looking forward teaching Computer Vision class 60 5 years ago Dick Gordon's blog - n 45 years ago Show 25 Show Search Pages Link Loading Previous Entries 2015 207 May 2015 37 Compressive Sensing work Three-dimensional coo Robust Rotation Synchronization via Low-rank Low-Rank Matrix Recovery Row-and-Column Affin Self-Dictionary Sparse Regression Hyperspectra Randomized Robust Subspace Recovery High Dimen Book Dictionary Learning Visual Computing Compressed Nonnegative Matrix Factorization Fas Saturday Morning Videos Slides Videos IC PCANet Simple Deep Learning Baseline Image Four million page views million million Great Convergence FlowNet Learning Optical F CSjob Post-Doc Structured Low-Rank Approximati Low-rank Modeling Applications Image Solving Random Quadratic Systems Equations N Identifiability Blind Deconvolution Subspa Tensor time Adaptive Higher-order Spectral Estima Blue Skies Foundational principles large scal Tensor sparsification via bound spectral Saturday Morning Videos Reruns Hamming's time Important Things Commodi Newton Sketch Linear-time Optimization Algorith Self-Expressive Decompositions Matrix Approxim Tonight Paris Machine Learning Meetup 9 Season Factorization Machines implementation Theano Frequent Directions Simple Deterministic Mat fastFM Library Factorization Machines - imp Structured Block Basis Factorization Scalable Tensorial Kernel kernel-based framework tens Kernel Spectral Clustering applications - impl Sparse LSSVM Reductions Large-Scale Data Random Bits Regression Strong General Predictor Phase retrieval via Kaczmarz methods - implementat Great Convergence Deep Learning Compressive Phase Transition Joint-Sparse Recovery Mul Saturday Morning Videos IMA Workshop Convexity Saturday Morning Videos IMA AP Workshop Inform Nuit Blanche Review April 2015 Apr 2015 45 Mar 2015 44 Feb 2015 31 Jan 2015 50 2014 536 Dec 2014 52 Nov 2014 43 Oct 2014 38 Sep 2014 41 Aug 2014 48 Jul 2014 52 Jun 2014 43 May 2014 56 Apr 2014 47 Mar 2014 44 Feb 2014 35 Jan 2014 37 2013 454 Dec 2013 43 Nov 2013 38 Oct 2013 38 Sep 2013 33 Aug 2013 36 Jul 2013 43 Jun 2013 29 May 2013 38 Apr 2013 40 Mar 2013 29 Feb 2013 47 Jan 2013 40 2012 488 Dec 2012 44 Nov 2012 39 Oct 2012 46 Sep 2012 28 Aug 2012 52 Jul 2012 20 Jun 2012 38 May 2012 60 Apr 2012 41 Mar 2012 50 Feb 2012 29 Jan 2012 41 2011 465 Dec 2011 47 Nov 2011 49 Oct 2011 47 Sep 2011 36 Aug 2011 24 Jul 2011 25 Jun 2011 47 May 2011 50 Apr 2011 56 Mar 2011 39 Feb 2011 17 Jan 2011 28 2010 358 Dec 2010 47 Nov 2010 35 Oct 2010 32 Sep 2010 28 Aug 2010 30 Jul 2010 33 Jun 2010 26 May 2010 27 Apr 2010 28 Mar 2010 28 Feb 2010 19 Jan 2010 25 2009 274 Dec 2009 22 Nov 2009 23 Oct 2009 24 Sep 2009 25 Aug 2009 25 Jul 2009 23 Jun 2009 20 May 2009 16 Apr 2009 25 Mar 2009 27 Feb 2009 21 Jan 2009 23 2008 302 Dec 2008 20 Nov 2008 23 Oct 2008 28 Sep 2008 28 Aug 2008 22 Jul 2008 17 Jun 2008 28 May 2008 22 Apr 2008 31 Mar 2008 32 Feb 2008 25 Jan 2008 26 2007 179 Dec 2007 23 Nov 2007 21 Oct 2007 14 Sep 2007 18 Aug 2007 13 Jul 2007 13 Jun 2007 9 May 2007 11 Apr 2007 9 Mar 2007 22 Feb 2007 19 Jan 2007 7 2006 30 Dec 2006 4 Nov 2006 4 Oct 2006 2 Sep 2006 2 Aug 2006 2 Jul 2006 2 Jun 2006 1 Mar 2006 2 Feb 2006 2 Jan 2006 9 2005 88 Dec 2005 1 Nov 2005 6 Oct 2005 3 Sep 2005 12 Aug 2005 1 Jul 2005 7 Jun 2005 4 May 2005 12 Apr 2005 7 Mar 2005 12 Feb 2005 8 Jan 2005 15 2004 214 Dec 2004 18 Nov 2004 8 Oct 2004 20 Sep 2004 44 Aug 2004 29 Jul 2004 13 Jun 2004 13 May 2004 18 Apr 2004 10 Mar 2004 22 Feb 2004 8 Jan 2004 11 2003 12 Dec 2003 10 Nov 2003 2 Books Wish List Start-ups like InView Technology Corporation See Inside World First Compressive Sensing Camera - Read InView white paper InView210 scientific SWIR camera InView210-CSCameraWhitePaper-Feb2015 See inside world first high 2 months ago Centice Centice Drug Analysis Systems Sold Major Federal Agency Aid Prescription Pill Abuse Operations - Nationwide program allows agents identify 3 800 prescription pills illicit drugs RESEARCH TRIANGLE PARK N C October 28 2014 Centice Co 7 months ago Press GraphLab twitterscroll - post twitterscroll appeared first GraphLab Inc 1 year ago Zoomin' - Aqueti TV Interview Gigapixel Images - Scott McCain interviewed July 10 2013 discuss gigapixel imaging WLOS ABC's Asheville affiliate 1 year ago Metamarkets Blog - wise io Machine Learning Service Big Data Analytics - Translate blog Focused Interest Compressed Sensing Compressive Sampling Compressive Sensing Mapping blog entries Compressed Sensing Cognition - Machine Learning Space Search Rescue Compressive Sensing Technology Watch Compressive Sensing Big Picture Compressive Sensing Hardware Compressed Sensing Videos Compressive Sensing Calendar Compressive Sensing Jobs Local Compressed Sensing Codes CS LinkedIn Group Recent links Blog CS Compressive Sensing 2 0 Community Compressive Sensing 2 0 blogs webpages Saturday Morning Cartoons Sherpa Romeo Publisher copyright policies self-archiving Categories Subjects Interest CS 2163 compressive sensing 1627 compressed sensing 1615 compressive sampling 1589 MF 521 implementation 359 Applied Math 209 ML 209 MatrixFactorization 194 space 152 AMP 113 calibration 105 CSHardware 103 CSjobs 90 CS Community 72 SaturdayMorningVideos 71 CSCommunity 66 BlindDeconvolution 64 QuantCS 62 phaseretrieval 62 RandNLA 60 hyperspectral 60 nonlinearCS 59 nuclear 59 technology 58 SundayMorningInsight 57 CSVideo 50 python 50 tensor 49 cognition 47 Meetups 46 Algorithm 45 grouptesting 45 meetup 43 1bit 42 publishing 41 synbio 41 graphlab 38 RandomFeatures 37 darpa 37 CSmeeting 36 AI 33 NuitBlancheReview 33 search rescue 33 weather modeling 33 remote sensing 32 Csstats 31 wow 30 business 29 bayes 28 data fusion 28 machine learning 28 MLParis 27 jim gray 24 neuroscience 23 autonomous 22 dimensionality reduction 22 mapmaker 20 thesis 20 AlexSmola 19 Kaczmarz 19 ParisMachineLearning 19 geocam 19 space debris 19 space situational awareness 19 medical 18 phaserecovery 18 ChristophStuder 17 ImagingWithNature 17 SAHD 17 maps 17 mishap 17 sleep 17 CSCalendar 16 monday morning algorithm 16 transport 16 CAI 15 energy 15 nanopore 15 phasediagrams 15 hasp 13 superresolution 13 ADMM 12 PatrickGill 12 Technologies Exist 12 causality 12 darpa urban challenge 12 sudoku 12 CSDiscussion 11 ICLR2015 11 TRL 11 qa 11 thermal engineering 11 GPU 10 fft 10 sie 10 videos 10 Computational Neuroscience 9 MultiplicativeNoise 9 RandomForest 9 StarTracker 9 aroundtheblogs 9 france 9 ELM 8 GenomeTV 8 HammingsTime 8 PredictingTheFuture 8 Good 8 collaborative task manager 8 exploration 8 random projections 8 situational awareness 8 sparsity 8 wavelet 8 GreatThoughtsFriday 7 TheGreatConvergence 7 collaborative work 7 innovation 7 mems 7 random lens imaging 7 CSCartoons 6 CitingNuitBlanche 6 CompressibleWGN 6 accidentalcamera 6 complexity vizualisation 6 maxent 6 randomization 6 startups 6 streaming 6 thedip 6 RMM 5 UQ 5 book 5 coded aperture 5 muscle 5 tex-mems 5 BP 4 British Petroleum 4 CfP 4 CompressiveSensingWhatIsItGoodFor 4 DataDrivenSensorDesign 4 HusHambug 4 Comment 4 ReproducibleResearch 4 google maps 4 hypergeocam 4 internet traffic 4 jionc 4 microsystems 4 scaling 4 technologie 4 Deepwater Horizon 3 disruptive technology 3 financement de la recherche 3 google 3 julia 3 radiation detection 3 recherche 3 sketching 3 Columbia 2 DC law 2 LowRank 2 MLZurich 2 MMDS 2 ManifoldSignalProcessing 2 NO-C-WE 2 TheNuitBlancheChronicles 2 UAV 2 aggregators 2 anecdote 2 challenge 2 diet 2 genomics 2 kinect hacks 2 microcontroller 2 notebynotecooking 2 sensor network 2 AWGN 1 BaltiAndBioinformatics 1 Blogger 1 CS MF 1 CT 1 CompanyX 1 JOTRSOI 1 Leonardo 1 QIS 1 RMT 1 SKA 1 SaturdayMorningCartoons 1 SensorsTheSizeOfAPlanet 1 YouAreNotPayingAttention 1 advice 1 aha 1 biographies 1 control 1 crowdfunding 1 csoped 1 dataset 1 donoho-tao 1 extremesampling 1 herschel 1 hushamburg 1 iLab 1 inverse problems 1 iot 1 jacques devooght 1 lfe 1 lua 1 memory 1 mindmaps 1 nanopre 1 octopus 1 oped 1 privacy 1 reference 1 request 1 rr 1 seinfeld 1 solver 1 theano 1 wonderingstar 1 youkeepusingthatword 1 sites interest Blogroll Natural Language Blog Hal Daume III Polylog Blog Andrew McGregor Eric Tramel's Espace Vide Blog Compressed Sensing Lianlin Li's Compressive Sensing blog Chinese Space Engineering Research Center Space Engineering Blog Frank Nielsen's Information Geometry blog David Brady's Blog Le Petit Chercheur Illustre Chaotic Pearls Indonesian De Rerum Natura Michele Guieu's blog Ergodic Walk Laurent Duval's site Laurent Duval's blog Thesilog Diffusion des savoirs - Ecole Normale Superieure What's New Terry Tao Statistical Modeling Causal Inference Social Science Andrew Gelman Aleks Jakulin Masanao Yajima Machine Learning etc Yaroslav Bulatov slice Pizza Muthu Mutukrishnan Geomblog Piotr Indyk Suresh Machine Learning Theory John Langford Lemonodor John Wiseman Yet another Machine Learning blog Pierre Dangauthier Make Magazine blog Theses en ligne Neurevolution blog Pedro Davalos website Damaris' blog Olivier's blog Julie's blog Michele Guieu's site Location visitors Nuit Blanche Dilbert counters Powered Blogger"),
('On the evolution of machine learning', "Menu Home Shop Video Training Books Radar Safari Books Online Conferences Courses Certificates oreilly com O'Reilly Radar RSS Feed Twitter Facebook Google Youtube Home Shop Video Training Books Radar Radar Animals Safari Books Online Conferences Courses Certificates Data Topics DataDesignEmerging TechIoTProgrammingWeb Ops PerformanceWeb Platform Help us test new look O Reilly Visit beta site Print Listen evolution machine learning linear models neural networks interview Reza Zadeh David Beyer dbeyer123 Comment May 3 2015 Comment Get notified free report Future Machine Intelligence Perspectives Leading Practitioners available download following interview one many included report part ongoing series interviews surveying frontiers machine intelligence recently interviewed Reza Zadeh Reza Consulting Professor Institute Computational Mathematical Engineering Stanford University Technical Advisor Databricks work focuses Machine Learning Theory Applications Distributed Computing Discrete Applied Mathematics Key Takeaways Neural networks made comeback playing growing role new approaches machine learning greatest successes achieved via supervised approach leveraging established algorithms Spark especially well-suited environment distributed machine learning David Beyer Tell us bit work Stanford Reza Zadeh Stanford designed teach distributed algorithms optimization CME 323 well course called discrete mathematics algorithms CME 305 discrete mathematics course teach algorithms completely theoretical perspective meaning tied programming language framework fill whiteboards many theorems proofs practical side distributed algorithms class work Spark cluster programming environment spend least half time Spark theory teach regard distributed algorithms machine learning gets implemented made concrete Spark put hands thousands industry academic folks use commodity clusters started running MapReduce jobs Google back 2006 Hadoop really popular even known MapReduce already mature Google 18 time even could see clearly something world needs outside Google spent lot time building thinking algorithms top MapReduce always worked stay current long leaving Google Spark came along nice open-source one could see internals contribute felt like right time jump board idea RDD right abstraction much distributed computing DB time Google present work re Spark chance see evolution machine learning ties distributed computing describe evolution RZ Machine learning several transition periods starting mid-90 1995 2005 lot focus natural language search information retrieval machine learning tools simpler re using today include things like logistic regression SVMs support vector machines kernels SVMs PageRank Google became immensely successful using technologies building major success stories like Google News Gmail spam classifier using easy-to-distribute algorithms ranking text classification using technologies already mature mid-90 around 2005 neural networks started making comeback Neural networks technology 80 would even date back 60 ve become retrocool thanks important recent advances computer vision Computer vision makes productive use convolutional neural networks fact become better established neural networks making way applications creeping areas like natural language processing machine translation problem neural networks probably challenging mentioned models distribute earlier models training successfully distributed use 100 machines train logistic regression SVM without much hassle developing distributed neural network learning setup difficult guess done successfully organization far Google pioneers yet much like scene back 2005 Google published MapReduce paper everyone scrambled build infrastructure Google managed distribute neural networks get bang buck everyone wishing situation re DB SVM logistic regression easier distribute neural network RZ First evaluating SVM lot easier ve learned SVM model logistic regression model linear model actual evaluation fast Say built spam classifier new email comes along classify spam takes little time one dot product linear algebra terms comes neural network lot computation even learned model figure model output even biggest problem typical SVM might happy million parameters smallest successful neural networks ve seen around 6 million absolutely smallest Another problem training algorithms benefit much optimization theory linear models use mathematical guarantees training finished guarantee found best model re going find optimization algorithms exist neural networks afford guarantees know ve trained neural network whether given setup best model could found re left wondering would better model kept training DB neural networks become powerful see subsuming work used bread butter linear methods RZ think yes Actually happening right always issue linear models discriminate linearly order get non-linearities involved would add change features involves lot work example computer vision scientists spent decade developing tuning things called SIFT features enable image classification vision tasks using linear methods neural networks came along SIFT features became unnecessary neural network approach make features automatically part training think asking much say neural networks replace feature construction techniques think happen always place linear models good human-driven feature engineering said pretty much researcher NIPS Conference beginning evaluate neural networks application Everyone testing whether application benefit non-linearities neural networks bring like never non-linear models many neural network model happens particularly powerful really work applications worth trying lot people see successes write papers far ve seen successes speech recognition computer vision machine translation wide array difficult tasks good reason excited DB neural network powerful compared traditional linear non-linear methods existed RZ linear model every feature either going hurt help whatever trying score assumption inherent linear models model might determine feature large indicative class 1 small indicative class 2 Even go way large values feature small values feature never situation say interval feature indicative class 1 another interval indicative class 2 limited Say analyzing images looking pictures dogs might certain subset feature values indicate whether picture dog rest values pixel patch image indicate another class draw line define complex set relationships Non-linear models much powerful time re much difficult train run hard problems optimization theory long thought neural networks weren good enough would over-fit powerful couldn precise guaranteed optimization temporarily vanished scene DB Within neural network theory multiple branches approaches computer learning summarize key approaches RZ far successful approach supervised approach older algorithm called backpropagation used build neural network many different outputs Let look neural network construction become popular called Convolutional Neural Networks idea machine learning researcher builds model constructed several layers handles connections previous layer different way first layer window slides patch across image becomes input layer called convolutional layer patch convolves overlaps several different types layers follow different properties pretty much introduce non-linearities last layer 10 000 potential neuron outputs one activations correspond particular label identifies image first class might cat second class might car 10 000 classes ImageNet first neuron firing 10 000 input identified belonging first class cat drawback supervised approach must apply labels images training car zoo Etc DB Right unsupervised approach RZ less popular approach involves autoencoders unsupervised neural networks neural network used classify image compress read image way described identifying patch feeding pixels convolutional layer Several layers follow including middle layer small compared others relatively neurons Basically re reading image going bottleneck coming side trying reconstruct image labels required training putting image ends neural network training network make image fit especially middle layer possession neural network knows compress images effectively giving features use classifiers little bit labeled training data problem always lot images Think images non-labeled training data use images build autoencoder autoencoder pull features good fit using little bit training data find neurons auto-encoded neural network susceptible particular patterns DB got Spark see set technologies heading RZ ve known Matei Zaharia creator Spark since undergraduates Waterloo actually interned Google time working developer productivity tools completely unrelated big data worked Google never touched MapReduce focus kind funny given ended Matei went Facebook worked Hadoop became immensely successful time kept thinking distributing machine learning none frameworks coming including Hadoop looked exciting enough build top knew time Google really possible DB Tell us bit Spark works particularly useful distributed machine learning RZ Spark cluster computing environment gives distributed vector works similar vectors re used programming single machine everything could regular vector example arbitrary random access via indices example intersect two vectors union sort dmany things would expect regular vector One reason Spark makes machine learning easy works keeping important parts data memory much possible without writing disk distributed environment typical way get fault resilience write disk replicate disk across network three times using HDFS makes suitable machine learning data come memory stay doesn fit memory fine get paged disk needed point fit memory stay benefits process go data many times machine learning Almost every machine learning algorithm needs go data tens hundreds times DB see Spark vis-a-vis MapReduce place different kinds workloads jobs RZ clear Hadoop ecosystem going thrive around long time think true MapReduce component Hadoop ecosystem regard MapReduce answer question think honestly think re starting new workload makes sense start MapReduce unless existing code base need maintain reason kind silly thing MapReduce days difference assembly C doesn make sense write assembly code write C code DB Spark headed RZ Spark pretty stable right biggest changes improvements happening right happening next couple years libraries machine learning library graph processing library SQL library streaming libraries rapidly developed every single one exciting roadmap next two years least features want nice see easily implemented m also excited community-driven contributions aren general enough put Spark support Spark community-driven set packages http spark-packages org think also helpful long-tail users time think Spark become de facto distribution engine build machine learning algorithms especially scale tags Big Data Big Data Artificial Intelligence Intelligence Matters data science data scientists neural networks spark Comment Get O Reilly Data Newsletter Stay informed Receive weekly insight industry insiders Featured Video Data science going - DJ Patil U government's first Chief Data Scientist looks future data science Strata Hadoop World 2015 San Jose Get Data Newsletter Stay informed Receive weekly insight industry insiders Featured Download Download free report free reports Recent Posts Real-world interfaces awkward playful stage finance disrupted Validating data models Kafka-based pipelines Four short links 28 May 2015 Protecting health open data management principles Recently Discussed Archives Archives Month May 2015 April 2015 March 2015 February 2015 January 2015 December 2014 November 2014 October 2014 September 2014 August 2014 July 2014 June 2014 May 2014 April 2014 March 2014 February 2014 January 2014 December 2013 November 2013 October 2013 September 2013 August 2013 July 2013 June 2013 May 2013 April 2013 March 2013 February 2013 January 2013 December 2012 November 2012 October 2012 September 2012 August 2012 July 2012 June 2012 May 2012 April 2012 March 2012 February 2012 January 2012 December 2011 November 2011 October 2011 September 2011 August 2011 July 2011 June 2011 May 2011 April 2011 March 2011 February 2011 January 2011 December 2010 November 2010 October 2010 September 2010 August 2010 July 2010 June 2010 May 2010 April 2010 March 2010 February 2010 January 2010 December 2009 November 2009 October 2009 September 2009 August 2009 July 2009 June 2009 May 2009 April 2009 March 2009 February 2009 January 2009 December 2008 November 2008 October 2008 September 2008 August 2008 July 2008 June 2008 May 2008 April 2008 March 2008 February 2008 January 2008 December 2007 November 2007 October 2007 September 2007 August 2007 July 2007 June 2007 May 2007 April 2007 March 2007 February 2007 January 2007 December 2006 November 2006 October 2006 September 2006 August 2006 July 2006 June 2006 May 2006 April 2006 March 2006 February 2006 January 2006 December 2005 November 2005 October 2005 September 2005 August 2005 July 2005 June 2005 May 2005 April 2005 March 2005 Archives Topic Data Design Emerging Tech IoT Programming Web Ops Performance Web Platform Archives Author Sinan Unur Aaron Sumner Adam DuVander Adam Flaherty Adam Messinger Adam Witwer Adrian Mendoza Alasdair Allan Alex Bordei Alex Bowyer Alex Iskold Alexander Macgillivray Alice Zheng Alistair Croll Allen Downey Allen Noren Allison Randal Ally MacDonald Alois Reitbauer Alysa Hutnik Amelia Bellamy-Royds Amr Awadallah Amy Heineike Amy Jollymore Amy Unruh Anant Jhingran Andreas Antonopoulos Andrew Collette Andrew Odewahn Andrew Savikas Andrew Shafer Andrew Baker Andy Fitzgerald Andy Kirk Andy Konwinski Andy Oram Angela Rufino Ann Spencer Ann Waldo Anna Smith Anne Gentle Anni Ylagan Ari Gesher Aria Haghighi Ariya Hidayat Arnold Robbins Artur Bergman Arun Gupta Audrey Watters Avi Bryant Barb Edson Barbara Bermes Baron Schwartz Barry Devlin Barry O'Reilly Beau Cronin Ben Christensen Ben Evans Ben Henick Ben Lorica Benjamin Hindman Bill Higgins Bill Lubanovic Bill McCoy Bonnie Feldman Bradley Voytek Brady Forrest Brandon Satrom Brett McLaughlin Brett Sandusky Brett Sheppard Brian Ahier Brian Anderson Brian Boyer brian d foy Brian d'Alessandro Brian Foster Brian Jepson Brian Kardell Brian MacDonald Brian O'Leary Brian Sawyer Brigitte Piniewski Bruce Stewart Carin Meier Carl Hewitt Carl Malamud Cathy O'Neil Chao Ray Feng Chiu-ki Chan Chris Cornutt Chris Meade Chris Vander Mey Chris Wiggins Christine Perey Ciara Byrne Cliff Miller Colt McAnlis Cornelia L vy-Bencheton Cory Doctorow Courtney Nash Dale Dougherty Dan Saffer Danese Cooper Darren Barefoot Dave Himrod Dave McClure Dave Zwieback David Beyer David Cranor David Elfi David Leinweber David Recordon David Sims David Stephenson DC Denison Deni Auclair Derek Jacoby Dinesh Subhraveti Dino Esposito DJ Patil Doug Finke Doug Hill Dr Venkat Subramaniam Drew Dara-Abrams Duncan Ross Dusty Phillips DW Wheeler Dylan Field E Vander Veer Edd Dumbill Edie Freedman Eli Goodman Elisabeth Robson Elizabeth Corcoran Ellen Friedman Elliott Hauser Elliotte Rusty Harold Emma Jane Westby Eoin Purcell Eric Redmond Eric Ries Ezra Haber Glenn Faye Williams Federico Castanedo Federico Lucifredi Fred Trotter Fred van den Bosch Gabe Zichermann Gavin Starks George Reese Gilad Rosner Glen Martin Greg Whisenant Gretchen Giles Gustavo Franco Gwen Shapira Hadley Wickham Hari K Gottipati Heather McCormack Helen Papagiannis Hew Wolff Howard Wen Hugh McGuire Ilya Grigorik Imran Ali J Paul Reed James Bridle James Turner Janaya Williams Jane Sarasohn-Kahn Jason Grigsby Jason Strimpel Jay Kreps Jay McGavren Jayant Shekar Jeevan Padiyar Jeff Gothelf Jeff Needham Jeffrey Carr Jeffrey Carr Jenn Webb Jennifer Pahlka Jeremy Freeman Jeremy Howard Jesper Andersen Jesse Anderson Jesse Robbins Jessica McKellar Jesus M Gonzalez-Barahona Jez Humble Jim Scott Jim Stogdill Jimmy Guterman Jo Prichard Joanne Molesky Jodee Rich Joe Procopio Johan Bergstr m John Adams John Allspaw John Battelle John Boxall John Feland John Foreman John Geraci John Graham-Cumming John King John Labovitz John Lindquist John Myles White John Piekos John Russell John Warren John Wilbanks Jon Bruner Jon Callas Jon Cowie Jon Roberts Jon Spinney Jon Udell Jonas Luster Jonathan Alexander Jonathan Reichental Ph D Jonathon Thurman Jono Bacon Joseph Hellerstein Joseph J Esposito Josh Lockhart Josh Simmons Joshua-Mich le Ross Joy Beatty Jud Valeski Julie Steele Justin Dombrowski Justin Hall Justo Hidalgo Karl Fogel Kassia Krozser Kat Meyer Kate Eltham Kate Pullinger Kathryn Barrett Kathy Sierra Kathy Walrath Katie Cunningham Katie Miller Keith Comito Keith Fahlgren Ken Yarmosh Kevin Shockey Kevin Sitto Kevin Smokler Khaled El Emam Kieren James-Lubin Kipp Bradford Kit Seeborg Kiyoto Tamura kmatsudaira Kurt Cagle Lara Swanson Laura Dawson Laura Klein Laurel Ruma Laurie Petrycki Leigh Dodds Liliana Bounegru Linda Stone Lisa Mann Liza Daly Lorna Jane Mitchell Lorne Lantz Luciano Ramalho Lucy Gray Lukas Biewald Mac Slocum Madhusudhan Konda Mandi Walls Manish Lachwani Marc Goodman Marc Hedlund Marie Beaugureau Marie Bjerede Mark Drapeau Mark Grover Mark Jeftovic Mark Lutz Mark Nelson Mark Pacelle Mark Sigal Marko Gargenta Martin Kalin Martin Kleppmann Mary Treseler Matt Garrish Matt Makai Matt Neuburg Matt Wood Matthew Burton Matthew Gast Matthew McCullough Matthew Russell Matthew Russell Max Kanat-Alexander Max Meyers Max Shron Meghan Athavale Meghan Blanchette Mehdi Daoudi Michael DeHaan Michael Driscoll Michael Ferrari Michael Freeman Michael Gold Michael Hunger Michael Jon Jensen Michael Lopp Michael McMillan Michael Scroggins Mike Amundsen Mike Barlow Mike Hendrickson Mike Honda Mike Loukides Mike Petrovich Mike Shatzkin Mitchell Hashimoto Naomi Robbins Nat Torkington Nate Osit Nathan Jepson Neal Ford Nicholas Tollervey Nick Bilton Nick Farina Nick Kolegraff Nick Lombardi Nick Ruffilo Nicolas Garcia Belmonte Nikolaj Nyholm O'Reilly Radar O'Reilly Strata Ohad Samet Osman Rashid Pablo Francisco Arrieta Gomez Paco Nathan Pamela Samuelson Paris Buttfield-Addison Patrick Mulder Patrick Reynolds Paul Kedrofsky Paul Spinrad Pete Hodgson Pete Warden Peter Arijs Peter Bennett Peter Cooper Peter Krautzberger Peter Laflin Peter Lewis Peter Meyers Philip Guo Philipp Janert Q Ethan McCallum Quinn Norton Rachel Roumeliotis Rael Dornfest Raffael Marty Rajat Bhargava Ramez Naam Randy Bias Raven Zachary Ray DiGiacomo Jr Renee DiResta Reynold Xin Richard Cook Richard Dallaway Richard Reese Richard Warburton Rob Tucker Robbie Allen Robert Kaye Robert Passarella Roberta Cairney Roger Chen Roger Magoulas Rogier DocWilco Mulhuijzen Ron Miller Roseanne Fallin Rune Madsen Russell J Dyer Ryan Bethencourt Ryan Neufeld Ryan Stewart Sam Newman Samuel Mullen Sanders Kleinfeld Sara Peyton Sara Winge Sarah Milstein Sarah Novotny Scott Jenson Scott Murray Scott Rich Scott Ruthfield Sean McGregor Sean O Sullivan bastien Pierre Semmy Purewal Seth Ladd Shahid Shah Shahin Farshchi Shai Almog Shannon Cutt Shyam Seshadri Silona Bonewald Simon Chan Simon Phipps Simon St Laurent Simon Wardley Spencer Critchley Stefan Thies Stephen Elston Stephen O'Grady Stephen O'Grady Steve Souders Steven Citron-Pousty Steven Shorrock Stoyan Stefanov Suzanne Axtell Tara Hunt Terrence Dorsey Terry Jones Tim Anderson Tim Busbice Tim Darling Tim O'Reilly Timothy M O'Brien Timothy McGovern Tish Shute Toby Inkster Todd Sattersten Tom Eisenmann Tom Steinberg Tony Quartarolo Trisha Gee Troy Topnik Tyler Bell Valeri Karpov Vandad Nahvandipoor Vanessa Fox Varun Nagaraj Cukierski William Mougayar William O'Connor Zigurd Mednieks CONTACT US Radar managing editor Jenn Webb Sign today receive special discounts product alerts news O'Reilly Privacy Policy View Sample Newsletter Twitter YouTube Slideshare Facebook Google RSS View RSS Feeds 2015 O'Reilly Media Inc 707 827-7019 800 889-8969 trademarks registered trademarks appearing oreilly com property respective owners O'Reilly O Reilly Radar Radar Contributors Academic Solutions Jobs Contacts Corporate Information Press Room Privacy Policy Terms Service Writing O Reilly Editorial Independence Community Authors Community Featured Users Forums Membership Newsletters O Reilly Answers RSS Feeds O Reilly Chimera beta Partner Sites makezine com makerfaire com craftzine com igniteshow com PayPal Developer Zone O Reilly Insights Forbes com Shop O'Reilly Customer Service Contact Us Shipping Information Ordering Payment Affiliate Program O Reilly Guarantee close Get O Reilly Data Newsletter Stay informed Receive weekly insight industry insiders"),
('KDD CUP 2015 - MOOC dropout prediction', "Login Sign XuetangX 20 000 432 Teams KDD Cup 2015 - Predicting dropouts MOOC Start End First Submission Deadline29 Jun 2015 05 00 00 Team Merger Deadline23 Jun 2015 05 00 00 1 months 3 Days go Information Introduction Evaluation Timeline Prizes Organizers Q Submission Data Rules Make Submission Submission Social Rank 1 Random Walker 2 CLMS 3 orange 4 CM 5 ESBL questions comments please send email support kddcup2015 com Background Students' high dropout rate MOOC platforms heavily criticized predicting likelihood dropout would useful maintaining encouraging students' learning activities Therefore KDD Cup 2015 predict dropout XuetangX one largest MOOC platforms China Description competition participants need predict whether user drop course within next 10 days based prior activities user C leaves records course C log next 10 days define dropout course C details log please refer Data Descriptions XuetangX XuetangX Chinese MOOC learning platform initiated Tsinghua University officially launched online Oct 10th 2013 April 2014 XuetangX signed contract edX one biggest global MOOC learning platform co-founded Harvard University MIT acquire exclusive authorization edX high-quality international courses December 2014 XuetangX signed Memorandum Cooperation FUN national MOOC platform France make bilateral effort course construction platform development aspects far 100 Chinese courses 260 international courses available XuetangX Contact Login Remember forget change password Sign individual may one single account Displayed Name Real Name real name won't displayed However must type real name participate competition E-mail Address use email login Password Confirm Password Verification code email sent Please go email click link complete registration Send mail failed"),
("Looking for an R equivalent of sklearn's PolynomialFeatures function", 'Home Installation Documentation Scikit-learn 0 16 Stable Tutorials User guide API FAQ Development Scikit-learn 0 15 Scikit-learn 0 14 Scikit-learn 0 13 versions Examples Previous sklearn preproce sklearn preprocessing StandardScaler Next sklearn preproce sklearn preprocessing add_dummy_feature Reference Reference documentation scikit-learn version 0 16 1 versions use software please consider citing scikit-learn sklearn preprocessing PolynomialFeatures Examples using sklearn preprocessing PolynomialFeatures sklearn preprocessing PolynomialFeatures class sklearn preprocessing PolynomialFeatures degree 2 interaction_only False include_bias True source Generate polynomial interaction features Generate new feature matrix consisting polynomial combinations features degree less equal specified degree example input sample two dimensional form b degree-2 polynomial features 1 b 2 ab b 2 Parameters degree integer degree polynomial features Default 2 interaction_only boolean default False true interaction features produced features products degree distinct input features x 1 2 x 0 x 2 3 etc include_bias boolean True default include bias column feature polynomial powers zero e column ones - acts intercept term linear model Attributes powers_ array shape n_input_features n_output_features powers_ j exponent jth input ith output n_input_features_ int total number input features n_output_features_ int total number polynomial output features number output features computed iterating suitably sized combinations input features Notes aware number features output array scales polynomially number features input array exponentially degree High degrees cause overfitting See examples linear_model plot_polynomial_interpolation py Examples X np arange 6 reshape 3 2 X array 0 1 2 3 4 5 poly PolynomialFeatures 2 poly fit_transform X array 1 0 1 0 0 1 1 2 3 4 6 9 1 4 5 16 20 25 poly PolynomialFeatures interaction_only True poly fit_transform X array 1 0 1 0 1 2 3 6 1 4 5 20 Methods fit X y Compute number output features fit_transform X y Fit data transform get_params deep Get parameters estimator set_params params Set parameters estimator transform X y Transform data polynomial features __init__ degree 2 interaction_only False include_bias True source fit X y None source Compute number output features fit_transform X y None fit_params source Fit data transform Fits transformer X y optional parameters fit_params returns transformed version X Parameters X numpy array shape n_samples n_features Training set y numpy array shape n_samples Target values Returns X_new numpy array shape n_samples n_features_new Transformed array get_params deep True source Get parameters estimator Parameters deep boolean optional True return parameters estimator contained subobjects estimators Returns params mapping string Parameter names mapped values set_params params source Set parameters estimator method works simple estimators well nested objects pipelines former parameters form component __ parameter possible update component nested object Returns self transform X y None source Transform data polynomial features Parameters X array shape n_samples n_features data transform row row Returns XP np ndarray shape n_samples NP matrix features NP number polynomial features generated combination inputs Examples using sklearn preprocessing PolynomialFeatures Polynomial interpolation Robust linear estimator fitting Underfitting vs Overfitting 2010 - 2014 scikit-learn developers BSD License Show page source Previous Next'),
('The connection between sensing, signal processing and machine learning', "Nuit Blanche name Igor Carron homepage Page Views Nuit Blanche since July 2010 Follow IgorCarron Cite Nuit Blanche related pages recent Compressive Sensing article Scientific Reports Attendant Project Page Please join comment Google Community 1502 CompressiveSensing subreddit 811 Facebook page LinkedIn Compressive Sensing group 3293 Advanced Matrix Factorization Group 1017 Reference pages include Big Picture Compressive Sensing Learning Compressive Sensing Advanced Matrix Factorization Jungle Page Highly Technical Reference Pages - Aggregators Technologies Exist CAI Cable Igor's Adventures Matrix Factorization search Reproducible Research page Paris Machine Learning Meetup Archives Meetup com register 2222 members LinkedIn post jobs 721 Google 233 Facebook follow-on discussions Twitter Monday June 17 2013 Sunday Morning Insight Quick Panorama Sensing Direct Imaging Machine Learning Email ThisBlogThis Share TwitterShare FacebookShare Pinterest sensor sensing fit Machine Learning common question think arise fact fields becoming specific thereby attenuating common sense big picture entails forewarned following brief exposition making numerous shortcuts abuse notation Everything starts simple notion see get x x words one side x sense x picture applies directly call direct imaging want image something sample mathy expression process simple x x identity yes let's talk matrices operators explanation requires Let us note sometimes remove certain rows applying restriction operator R undersampling Reconstructing full x subsampled version RIx called interpolation inpainting Let us call inpainting operator x R Ix far good Let us also note optics generally following x BA x BA generally represents lenses camera words engineering behind lens design using tools like Zemax geared towards designing BA also called PSF close possible identity sensor measurement process obtains BA x B linear operators process actually little complex see 1 credit NASA case indirect imaging attempted since early 1960s coded aperture instance x L Ax L linear operators sensors measure Ax x matrix multiplication away getting x back Famous examples include coded aperture X-ray imaging orbital observatories also CT MRI Encryption also fits model Credit Rice University compressive sensing things get little complicated indirect imaging twist x N Ax even x N Bx B linear operators x sparse compressible canonical basis bases N nonlinear operator indirect imaging sensed Ax Bx difference new approach traditional indirect imaging lies nonlinearity reconstruction operator N compressive sensing seek rectangular subsampling nonlinearity N comes fact seek sparse x's Historically N initially tied linear convex programming greedy methods different approaches IHT eventually AMP Additional insight found 2 Muthu see 3 points ought quantities could extract linear sensing mechanism instead x N Ax ought look Hx N Ax sensor obtains Ax N nonlinear reconstruction solver sorts reconstruct Hx hopefully important feature x compressive sensing feature sparsity pattern x Instead full reconstruction aim detecting items interest optics sometimes talk task specific imagers detection features manifold signal processing MRI call fingerprinting Theoretical Computer Science concerned statistics x since streaming model cannot even store x process goes name Heavy Hitters related algorithms looks like homomorphic encryption would fit model One could also argue current conventional direct imaging also falls category see 1 let us go making first sensing operator nonlinear one approach typically seen nonlinear compressive sensing also machine learning x N2 N1 x Unsupervised Feature Learning Deep Learning tutorial machine learning particular neural networks x N4 N3 N2 N1 x sort successful construct known autoencoders deep learning operator N nonlinear specific choice regularization layers nodes even make look like compressive sensing approach case talk sparse autoencoders e roughly speaking sparse set weights every images part training set neural network trained traditional sensing also natural nonlinear sampling schemes one introduced quantization see Petros' resource Quantization FROG simply found phase retrieval problems Spatio-Temporal Convolutional Sparse Auto-Encoder Sequence Classification Moez Baccouche traditional area Machine Learning classification Hx N4 N3 N2 N1 x Hx represents whether x belongs class 1 -1 seen recently connection Compressive Sensing Machine Learning stems large part nonlinearities involved sampling stage specifically grab features interest guided preferable nonlinearities one comes back linear case interesting information might eventually useful designing nonlinearities sampling system 4 show suggested instance optimality equivalent generalized null space property M discuss possible relations generalized restricted isometry properties also lessons learned 5 time involves low rank issues condition nonlinearity beginning seeing two fields merging 1 Let us note entirely true conventional imaging 2D scene transformed 2D scene whereas 3D scene transformed 2D scene AB really identity sort projection Let us note conventional direct imaging things really complicated AB really two nonlinear operators include nonlinearities FPA quantization issues compression JPEG one reason Mark Neifeld tells us already using compressive systems talk entitled Adaptation Task-Specific Compressive Imaging 2 One note rapid convergence algorithms makes one think could parallel neural networks conditions x N although better acknowledged direct imaging case really restriction due concept sparsity indeed solvers N using l_infty l_p norms regularizers find signal certain structure within set signals destroyed B underdetermined system rectangular saying essence performing measurement Ax losing information x 3 Muthu's 2010 Massive Data Streams Research Go Compressed Functional Sensing Streaming compressed sensing brought two groups researchers CS signal processing together common problems minimal amount data sensed captured stored data sources reconstructed least approximately productive development research fundamental insights geometry high dimensional spaces well Uncertainty Principle addition Engineering Industry impacted signi cantly analog information paradigm however beginning need extend compressed sensing functional sensing sense appropriate compute different function rather simply reconstructing furthermore extend theory massively distributed continual framework truly useful new massive data applications 4 Compressed Sensing Best Approximation Unions Subspaces Beyond Dictionaries Tomer Peleg R mi Gribonval Mike E Davies propose theoretical study conditions guaranteeing decoder obtain optimal signal recovery underdetermined set linear measurements special type performance guarantee termed instance optimality typically related certain properties dimensionality-reducing matrix M work extends traditional results sparse recovery instance optimality expressed respect set sparse vectors replac- ing set arbitrary finite union subspaces show suggested instance optimality equivalent generalized null space property M discuss possible relations generalized restricted isometry properties 5 Learning non-parametric basis independent models point queries via low-rank methods Tyagi Hemant Cevher Volkan Join CompressiveSensing subreddit Google Community post Liked entry subscribe Nuit Blanche's feed there's came also subscribe Nuit Blanche Email explore Big Picture Compressive Sensing Matrix Factorization Jungle join conversations compressive sensing advanced matrix factorization calibration issues Linkedin labels CS SundayMorningInsight Igor 6 17 2013 05 20 00 comments Post Comment Newer Post Older Post Home Subscribe Post Comments Atom Printfriendly Nuit Blanche Stemming Data Tsunami One Algorithm Time Tweet Igor Search Nuit Blanche Loading Subscribe E-MAIL Nuit Blanche Get new entries directly mailbox enter email address Nuit Blanche Nuit Blanche blog focuses Compressive Sensing Advanced Matrix Factorization Techniques Machine Learning well many engaging ideas techniques needed handle make sense high dimensional data also known Big Data Nuit Blanche french expression translates nighter restless night Contact Cassini MSL Oppy HiRISE SOHO SDO Rosetta-Philae Rosetta Contact igorcarron gmail com Webon LinkedIn Twitter Pages Home Reproducible Research implementations Randomized Numerical Linear Algebra RandNLA Advanced Matrix Factorization Learning Compressed Sensing It's CAI Cable Igor's Adventures Matrix Factorization Machine Learning Meetups Around World Compressed Sensing Pages Focused Interest Pages Datasets Challenges Nuit Blanche Conversations Linking Nuit Blanche blogs CS Meetings Real Time Experiments Highly Technical Reference Pages - Aggregators Recent Nuit Blanche entries Paris Machine Learning Meetup Archives Pinterest Boards Imaging Nature Technologies Exist Wondering Star Computational Photography Subscribe LinkedIn Matrix Factorization Group 1001 members right one Link stats Subscribe Nuit Blanche RSS Feed Posts Atom Posts Comments Atom Comments Subscribe LinkedIn Compressive Sensing group 3145 members right one Link stats Google Badge updated profile LinkedIn reflect activities Nuit Blanche means provide recommendations based experience reading blog Nuit Blanche QR code Search Nuit Blanche LoadingLatest news Compressive Compressed Sensing Arxiv Full Text Search Arxiv Google Compressive Sensing Compressed Sensing 24 hours week month Rice University Compressive Sensing repositoryLatest news Matrix Factorization Arxiv old Arxiv new Google 24 hours week month Readership Statistics another set watching blog feedreaders 740 readers receive every entries mailboxes 600 people come site directly everyday detailed information following blog entries far site seen 3 500 000 pageviews since counter installed 2007 Nuit Blanche Referenced Dead Tree World Big Picture Compressive Sensing mentioned article La Recherche french speaking equivalent competitor Science October 2010 issue page 20-21 Wired Magazine piece Compressed Sensing featuring links blog Big Picture March 1 2010 Emmanuel Candes Terry Tao wrote Nuit Blanche Dec '08 issue IEEE Information Theory Society Newsletter Xiaochuan Pan Emil Sidky Michael Vannier wrote Nuit Blanche commercial CT scanners still employ traditional filtered back-projection image reconstruction Check also acknowledgments Ghost Imaging paper one Like Link Xi'an's Og Toscana 3 - Filed Mountains pictures Running Travel Wines Tagged Chianti farmhouse Italia ruins sunset Tuscany 36 minutes ago Hack Day Eye-Controlled Wheelchair Advances Talented Teenage Hackers - Myrijam Stoetzer friend Paul Foltin 14 15 years old kids Duisburg Germany working eye movement controller wheel chair Th 53 minutes ago Terahertz Technology Abstract-Terahertz response patterned epitaxial graphene - Christian Sorger Sascha Preu1 2 Johannes Schmidt3 Stephan Winnerl3 Yuliy V Bludov4 Nuno M R Peres4 Mikhail Vasilevskiy4 Heiko B Weber1 http 1 hour ago Endeavour Data code regulation - Data code code data distinction software code input data blurry best arbitrary worst distinction 6 hours ago Another Word Cybersecurity Authoritative Reports Resources Topic Need Librarians - Cybersecurity Authoritative Reports Resources Topic Rita Tehan Information Specialist Congressional Research Service summary 18 hours ago Image Sensors World Sony 2015 IR Day - Sony held 2015 Investor Relations Day today Device Segment presentation Tomoyuki Suzuki Executive Deputy President Corporate Executive Offic 21 hours ago free hunch Interactive R Tutorial Machine Learning Titanic Competition - Always wanted compete Kaggle competition sure right skill set DataCamp created free interactive tutorial help 21 hours ago High Noon GMT Oh torn 'twixt love an' tenure Behold molten children Recurrent Neural Networks Generating Even Word God - Please note post taken seriously show artificial system learned enough lot text generate new text 2 days ago What's new Cancellation multilinear Hilbert transform - ve uploaded arXiv paper Cancellation multilinear Hilbert transform submitted Collectanea Mathematica paper uses method 2 days ago G del's Lost Letter P NP John Alicia Nash 1928 1933 2015 - condolences Awesome Stories source John Nash wife Alicia killed taxi accident New Jersey Turnpike Saturday afternoon wer 2 days ago Timothy Lottes OS Project 7 - PS 2 Misc - started back long break Finished PS 2 driver Simplified keyboard interface one 64-bit bit array memory simple p 3 days ago Ergodic Walk AISTATS 2015 talks one day - attended AISTATS day change year unfortunately due teaching missed poster Shuang Song presented work 3 days ago Herve La cuisine cr erait-elle son objet - La chimie cr e son objet la phrase est paradoxale dit qu'elle est du chimiste Marcellin Berthelot mais est-elle vraiment de lui Voir Marcellin 3 days ago ChapterZero quick thought Supernatural tv shows - finished season 9 Supernatural ve got give show credit one demands deus ex machina ending Anything less 3 days ago Mr Vacuum Tube Phased Array Radar Looking Walls - Phased Array Radar Looking Walls http blog array2016 org p 24 5 days ago Walking Randomly MATLAB Vectorisation double-edged sword - Imagine new MATLAB programmer create N x N matrix called j j first attempt solution might 6 days ago slice pizza Morning Madness Ode Mercedes - morning madness drive along skyline Cutting fog thick stew waves dew 70mph hugging memorized curves without 1 week ago Decision Science News Gelman sense dubious Science article - Statistician Andrew Gelman sense something dubious Science article soon published post Gelman sense 1 week ago 0xDE Graham Erd Egyptian fractions - recent paper Ron Graham surveys work Paul Erd Egyptian fractions know Erd s' second paper subject didn't p 1 week ago Geomblog ITA conference really enjoy - Continuing thoughts STOC 2017 reboot went back Boaz's original question would make likely go STOC thought I'd 1 week ago Pillow Lab Blog Fast Kronecker trick Gaussian Process regression expressive kernels - May 11th presented following paper lab meeting Fast Kernel Learning Multidimensional Pattern Extrapolation Andrew Gordon Wilson Elad Gil 2 weeks ago Machine Learning etc ICLR 2015 - ICLR posters caught eye larger image simple implement idea gives impressive results force two groups units un 2 weeks ago Libres pens es d'un math maticien ordinaire adventure Google search - interesting experience Google recently related Electronic Journal Probability EJP Electronic Communications Probability ECP 2 weeks ago tombone's blog Dyson 360 Eye Baidu Deep Learning Embedded Vision Summit Santa Clara - Bringing Computer Vision Consumer Mike Aldred Electronics Lead Dyson Ltd vision research priority decades results 3 weeks ago Secrets Consulting Requirements Hints Variations - volumes Exploring Requirements follow chapter section hints variations topic chapter Many readers tel 3 weeks ago Harvest Imaging Blog Third HARVEST IMAGING FORUM December 2015 - successful forums 2013 2014 third one organized December 2015 Voorburg Hague Netherlands basic inte 3 weeks ago m bandit COLT 2015 accepted papers cool videos - Like last year compiled list COLT 2015 accepted papers together links arxiv version whenever could find one papers 3 weeks ago Information Structuralist Counting bits Vapnik Chervonenkis - Machine learning enabling computers improve performance given task get data express intuition quantitative 4 weeks ago Le Petit Chercheur Illustr Quasi-isometric embeddings vector sets quantized sub-Gaussian projections - Last January honored invited RWTH Aachen University Holger Rauhut Sjoerd Dirksen give talk general topic quantized co 4 weeks ago Machine Learning Theory Randomized experimentation - One good thing machine learning present people actually use back-ends many systems interact daily basis 5 weeks ago La vertu d'un LA virtue - fortunate hive D dom nologie la science du traitement de donn es signal images etc - O l'on propose le n ologisme d dom nologie pour d signer la technique la pratique la science du traitement de signal et de l'analyse d'images au c 1 month ago robots net Robots Podcast Farewell robots net join us Robohub - Since May 2007 colleagues Robots Podcast Robohub working robots net bring latest news views robo 1 month ago Machine Learning Deep Learning Works II Renormalization Group - Deep Learning amazing Deep Learning successful Deep Learning old-school Neural Networks modern hardware w 1 month ago Follow Data Genomics Today Tomorrow presentation - Slideshare link widget presentation gave Genomics Today Tomorrow event Uppsala couple weeks ago March 19 2015 sp 1 month ago Adventures Signal Processing Open Science Open Access Journals Missing - end could value proposition future journals 1 month ago Thoughts Mysterious Universe State Probabilistic Programming - two weeks last July cocooned hotel Portland living breathing probabilistic programming student probabilistic p 1 month ago Epistasis Blog Biomedical Informatics Faculty Positions University Pennsylvania - recently moved research lab Perelman School Medicine University Pennsylvania serve Director Institute Biomed 2 months ago Petros Boufounos Internship Opening Sensor Fusion - new internship opening MERL area sensor fusion posting follows MM880 Sensor fusion MERL looking well qualified 2 months ago G-media Le blog Nouveaut d veloppeurs domotique et openpicus Prise Gigogne et Dolphin View - Dans cet article nous allons voir comment utiliser la prise gigogne avec mesure de comptage depuis le logiciel Dolphin View Mat riel n cessaire une c 2 months ago Neurevolution Neurevolution relaunch - hard believe started blog eight years ago way back grad students long way ve come Patryk Dir 2 months ago Inductio Ex Machina Upcoming Conference Sydney Predictive APIs Apps - big kick-off Barcelona last year 200 attendees second International Conference Predictive APIs Apps held Sydn 2 months ago Ars Mathematica Nine Chapters Semigroup Art - Googling something came across Nine Chapters Semigroup Art leisurely introduction theory semigroups 2 months ago yellow noise takis champs magnetiques palais de tokyo february 2015 - takis champs magnetiques palais de tokyo february 2015 3 months ago polylogblog CPM 2015 - Ely asked remind everyone deadline 26th Annual Symposium Combinatorial Pattern Matching fast approaching 2nd F 4 months ago Mirror Image Kinect depth sensor works stereo triangulation simple ways speed convnet little - quite complex methods making convolutional networks converge faster Natural gradient dual coordinate ascent second order hessian fre 5 months ago natural language processing blog myth strong baseline - probably count fingers number papers I've submitted reviewer hasn't complained baseline way don't mean 6 months ago Building Intelligent Probabilistic Systems Harvard Center Research Computation Society Call Fellows Visiting Scholars - Harvard Center Research Computation Society CRCS solicits applications Postdoctoral Fellows Visiting Scholars Programs 7 months ago Pixel shaker Pics Manipulated Photos Notable Historic Figures Digital Era Images - Manipulating photos happened way Photoshop around series shows afters famous notable figures digital era 7 months ago Victoria Stodden input OSTP RFI reproducibility - Sept 23 2014 US Office Science Technology Policy Whitehouse accepting comments Strategy American Innovation 8 months ago Ga l Varoquaux Hiring engineer mine large brain connectivity databases - Work us leverage leading-edge machine learning neuroimaging Parietal research team work improving way brain images analyz 8 months ago Computers don't see Compiling OpenCV 3 0 alpha CUDA support MacOS X - quick tip people troubles compiling OpenCV 3 0 alpha MacOS X variable called CUDA_TOOLKIT_DIR cmake configura 8 months ago Herr Strathmann - home Shogun NYC - late August invited NYC present Shogun open-source Machine Learning software workshop link organised John Langford Seeing Sh 8 months ago Lousodrome Vie au Japon Le certificat de r sidence j minhy - Le certificat de r sidence j minhy en japonais est un simple document d une page qui certifie votre adresse de r sidence et qui est demand pour 10 months ago trekkinglemon's fresh squeeze Data processing flashy yellow Peyresq 2014 Last day - 2014 edition Peyresq summer school finished coda let us summarize last talk Continue reading 10 months ago Camdp com updates DataOrigami Launch - I'm proud announce latest project dataorigami net still go check 11 months ago Neighborhood Infinity Cofree meets Free - - LANGUAGE RankNTypes MultiParamTypeClasses TypeOperators - Introduction spoke BayHac 2014 free monads asked co 1 year ago Statistical Trader Follow twitter StatTrader - Since working full time managing Data Sciences team Bloomberg Global Data haven't done sort long-form blogging used 1 year ago Ivan Oseledets homepage Introduction cross approximation - written short incomplete introductory page skeleton decomposition using maxvol algorithm update references 1 year ago MAKE Magazine New Project TV-B-Gone Kit - Tired LCD TVs everywhere Want break advertisements re trying eat Want zap screens across street TV-B-Go 1 year ago i2pi com focus - focus 1 year ago brain map statistics connection Linus Pauling fMRI - think Linus Pauling two things come mind work nature chemical bond awarded 1954 Nobel Prize Chem 1 year ago Computational Information Geometry Wonderland New blog address Moving Wordpress - use anymore blog system rather use Wordpress supports many goodies like latex Please update yo 1 year ago Martin Tall Gaze Interaction Introducing Eye Tribe Tracker - It's great pride today introduce Eye Tribe Tracker It's worlds smallest remote tracker first use USB3 0 one 100 1 year ago De Rerum Natura Functional programming - least interesting concept programming languages purity Java prime example Everything object much prefer Python wa 1 year ago Doyung Pig Hive Pig Hive - Hadoop ecosystem Data processing Pig Hive Pig Hive pig hive data 2 years ago BlackbordRMT Dyson Brownian motion thirty stationnary Brownian motions Ornstein Uhlenbeck - image Image Hosted ImageShack us Dyson Brownian motion thirty stationnary Brownian motions Ornstein Uhlenbeck processes constr 2 years ago Freakonometrics Mod lisation et pr vision cas d' cole - Quelques lignes de code que l'on reprendra au prochain cours avec une transformation en log et une tendance lin aire Consid rons la recherche du mot c 2 years ago Brain Windows WordPress accepts Bitcoin - WordPress blog platform accepts bitcoin hosting costs via BitPay WordPress hosts Brain Windows many world biggest best blogs 2 years ago Science Sands Science Sands moved - Dear readers migrated blog along things new site http davidketcheson info New posts longer appear blo 2 years ago Journey Randomness SMC sampler - monte carlo sequential monte carlo SMC sampler Jasra Doucet Del Moral 2 years ago Hao's TechBlog Sampling Rate Pixels compare sampling rate camera single-pixel camera - beginning I'd like make clear two terms Nyquist frequency Nyquist rate may take thing even text 2 years ago bpchesney org Nested Linear Programs Find Agreement among Sensors - Suppose 3 sensors B C set readings goes column matrix b matrix constructed bA bB bC repr 2 years ago Stupid Matlab Hacks busy recently hacks mean time go play - busy recently hacks mean time go play http www mathworks com matlabcentral fileexchange 37104-kappatau mak 2 years ago Becoming Astronaut Orbiters going - month NASA commenced delivery four Space Shuttle orbiters final destinations extensive decommissioning process 3 years ago FUTUREPICTURE Note comments - six months ago hit serious rash spam 20 000 comments posted span two weeks Unfortunately ti 3 years ago CyberGi Os ciborgues da Campus Party - Essa semana fui na quinta edi o da Campus Party e ontem dia 9 tive o prazer de conhecer dois ciborgues o Rob Spencer que fez o document rio Eyeborg q 3 years ago Collective Research Interaction Sound Signal Processing Sonification Handbook - yet heard Sonification Handbook edited Thomas Hermann Andy Hunt John G Neuhoff published even better freely avai 3 years ago Espace Vide PCA Compressive Measurements Video - I've little bit fun visualizations today outcomes pretty nice potentially artistic thought I'd share 3 years ago inspiration etc Session 4 5th Graders - Doodling 2 - Session 4 - Doodling Approximately one half hours Starting one element Adding proximity expanding drawing Tool 3 years ago Marcio Marim Welcome new website - time work second version didn publish happy release new website share creations interests whi 3 years ago Cognitive Radio Blog G Vazquez-Vilar PhD Thesis Interference Management Cognitive Radio - image Thesis Cognitive radio dissertation examination took place couple weeks ago Happily passed say one unexpect 3 years ago OISblog Liquid Crystal Eyeglasses - Pixel Optics introduced Empower eyeglasses use liquid crystal lenses actively adjust power 4 years ago Big Numbers Going commission - m going focus completely school blog going hiatus months ll start ve passed hurd 4 years ago Another Dimension three musketeers - Given vectors three quantities interesting indeed fact concept inner product vector space revolves largely around thos 4 years ago Electrons holes Indefinite hiatus - blog put indefinite hiatus 4 years ago Arthur Charpentier Blog transfert - mentioned past weeks blog transfered please update links bookmarks redirected shortly http freako 4 years ago Chaotic Pearls Indonesian Contoh Program Phase Unwrapping - Ide dari phase-unwraping PU progresif ini muncul di suatu sore hari ketika saya sedang berjalan-jalan di sekitar kampus sekitar tahun 2002-an Saya ber 4 years ago YALL1 ALgorithms L1 Announcements - June 4 2010 Toeplitz circulant sampling demos released June 4 2010 YALL1 version 1 0 released open-source Download link YALL1 n 4 years ago Hashimoto Laboratory's Blog Personal Mobility Next Level - Improving concept self-balancing unicycle Honda introduced brand new U3-X new personal mobility platform regular large whee 5 years ago Lianlin Li's Compressive Sensing blog Chinese Igor's Blog today - Today Post updated Igor's blog following CS Matrix Completion via Thresholding Dick Gordon's Op-Ed Lianlin L 5 years ago Guan Gui's blog expoit channel structure - 6 years ago Three-Toed Sloth - ML Counterexamples Pt 2 - Regression Post-PCA camdp com blogs - Willow Garage Blog Willow Garage - Latest News - KinectHacks net - Little Knowledge - Blog Robotics - Show 25 Show Another Blog List Haldane's Sieve SWEEPFINDER2 Increased sensitivity robustness flexibility - SWEEPFINDER2 Increased sensitivity robustness flexibility Michael DeGiorgio Christian D Huber Melissa J Hubisz Ines Hellmann Rasmus Nielsen Su 5 hours ago Information Processing John Nash dead 86 - original title post won Nobel Memorial Prize see sad news bottom Beautiful Mind Nash went see von Neuman 4 days ago Scientific Clearing House Selection week - Violinist Hilary Hahn hails Baltimore pianist Valentina Lisitsa play first movement American composer Charles Ives Fourth Sonata 6 days ago leon bottou org news news graph_transducer_networks_explained - Graph Transducer Networks explained scavenging old emails couple weeks ago found copy early technical report describes 2 weeks ago tombone's blog Dyson 360 Eye Baidu Deep Learning Embedded Vision Summit Santa Clara - Bringing Computer Vision Consumer Mike Aldred Electronics Lead Dyson Ltd vision research priority decades results 3 weeks ago Property Testing Review News April 2015 - April busy time property testing 9 papers posted online month let jump right Testing properties graphs Approximately Co 3 weeks ago Relax Conquer Courant Institute Mathematical Sciences - Finally longer job market excited announce join Courant Institute Mathematical Sciences Assistant Profess 1 month ago Moody Rd Competing data science contest without reading data - Machine learning competitions become extremely popular format solving prediction classification problems sorts famous ex 2 months ago AK Tech Blog Neustar SIAM SODA 2015 - Author Note Hello readers m Sonya Berg first post Neustar research blog data scientist Neustar Research foc 4 months ago Mostly linguistically computational Adventure collaborative filtering information retrieval matrix factorization stuff Count-Min-Log Strange effect - followed previous steps mentionning previous post odd behaviour Count-Min-Log MAX sampling w 5 months ago Deep Learning Recent Reddit AMA Deep Learning - Recently Geoffrey Hinton Yann Lecun Yoshua Bengio reddit AMA subscribers r MachineLearning asked questions AMA contains 6 months ago jim learning choose mentor - http www cell com neuron fulltext S0896-6273 13 00907-0 https www cs princeton edu courses archive spring15 cos598D http www cs princeton edu cours 1 year ago Blog Interphase Transport Phenomena Laboratory Texas M University Fun Boiling - 1 year ago Math Good another WordPress site Linear Algebra good - Linear algebra branch mathematics concerned solving linear systems natural think linear algebra solving unkno 1 year ago BayesRules STAT 330 November 29 2012 - finished discussion model selection averaging went missing data models cases fact data observed h 2 years ago Comments Lecture Schedule Embryo Physics Course - Blog List FlowingData Compare curve reality income versus college attendance - image Draw line grow poorer families less likely go college grow richer families likely 1 hour ago SynBioFromLeukipposInstitute Scoop Three developments help synthetic biology live promise Genetic Literacy Project - Three developments help synthetic biology live promise Dominic Basulto May 28 2015 Washington Post See Scoop via 2 hours ago Retraction Watch chocolate-diet sting study retracted coverage doesn surprise news watchdog - Yesterday John Bohannon described i09 com successfully created health news conducted flawed trial health benefits chocolate 4 hours ago Statistical Modeling Causal Inference Social Science Cracked com Huffington Post Wall Street Journal New York Times - David Christopher Bell goes trouble link Palko explain Every Map Popular _________ State Bullshit long 4 hours ago olimex MOD-LCD3310 OSHW monochrome LCD 84 48 pixels board UEXT connector - MOD-LCD3310 Open Source Hardware board released Apache 2 0 Licensee low cost 84 48 pixels LCD connect development bo 6 hours ago Science-Based Medicine Florida strikes Brian Clement - Brian Clement charlatan Unfortunately doesn seem problem State Florida made two turned three attempts g 12 hours ago Sage Open Source Mathematics Software Guiding principles SageMath Inc - February year 2015 founded Delaware C Corporation called SageMath Inc first stab guiding principles compan 21 hours ago Quomodocumque evil impulse good - learned teaching Rabbi Rebecca Ben-Gideon last week turning mind Rabbi Nahman said Rabbi Samuel name Behold 1 day ago Machine Vision 4 Users re backlighting cylindrical parts - see backlighting used time machine vision training classes trade shows typically gauging locating shapes Look closely though 1 day ago Skulls Stars comics moment inspires Suicide Squad - Update Forgot say thank Dad mailing complete Suicide Squad collection made whole post possible suspect peop 1 day ago InnoCentive Challenges Bioabsorbable Elastomeric Film - Seeker looking bio-absorbable elastomeric film specific properties could existing product one adapted 1 day ago What's new Cancellation multilinear Hilbert transform - ve uploaded arXiv paper Cancellation multilinear Hilbert transform submitted Collectanea Mathematica paper uses method 2 days ago Ben Krasnow physics floating screwdrivers - explain jet air float common screwdriver Plans make fluid turbulence disc http makezine com projects rheoscopic-coffee-tab 2 days ago Short Fat Matrices Three Paper Announcements - ve pretty busy lately writing researching visitors announcements serve quick summary ve 1 Tables 1 week ago 2Physics Current Fluctuations - left right Pierre Pfe er Fabian Hartmann Sven H ing Martin Kamp Lukas Worschech Authors Pierre Pfe er1 Fabian Hartmann1 Sven H ing1 2 1 week ago Large Scale Machine Learning Animals Open Data Science Conference - May 30 Boston - 1 week ago Zhilin's Scientific Journey Yann LeCun's Comments Extreme Learning Machine ELM - Yann LeCun https www facebook com yann lecun posts 10152872571572143 Facebook commented ELM quoted What's great Ext 2 weeks ago Gowers's Weblog Nick Clegg Liberal Democrat - life found Liberal Democrat policies Liberal-SDP Alliance policies Liberal policies n 4 weeks ago JeremyBlum com Shapeoko2 CNC Mill Build Log Review - new Inventables Shapeoko2 CNC mill carving away Watch timelapse build-log thorough review do-it-yourself CNC milling machine Continue 4 weeks ago F Pedregosa IPython Jupyter notebook gallery - Draft - TL DR created gallery IPython Jupyter notebooks Check - image Notebook gallery couple months ago put online website d 5 weeks ago Richard Baraniuk Probabilistic Theory Deep Learning - Patel Nguyen R G Baraniuk Probabilistic Theory Deep Learning arXiv preprint arxiv org abs 1504 00641 2 April 2014 grand challeng 1 month ago Welcome Sparse Land Discreteness Sparsity - Discrete signals may sparse whereas sparse signals may discrete Let us consider following signal x 1 1 -1 1 -1 -1 1 1 2 months ago Various Consequences Reliability Growth Enhancing Defense System Reliability - report pdf National academies reliability growth interesting There's lot good stuff design reliability physics fai 2 months ago Inductio Ex Machina Upcoming Conference Sydney Predictive APIs Apps - big kick-off Barcelona last year 200 attendees second International Conference Predictive APIs Apps held Sydn 2 months ago Highly Scalable Blog Data Mining Problems Retail - Retail one important business domains data science data mining applications prolific data numerous optimization p 2 months ago regularize Farewell Ernie Esser - Today learned Ernie Esser passed away March 8th 2015 age 34 Ernie PostDoc UBC Vancouver PhD UCLA worked applied math 2 months ago Thingiverse Blog MakerBot PrintShop 1 4 3D Print iPad 3G 4G - Last week introduced exciting new feature MakerBot 3D Ecosystem ability start 3D print remotely using smartphone MakerBot P 3 months ago much little time Post-doc Molecular Informatics Opening NCATS - post-doc opening Informatics group NCATS work computational aspects high throughput combination screening topics includ 3 months ago next big thing syndrome Books read 2014 - disclaimer book cover images post Amazon Affiliate links click buy book receive cents form 5 months ago Pursuits Null Space Blog done moved - first hesitant handling latex Rolf Mathcination pointed excellent tool latex-to-wordpress wil 5 months ago Proof Pudding M571 Fall 2014 Lecture 5 - 1 Agenda QR Factorization Gram-Schmidt classical modified Householder QR reflectors ended discussion projectors several impo 6 months ago Tianyi Zhou's Research Blog Learn Low-rank Sparse Structures via Randomized Alternating Projections List Submodular Optimization Streaming Data Update - Coresets k-Segmentation Streaming Data NIPS 2014 Streaming Submodular Optimization Massive Data Summarization Fly KDD 2014 8 months ago Wondering Star Lunar Detection Ultra-High-Energy Cosmic Rays Neutrinos - Spotted ArXiv Physics blog using SKA array Moon collector would certainly qualify sensors size pl 8 months ago Dan's Blog make paper spherical panorama - image image Photos usually show rectangular fragment scene image taken Typical panoramic images display landscap 8 months ago Seth's blog Videos Seth Roberts Memorial Talks - Video recordings Ancestral Health Society public talks August 10 2014 honoring Seth life work posted http bit ly 1v33kbM Many 9 months ago Thoughts Artificial Intelligence - blog moved new post metaphor mathematics new blog 9 months ago Lupi Software thoughts Monitorama 2014 PDX - Monitorama fantastic conference came mixed feelings Great work done open source software however based con 1 year ago Andrej Karpathy Blog Interview Data Science Weekly Neural Nets ConvNetJS - Quick post thought mention ve given interview two months ago ConvNetJS background perspectives neural 1 year ago Ivan Oseledets homepage Introduction cross approximation - written short incomplete introductory page skeleton decomposition using maxvol algorithm update references 1 year ago Artificial Intelligence Blog John Dobson 1915 2014 - September 2008 met John Harrisburg slept overnight friend Zoungy place enjoyed beautiful drive Cherry Springs th 1 year ago Hey What's BIG idea 3D Printing Tangible Idea - heywhatsthebigidea net B J Rao write another article 3D printing internet already offers abundance information subject Moun 1 year ago Normal Deviate END - addition best comedy TV show ever Seinfeld great source wisdom one episode Jerry counsels George hit high 1 year ago Math Good another WordPress site Linear Algebra good - Linear algebra branch mathematics concerned solving linear systems natural think linear algebra solving unkno 1 year ago tcs math - mathematics theoretical computer science Kadison-Singer Quantum Mechanics - accounts Kadison-Singer problem mention arose considerations foundations quantum mechanics Specifically question abo 1 year ago programming machine learning blog BibTeX-powered publications list Pelican pelican-bibtex - Hook Wouldn like manage academic publications list easily within context static website Without resorting external services 2 years ago Mathblogging org -- Blog Mathematical Instruments Gianluigi Filippelli - post part series Mathematical Instruments introduce math bloggers listed site Today Gianluigi Filipp 2 years ago openPicus blog Better BBQ Flyport Cosm - knew Austria famous BBQ remember summer 2006 great barbeque austrian friends Klagenfurt fantastic 8 2 years ago Dirac Sea Survey Non Parametric Bayesian marginal applications - Survey Non Parametric Bayesian marginal applications mystery Zoubin one favorite researchers papers usually 2 years ago Biophotonics Review Biology Games Biomedical Research - Games become important part cultures bringing lots entertainment creativity societies Beyond traditional understanding o 2 years ago Clouds Fukushima 9 Months later - regards airborne radiation ground measurement reading NaI detector KEK Tsukuba showing steady decline past 3 years ago Gigapixel News Journal ipConfigure Presents First Gigapixel Wide Area Surveillance Platform - ipConfigure privately owned software research development company announces world first multi-Gigapixel surveillance platform designed 3 years ago Gustavo Tinkers Source Code GUI - Hosted https github com goretkin soundcard-radar 3 years ago Passive Vision Passive Vision - Welcome plan use blog ideas research teaching moment m looking forward teaching Computer Vision class 60 5 years ago Dick Gordon's blog - n 45 years ago Show 25 Show Search Pages Link Loading Previous Entries 2015 207 May 2015 37 Apr 2015 45 Mar 2015 44 Feb 2015 31 Jan 2015 50 2014 536 Dec 2014 52 Nov 2014 43 Oct 2014 38 Sep 2014 41 Aug 2014 48 Jul 2014 52 Jun 2014 43 May 2014 56 Apr 2014 47 Mar 2014 44 Feb 2014 35 Jan 2014 37 2013 454 Dec 2013 43 Nov 2013 38 Oct 2013 38 Sep 2013 33 Aug 2013 36 Jul 2013 43 Jun 2013 29 Around blogs 78 hours CIMI2013 CVPR List Computation sparse low degree interpolating pol Physical Principles Scalable Neural Recording Blind Calibration Compressed Sensing using Mess Quikr Method Rapid Reconstruction Bacter Sunday Morning Insight Enabling Verify Multi-View Lensless Compressive Imaging Towards better compressed sensing Around blogs 78 hours Finite rate innovation based modeling compr Precisely Verifying Null Space Conditions C FrameSense Near-Optimal Sensor Placement Line Different Sensing Modalities Compressive Object Sunday Morning Insight Quick Panorama Sensin Saturday Morning Videos Week Review Seinfeldian problem Meet-U CSJob PostDoc Research Associate Compressive PyHST2 hybrid distributed code high speed Quantized Iterative Hard Thresholding Bridging 1- Simple Deterministic Matrix Sketches Near- Sparse Recovery Streaming Signals Using l_1-Hom Saturday Morning Videos Robot Programming week Review COxSwAIN Machine Learning Post-Doc QM-MPI Joint Information System Lab Structure Discovery Nonparametric Regression th Lensless Imaging Compressive Sensing Nuit Blanche Review May 2013 Engineering Healthcare System Deliver Genomic Saturday Morning Videos May 2013 38 Apr 2013 40 Mar 2013 29 Feb 2013 47 Jan 2013 40 2012 488 Dec 2012 44 Nov 2012 39 Oct 2012 46 Sep 2012 28 Aug 2012 52 Jul 2012 20 Jun 2012 38 May 2012 60 Apr 2012 41 Mar 2012 50 Feb 2012 29 Jan 2012 41 2011 465 Dec 2011 47 Nov 2011 49 Oct 2011 47 Sep 2011 36 Aug 2011 24 Jul 2011 25 Jun 2011 47 May 2011 50 Apr 2011 56 Mar 2011 39 Feb 2011 17 Jan 2011 28 2010 358 Dec 2010 47 Nov 2010 35 Oct 2010 32 Sep 2010 28 Aug 2010 30 Jul 2010 33 Jun 2010 26 May 2010 27 Apr 2010 28 Mar 2010 28 Feb 2010 19 Jan 2010 25 2009 274 Dec 2009 22 Nov 2009 23 Oct 2009 24 Sep 2009 25 Aug 2009 25 Jul 2009 23 Jun 2009 20 May 2009 16 Apr 2009 25 Mar 2009 27 Feb 2009 21 Jan 2009 23 2008 302 Dec 2008 20 Nov 2008 23 Oct 2008 28 Sep 2008 28 Aug 2008 22 Jul 2008 17 Jun 2008 28 May 2008 22 Apr 2008 31 Mar 2008 32 Feb 2008 25 Jan 2008 26 2007 179 Dec 2007 23 Nov 2007 21 Oct 2007 14 Sep 2007 18 Aug 2007 13 Jul 2007 13 Jun 2007 9 May 2007 11 Apr 2007 9 Mar 2007 22 Feb 2007 19 Jan 2007 7 2006 30 Dec 2006 4 Nov 2006 4 Oct 2006 2 Sep 2006 2 Aug 2006 2 Jul 2006 2 Jun 2006 1 Mar 2006 2 Feb 2006 2 Jan 2006 9 2005 88 Dec 2005 1 Nov 2005 6 Oct 2005 3 Sep 2005 12 Aug 2005 1 Jul 2005 7 Jun 2005 4 May 2005 12 Apr 2005 7 Mar 2005 12 Feb 2005 8 Jan 2005 15 2004 214 Dec 2004 18 Nov 2004 8 Oct 2004 20 Sep 2004 44 Aug 2004 29 Jul 2004 13 Jun 2004 13 May 2004 18 Apr 2004 10 Mar 2004 22 Feb 2004 8 Jan 2004 11 2003 12 Dec 2003 10 Nov 2003 2 Books Wish List Start-ups like InView Technology Corporation See Inside World First Compressive Sensing Camera - Read InView white paper InView210 scientific SWIR camera InView210-CSCameraWhitePaper-Feb2015 See inside world first high 2 months ago Centice Centice Drug Analysis Systems Sold Major Federal Agency Aid Prescription Pill Abuse Operations - Nationwide program allows agents identify 3 800 prescription pills illicit drugs RESEARCH TRIANGLE PARK N C October 28 2014 Centice Co 7 months ago Press GraphLab twitterscroll - post twitterscroll appeared first GraphLab Inc 1 year ago Zoomin' - Aqueti TV Interview Gigapixel Images - Scott McCain interviewed July 10 2013 discuss gigapixel imaging WLOS ABC's Asheville affiliate 1 year ago Metamarkets Blog - wise io Machine Learning Service Big Data Analytics - Translate blog Focused Interest Compressed Sensing Compressive Sampling Compressive Sensing Mapping blog entries Compressed Sensing Cognition - Machine Learning Space Search Rescue Compressive Sensing Technology Watch Compressive Sensing Big Picture Compressive Sensing Hardware Compressed Sensing Videos Compressive Sensing Calendar Compressive Sensing Jobs Local Compressed Sensing Codes CS LinkedIn Group Recent links Blog CS Compressive Sensing 2 0 Community Compressive Sensing 2 0 blogs webpages Saturday Morning Cartoons Sherpa Romeo Publisher copyright policies self-archiving Categories Subjects Interest CS 2163 compressive sensing 1627 compressed sensing 1615 compressive sampling 1589 MF 521 implementation 359 Applied Math 209 ML 209 MatrixFactorization 194 space 152 AMP 113 calibration 105 CSHardware 103 CSjobs 90 CS Community 72 SaturdayMorningVideos 71 CSCommunity 66 BlindDeconvolution 64 QuantCS 62 phaseretrieval 62 RandNLA 60 hyperspectral 60 nonlinearCS 59 nuclear 59 technology 58 SundayMorningInsight 57 CSVideo 50 python 50 tensor 49 cognition 47 Meetups 46 Algorithm 45 grouptesting 45 meetup 43 1bit 42 publishing 41 synbio 41 graphlab 38 RandomFeatures 37 darpa 37 CSmeeting 36 AI 33 NuitBlancheReview 33 search rescue 33 weather modeling 33 remote sensing 32 Csstats 31 wow 30 business 29 bayes 28 data fusion 28 machine learning 28 MLParis 27 jim gray 24 neuroscience 23 autonomous 22 dimensionality reduction 22 mapmaker 20 thesis 20 AlexSmola 19 Kaczmarz 19 ParisMachineLearning 19 geocam 19 space debris 19 space situational awareness 19 medical 18 phaserecovery 18 ChristophStuder 17 ImagingWithNature 17 SAHD 17 maps 17 mishap 17 sleep 17 CSCalendar 16 monday morning algorithm 16 transport 16 CAI 15 energy 15 nanopore 15 phasediagrams 15 hasp 13 superresolution 13 ADMM 12 PatrickGill 12 Technologies Exist 12 causality 12 darpa urban challenge 12 sudoku 12 CSDiscussion 11 ICLR2015 11 TRL 11 qa 11 thermal engineering 11 GPU 10 fft 10 sie 10 videos 10 Computational Neuroscience 9 MultiplicativeNoise 9 RandomForest 9 StarTracker 9 aroundtheblogs 9 france 9 ELM 8 GenomeTV 8 HammingsTime 8 PredictingTheFuture 8 Good 8 collaborative task manager 8 exploration 8 random projections 8 situational awareness 8 sparsity 8 wavelet 8 GreatThoughtsFriday 7 TheGreatConvergence 7 collaborative work 7 innovation 7 mems 7 random lens imaging 7 CSCartoons 6 CitingNuitBlanche 6 CompressibleWGN 6 accidentalcamera 6 complexity vizualisation 6 maxent 6 randomization 6 startups 6 streaming 6 thedip 6 RMM 5 UQ 5 book 5 coded aperture 5 muscle 5 tex-mems 5 BP 4 British Petroleum 4 CfP 4 CompressiveSensingWhatIsItGoodFor 4 DataDrivenSensorDesign 4 HusHambug 4 Comment 4 ReproducibleResearch 4 google maps 4 hypergeocam 4 internet traffic 4 jionc 4 microsystems 4 scaling 4 technologie 4 Deepwater Horizon 3 disruptive technology 3 financement de la recherche 3 google 3 julia 3 radiation detection 3 recherche 3 sketching 3 Columbia 2 DC law 2 LowRank 2 MLZurich 2 MMDS 2 ManifoldSignalProcessing 2 NO-C-WE 2 TheNuitBlancheChronicles 2 UAV 2 aggregators 2 anecdote 2 challenge 2 diet 2 genomics 2 kinect hacks 2 microcontroller 2 notebynotecooking 2 sensor network 2 AWGN 1 BaltiAndBioinformatics 1 Blogger 1 CS MF 1 CT 1 CompanyX 1 JOTRSOI 1 Leonardo 1 QIS 1 RMT 1 SKA 1 SaturdayMorningCartoons 1 SensorsTheSizeOfAPlanet 1 YouAreNotPayingAttention 1 advice 1 aha 1 biographies 1 control 1 crowdfunding 1 csoped 1 dataset 1 donoho-tao 1 extremesampling 1 herschel 1 hushamburg 1 iLab 1 inverse problems 1 iot 1 jacques devooght 1 lfe 1 lua 1 memory 1 mindmaps 1 nanopre 1 octopus 1 oped 1 privacy 1 reference 1 request 1 rr 1 seinfeld 1 solver 1 theano 1 wonderingstar 1 youkeepusingthatword 1 sites interest Blogroll Natural Language Blog Hal Daume III Polylog Blog Andrew McGregor Eric Tramel's Espace Vide Blog Compressed Sensing Lianlin Li's Compressive Sensing blog Chinese Space Engineering Research Center Space Engineering Blog Frank Nielsen's Information Geometry blog David Brady's Blog Le Petit Chercheur Illustre Chaotic Pearls Indonesian De Rerum Natura Michele Guieu's blog Ergodic Walk Laurent Duval's site Laurent Duval's blog Thesilog Diffusion des savoirs - Ecole Normale Superieure What's New Terry Tao Statistical Modeling Causal Inference Social Science Andrew Gelman Aleks Jakulin Masanao Yajima Machine Learning etc Yaroslav Bulatov slice Pizza Muthu Mutukrishnan Geomblog Piotr Indyk Suresh Machine Learning Theory John Langford Lemonodor John Wiseman Yet another Machine Learning blog Pierre Dangauthier Make Magazine blog Theses en ligne Neurevolution blog Pedro Davalos website Damaris' blog Olivier's blog Julie's blog Michele Guieu's site Location visitors Nuit Blanche Dilbert counters Powered Blogger"),
('ML/NLP competition to predict Boston hygiene violations using Yelp reviews', "Toggle navigation Competitions Forum Contact Work us partner competitor Sign Log Keeping Fresh Predict Restaurant Inspections Hosted DrivenData Home Problem Description Yelp Boston 1 month 1 week left 5 000 00 5 000 00 Leaderboard You're part competition Yet Join competition Submissions Close July 7 2015 11 59 p m Evaluation Period Ends Aug 18 2015 11 59 p m Place Prize Amount 1st 3 000 2nd 1 000 3rd 1 000 Challenge Summary City Boston regularly inspects every restaurant monitor improve food safety public health cities health inspections generally random increase time spent spot checks clean restaurants following rules closely missed opportunities improve health hygiene places pressing food safety issues year millions people cycle post Yelp reviews experiences restaurants information reviews potential improve City inspection efforts could transform way inspections targeted team Harvard economists Yelp support City Boston co-sponsoring competition explore ways use Yelp review data improve inspections process looking help achieve goal Winning algorithms awarded financial prizes real prize opportunity help City Boston committed examining ways integrate winning algorithm day-to-day inspection operations goal competition use data social media narrow search health code violations Boston Competitors access historical hygiene violation records City Boston leader open government data Yelp's consumer reviews challenge Figure words phrases ratings patterns predict violations help public health inspectors job better competition opens Monday April 27th accept submissions eight weeks Submissions evaluated fresh hygiene inspection results six weeks following competition prizes awarded submission put running prize chance transform city governments ensure public health Keeping Fresh Predict Restaurant Inspections Rules Terms Data Use DrivenData Competition Rules GUIDELINES One account per participant cannot sign DrivenData multiple accounts therefore cannot submit multiple accounts Private sharing code Privately sharing code data outside teams permitted Public dissemination entries DrivenData competition host right publicly disseminate entries models Open licensing winners Winning solutions need made available MIT License open source software license commonly described http opensource org licenses MIT order eligible recognition prize money offered Documentation winning solutions Winning solutions must documented using provided Winning Model Documentation Template order eligible recognition prize money offered Submission limits number submissions per day restricted fixed value per-competition basis attempt circumvent limit result disqualification External data Competitors download external dataset Yelp Inc bound terms conditions present Yelp data download page External data allowed unless otherwise noted explicitly competition pages Participants agree make attempt use additional data data sources provided competition pages FULL RULES complete official rules Competition 'Competition Rules' incorporate reference contents Competition Website listed downloading dataset linked Competition Website submitting entry Competition joining Team Competition agreeing bound Competition Rules constitute binding agreement DrivenData applicable rules restrictions may imposed Competition Sponsor Competition sponsored Yelp Inc also referred herein time time Competition Sponsor Sponsor hosted Competition Sponsor's behalf DrivenData Inc 'DrivenData' Competition run according dates listed Competition Website Competition Period registered individual Team referred Participant may compete using single unique DrivenData account registered http www drivendata org Competing using one DrivenData account per individual breach Competition Rules Competition Sponsor reserves right disqualify individual Team including individual found breach Competition Rules Eligibility Competition open individuals age 18 time entry validly formed legal entities declared declared bankruptcy Officers directors employees advisory board members immediate families members household Competition Sponsor DrivenData respective affiliates subsidiaries contractors agents judges advertising promotion agencies eligible participate Competition eligible receive Prize Competition resident country designated United States Treasury's Office Foreign Assets Control Registration meet eligibility requirements would like participate must first complete registration process Website within Competition Period complete registration process receive access Data described Competition page enable develop submit one Entries defined Entries must received Competition Period register visit Website follow onscreen instructions complete submit registration registration information provide collectively referred Account already created Account enter user name password follow on-screen instructions register individually may join team Team expressly permitted rules may register register Competition remainder Team may disqualified Sponsor's sole discretion acknowledge agree solely responsible abiding employer's policies regarding participation Competition Sponsor disclaims liability responsibility disputes arising employee employer related Competition completed registration process Team provided access Data use develop Entries ENTERING COMPETITION ACCEPT CONDITIONS STATED OFFICIAL RULES INCLUDING RULES COMPETITION PAGE AGREE BOUND DECISIONS JUDGES WARRANT ELIGIBLE PARTICIPATE COMPETITION ACCEPT OFFICIAL RULES PLEASE REGISTER COMPETITION RECOMMEND PRINT COPY OFFICIAL RULES FUTURE REFERENCE Teams FORMING TEAM Multiple individuals entities may collaborate team Team may participate one Team Team member must single individual operating separate DrivenData account must register individually Competition joining Team must confirm Team membership make official responding Team notification message sent Account TEAM PRIZES Team wins monetary Prize Competition Sponsor allocate Prize money even shares Team members unless Team unanimously contacts DrivenData within three business days following Submission deadline request alternative prize distribution Entry submissions Entry must uploaded Website manner format specified Website Entries must received Competition Period Entry data submitted manner format specified Website via Entry form number Entries Participant may submit calendar day Competition Period displayed Competition page Sponsor reserves right request Participant submit algorithm associated Entry Sponsor Competition multi-stage Competition temporally separate training data leaderboard data one valid Entries must submitted selected stage Competition manner described Website USER SUBMISSIONS MUST ENTIRETY COMPLY APPLICABLE FEDERAL STATE LOCAL INTERNATIONAL LAWS REGULATIONS WITHOUT LIMITING FOREGOING ORDER ELIGIBLE WARRANT UNLESS OTHERWISE SPECIFIED PROBLEM STATEMENT SUBMISSION COMPETITION include anticipate inclusion content violation infringes third party intellectual property rights including limited copyrights including music copyrights patents trade secrets trademarks rights publicity privacy free clear liens claims encumbrances demands third parties include anticipate inclusion unsuitable offensive content including nudity sexually explicit disparaging libelous inappropriate content entered previous challenges won previous awards published distributed previously media suitable general audience contain claims cannot substantiated would false misleading reasonable consumer Selection winners Competition challenge skill final results determined solely leaderboard ranking private leaderboard subject compliance Competition Rules Participants' scores ranks Competition Website given stage Competition based evaluation metric described Competition Website determined applying predictions Submission ground truth validation dataset whose instances fixed set sampled Data evaluation metric used scoring ranking Submissions displayed Competition Website prize awards subject verification eligibility compliance Competition Rules decisions DrivenData Competition Sponsor judges final binding matters relating Competition DrivenData Competition Sponsor reserves right examine Submission associated code documentation compliance Competition Rules event Submission demonstrates breach Competition Rules Competition Sponsor may discretion take either following actions disqualify Submission require remediate within one week issues identified Submission including without limitation resolution license conflicts fulfillment obligations required software licenses removal software violates software restrictions tie two valid identically ranked submissions resolved favor tied submission submitted first Participant may decline nominated Winner notifying DrivenData directly within one week following Competition deadline case declining Participant forgoes prize features associated winning Competition DrivenData reserves right disqualify Participant declines DrivenData's sole discretion DrivenData deems disqualification appropriate Prizes conditions Competition prize Prize Winner notified announced within 30 days end Competition Website Prize Winner receive prizes awarded check wire transfer Team wins prize Team members must submit single written statement describing prize allocated among Team members Team fails submit statement within 30 days Sponsor requests Sponsor distribute prize among Team members equal shares obligation winning Team members condition receipt prize winner must deliver algorithm's code documentation Sponsor source code must contain description resources required build run algorithm accompanying documentation consistent DrivenData's Winning Model Documentation Template provided winners within 30 days end Competition Period prize may awarded alternate winner required documentation returned within thirty 30 days mailing winner prize notification letter email prize returned undeliverable winner respond email communication Sponsor within ten 10 days date sent Allow thirty 30 days prize delivery accepting prize award Winner agrees allow DrivenData Competition Sponsor disclose Winner's first name first initial last name city residence well prize information extent required law required law Winner agrees disclosure additional personal information without additional compensation Sponsor responsible prize delivery responsible prize utility otherwise substitution transfer prizes permitted taxes fees expenses associated participation Competition receipt use prize sole responsibility Prize Winner PARTICIPANT INTELLECTUAL PROPERTY LICENSING condition receipt Prize winning Participant thereby licenses winning Submission source code used generate Submission according Winner License Type specified note Winner License Type specified Winner License Type deemed Non-Exclusive License NON-EXCLUSIVE LICENSE Winner License Type Competition see Winner License Type Non-Exclusive License Winner accepting Prize thereby grants Competition Sponsor designees worldwide non-exclusive sub-licensable transferable fully paid-up royalty-free perpetual irrevocable right use use reproduce distribute create derivative works publicly perform publicly display digitally perform make made sell offer sale import winning Submission source code used generate Submission media known hereafter developed purpose whatsoever commercial otherwise without approval payment Participant represents unrestricted right grant license OPEN SOURCE LICENSE Winner License Type Competition see Winner License Type Open Source License Winner accepting Prize thereby licenses winning Submission source code used generate Submission MIT License open source software license commonly described http opensource org licenses MIT represents unrestricted right grant license CHEATING Participating using one DrivenData account deemed cheating discovered result disqualification Competition affected Competitions may result banning deactivation affected DrivenData accounts DrivenData reserves right request information associated investigation suspected cheating Failure respond requests including failure furnish requested information within ten 10 days grounds disqualification RECEIVING PRIZES verification eligibility prizes awarded Prize winner receive prize form check wire transfer made Prize winner individual individual Team members Team Allow 30 days final confirmation Prize delivery winners U citizens receive IRS 1099 form amount prize appropriate time Prize winners responsible taxes fees liability resulting receipt Prize Permissions Except prohibited law entry constitutes permission use winners' names hometowns likenesses online posting advertising publicity without additional compensation Winners may also required sign return release liability declaration eligibility lawful publicity consent agreement conditions receiving prize Failure comply aforementioned conditions shall grounds forfeiture prize Data use code sharing DATA Data means Data Datasets linked Competition Website purpose use Participants Competition avoidance doubt Data deemed purpose Competition Rules include prototype executable code provided Participants DrivenData Competition Sponsor via Website Participants must use Data permitted Competition Rules associated data use rules specified Competition Website Unless otherwise permitted terms Competition Website Participants must use Data solely purpose duration Competition including limited reading learning Data analyzing Data modifying Data generally preparing Submission underlying models participating forum discussions Website Participants agree use suitable measures prevent persons formally agreed Competition Rules gaining access Data agree transmit duplicate publish redistribute otherwise provide make available Data party participating Competition Participants agree notify DrivenData immediately upon learning possible unauthorized transmission unauthorized access Data agree work DrivenData rectify unauthorized transmission Participants agree participation Competition shall construed granted license expressly implication estoppel otherwise right ownership Data EXTERNAL DATA Unless otherwise expressly stated Competition Website Participants must use data Data develop test models Submissions Competition Sponsor reserves right sole discretion disqualify Participant Competition Sponsor discovers undertaken attempted undertake use data Data uses Data permitted according Competition Website Competition Rules course Competition CODE SHARING Participants prohibited privately sharing source executable code developed connection based upon Data sharing breach Competition Rules may result disqualification Participants permitted publicly share source executable code developed connection based upon Data otherwise relevant Competition provided sharing violate intellectual property rights third party sharing sharing Participant thereby deemed licensed shared code MIT License open source software license commonly described http opensource org licenses MIT OPEN-SOURCE CODE Submission ineligible win prize developed using code containing depending software licensed open source license Open Source Initiative-approved license see http opensource org licenses open source license prohibits commercial use Participant warranties obligations registering agree Account complete correct accurate b registration may rejected terminated Entries submitted Team may disqualified information Account Competition Sponsor reasonable grounds believe incomplete incorrect inaccurate solely responsible Account registration information deemed collected United States Participation subject federal state local laws regulations Void prohibited restricted law responsible checking applicable laws regulations jurisdiction participating Competition make sure participation legal responsible taxes reporting related award may receive part Competition responsible abiding employer's policies regarding participation Competition Competition Sponsor disclaims liability responsibility disputes arising employer related Competition Participant solely responsible equipment including necessarily limited computer internet connection necessary access Website develop upload Submission telephone data hosting service fees associated access well costs incurred behalf Entrant participating Competition entering Submission represent warrant information enter Website true complete best knowledge right authority make Submission including underlying code model behalf behalf persons entities specify within Submission Submission complies applicable federal state local international laws regulations original work used permission case full proper credit identify given third party contributions clearly identified within Submission contain confidential information trade secrets subject registered patent pending patent application violate infringe upon patent rights industrial design rights copyrights trademarks rights privacy publicity intellectual property rights person entity contain malicious code viruses timebombs cancelbots worms Trojan horses potentially harmful programs material information violate applicable law statute ordinance rule regulation trigger reporting royalty obligation third party previously published won prize award breach warranties result corresponding Submission invalid Confidentiality Confidential Information Defined used Section Confidential Information means Competition Sponsor Materials information provided DrivenData hereunder whether technical business nature including without limitation information relating party's technology software products services designs methodologies business plans finances marketing plans Sponsors prospects affairs received Participant Exclusions Participant's obligations Section disclosing party's Confidential Information include information Participant document known Participant prior receiving disclosing party connection Agreement b independently developed Participant without use reference Confidential Information disclosing party c acquired Participant another source without restriction use disclosure d becomes part public domain fault action Participant Nondisclosure consideration terms conditions Agreement good valuable consideration receipt sufficiency acknowledged term Agreement Participant use disclosing party's Confidential Information solely purpose provided b disclose disclosing party's Confidential Information third party unless third party must access Confidential Information perform accordance Agreement third party executed written agreement contains terms substantially similar terms contained Section c maintain secrecy protect unauthorized use disclosure disclosing party's Confidential Information extent using less reasonable degree care Participant protects Confidential Information similar nature Participant required law governmental semi-governmental agency court disclose disclosing party's Confidential Information terms Agreement Participant must give prompt written notice requirement disclosing party disclosure assist disclosing party attempting obtain order protecting Confidential Information public disclosure Return Information Upon request Participant destroy deliver disclosing party Confidential Information Participant may possession control Injunctive Relief Participant acknowledges violation threatened violation Section may cause irreparable injury party entitling disclosing party obtain injunctive relief addition legal remedies Equipment costs participation Participant solely responsible equipment including computer modem necessary establish connection World Wide Web access World Wide Web telephone data hosting service fees associated access well costs incurred behalf Participant participating Competition Delivery receipt entries Sponsor responsible late lost stolen damaged garbled incomplete incorrect misdirected Entries communications b errors omissions interruptions deletions defects delays operations transmission information case whether arising way technical failures malfunctions computer hardware software communications devices transmission lines c data corruption theft destruction unauthorized access alteration Entry materials loss otherwise Sponsor responsible electronic communications emails undeliverable result form active passive filtering kind insufficient space email account receive email messages Sponsor disclaims liability damage computer system resulting participation accessing downloading information connection Competition Reservation rights DrivenData sole discretion reserves right disqualify person tampering entry process operation Web site competition process otherwise violation rules DrivenData reserves right time disqualify Submission Competition acting good faith believes reasonable grounds warrant disqualification example DrivenData determination Submission provide functionality described required Submission appears purposely designed circumvent Rules spirit Competition would grounds disqualification DrivenData reserves right cancel terminate modify competition capable completion planned reason including infection computer virus bugs tampering unauthorized intervention technical failures sort General release entering Competition release discharge Competition Entities liability whatsoever connection Competition acceptance possession use misuse prize including without limitation legal claims costs injuries losses damages demands actions kind including without limitation personal injuries death damage loss destruction property rights publicity privacy defamation portrayal false light Competition Entities responsible typographical printing inadvertent errors Official Rules materials relating Competition Additionally hereby agree indemnify Competition Entities losses damages costs expenses rights claims demands actions including attorney's fees expenses litigation settlement may brought one anyone claiming suffered loss damage result participation Competition Limitations liability participating Competition Participant agrees 1 disputes claims causes action arising connection Competition prize awarded shall resolved individually without resort form class action 2 claims judgments awards shall limited actual out-of-pocket costs incurred including costs associated entering Competition event attorney's fees 3 circumstances Participant permitted obtain award Participant hereby waives rights claim punitive incidental consequential damages rights damages multiplied otherwise increased damages damages actual out-of-pocket expenses federal state local laws apply DrivenData Inc employees officers directors shareholders agents representatives affiliates subsidiaries advertising promotional legal advisors responsible shall liable late lost delayed damaged misdirected incomplete illegible unintelligible entries ii telephone electronic hardware software program network Internet computer malfunctions failures difficulties iii errors transmission iv condition caused events beyond control DrivenData Inc may cause contest disrupted corrupted v injuries losses damages kind caused prize resulting acceptance possession use prize participation contest vi printing typographical errors materials associated contest Disclaimer Warranties understand cannot guarantee warrant files available downloading internet Website free viruses destructive code responsible implementing sufficient procedures checkpoints satisfy particular requirements anti-virus protection accuracy data input output maintaining means external site reconstruction lost data LIABLE LOSS DAMAGE CAUSED DISTRIBUTED DENIAL-OF-SERVICE ATTACK VIRUSES TECHNOLOGICALLY HARMFUL MATERIAL MAY INFECT COMPUTER EQUIPMENT COMPUTER PROGRAMS DATA PROPRIETARY MATERIAL DUE USE WEBSITE SERVICES ITEMS OBTAINED WEBSITE DOWNLOADING MATERIAL POSTED WEBSITE LINKED USE WEBSITE CONTENT SERVICES ITEMS OBTAINED WEBSITE RISK WEBSITE CONTENT SERVICES ITEMS OBTAINED WEBSITE PROVIDED AVAILABLE BASIS WITHOUT WARRANTIES KIND EITHER EXPRESS IMPLIED NEITHER COMPANY PERSON ASSOCIATED COMPANY MAKES WARRANTY REPRESENTATION RESPECT COMPLETENESS SECURITY RELIABILITY QUALITY ACCURACY AVAILABILITY WEBSITE WITHOUT LIMITING FOREGOING NEITHER COMPANY ANYONE ASSOCIATED COMPANY REPRESENTS WARRANTS WEBSITE CONTENT SERVICES ITEMS OBTAINED WEBSITE ACCURATE RELIABLE ERROR-FREE UNINTERRUPTED DEFECTS CORRECTED SITE SERVER MAKES AVAILABLE FREE VIRUSES HARMFUL COMPONENTS WEBSITE SERVICES ITEMS OBTAINED WEBSITE OTHERWISE MEET NEEDS EXPECTATIONS COMPANY HEREBY DISCLAIMS WARRANTIES KIND WHETHER EXPRESS IMPLIED STATUTORY OTHERWISE INCLUDING LIMITED WARRANTIES MERCHANTABILITY NON-INFRINGEMENT FITNESS PARTICULAR PURPOSE FOREGOING AFFECT WARRANTIES CANNOT EXCLUDED LIMITED APPLICABLE LAW Governing Law Jurisdiction DrivenData shall sole interpreter Official Rules matters relating Official Rules Competition dispute claim arising therefrom related thereto case including non-contractual disputes claims shall governed construed accordance internal laws Commonwealth Massachusetts without giving effect choice conflict law provision rule whether Commonwealth Massachusetts jurisdiction legal suit action proceeding arising related Competition Website shall instituted exclusively federal courts United States courts Commonwealth Massachusetts although retain right bring suit action proceeding breach Terms Use country residence relevant country waive objections exercise jurisdiction courts venue courts invalidity unenforceability provision Official Rules shall affect validity enforceability provision event provision determined invalid otherwise unenforceable illegal Official Rules shall otherwise remain effect construed accordance terms invalid illegal provision contained herein Decline Agree DrivenData Blog Work Us partner competitor Join competition Legal Terms Use Copyright Policy Privacy Policy Contact info drivendata org DrivenData Inc PO Box 390664Cambridge MA 02139"),
('Azure Machine Learning: simplified predictive analytics', "CloudAcademy BlogCloud AcademyHow worksPlans PricingVideo CoursesAWS CertificationsCommunityFAQs Cloud Academy Blog Azure Machine Learning simplified predictive analyticsAzure Machine Learning simplified predictive analyticsMay 4 2015 Alex CasalboniAzure Machine Learning cloud-based predictive analytics serviceLast week wrote using AWS Machine Learning tool built models open dataset Since feeling needed control happens hood particular far kind models trained evaluated decided give Microsoft Azure Machine Learning try case re yet familiar Microsoft Azure talking Platform Service PaaS Infrastructure Service IaaS solution created Microsoft back 2010 Cloud Academy excellent courses introducing platform Azure Machine Learning launched February year immediately recognized game changer applying power cloud solve problem big data processing Since m new Azure world decided start 1 month free trial including 150 credit dashboard running within minutes gladly took quick tour dashboard features clicks discovered Azure Machine Learning offers fairly independent environment work precise called ML Workspace ML-related objects live although still able monitor ML web services directly general Azure dashboard First step create Azure Machine Learning WorkspaceThis quite simple every time want create something ll find New button bottom left part dashboard looked Machine Learning Data Services section creation process straightforward provide names Workspace new storage account always access Azure Machine Learning Studio https studio azureml net spent time workspace UI coherent clear although dependency items might always immediately evident Let briefly describe kind items re going work Datasets would image data containers create new one local file although ways feed models via special Reader objects Experiments experiment set connected components used create train score test model Additional modules added take care data preprocessing features selection data splitting cross-validation etc find plenty import Trained Models happy model ve experimented save even reuse future experiments Web Services order obtain new predictions either online batch predictions ll need create query Web Service Azure even help creating documentation sample code accessing endpoints via simple POST requests import datasetAs previous article Amazon Machine Learning using open dataset specifically designed Human Activity Recognition HAR contains 10 000 records defined 560 features input columns plus one target column represents activity type want classify authors manually labelled row one six possible activity types means trying solve multiclass problem ll manipulating raw dataset create one single csv file see Python script although could also used one formats input Dataset object CSV without header row TSV without without header row Plain Text txt ZIP zip SvmLight info ARFF Attribute Relation File Format RData R Object Workspace aren ways bring data Azure Machine Learning might use special Reader object experiments retrieve data via one following Web URL HTTP Azure Blob StorageAzure SQL DatabaseAzure TableHive QueryData Feed Provider Odata OK let import csv file without header row create new Dataset object step might take based network speed continue working experiment create Azure Machine Learning modelUnexpectedly working Experiments quite fun soon create first blank experiment re shown user-friendly UI lot building blocks link together order train evaluate model full control going input processing phase configured removed time First things first dragged Dataset object named HAR dataset csv see screenshot course much happened decide model use find available models Machine Learning Intialize model menu left selected Classification focused Multiclass models Decision Forest Decision Jungle Logistic Regression Neural Network purpose demonstration let assume directly chose Logistic Regression although truth came conclusion training evaluating Logistic Regression Neural Network show end article Note drag Logistic Regression block experiments practice creating model configuration object contain model parameters always edit even running experiment order actually train model Train Model object needed find Machine Learning Train menu block connected set inputs outputs experiment won even run every required connection filled Train Model block takes input Untrained Model model config Dataset object UI really help see input output type simply hovering mouse connection slot re yet done training still need let new block know target column re trying predict Train Model properties sidebar click Launch column selected choose Col1 variable target column Note since uploaded header-less csv file columns named automatically receive incremental number going quite simple want create model able understand performs general solution splitting dataset two random sets let say 70 30 using 70 dataset train robust model testing remaining 30 ensure model behaves well new data big deal add new Split block takes one input Dataset object gives two output links based Fraction rows parameter try tune across several experiment runs see changes maybe 60 40 50 50 Train Model block output fact trained model ll put work add Score Model block verify take remaining 30 dataset generate actual predictions everything configured finally run experiment yes clicking big Run button bottom Incredibly takes seconds ten seconds dataset quickly inspect results clicking output new Score Model block Visualize find predicted class 3 090 rows together statistics column including mean median min max standard deviation graphical distribution values histogram course manually reading 3 000 rows evaluate model ll need add one last Evaluate Model block Machine Learning Evaluate menu ll quickly re-run experiment click evaluation block output Visualize find useful data model accuracy precision recall intuitively graphically visualize 6 possible classes classified testing set looking Confusion Matrix got pretty good results using Logistic Regression ranging 94 100 precision may also notice particular patterns example squares around classes 1 2 3 4 5 happens classes indeed confused model case 1 2 3 walking classes 4 5 sitting standing expected problems predictions way Amazon Machine Learning Amazon Machine Learning however achieved 100 precision laying activity use model codeIn order finally use model real time predictions need expose Web Service fact Web Services created result experiment inside Azure Machine Learning Luckily converting already created experiment first Web Service fairly painless requiring click Create Scoring Experiment also enjoy beautiful animation watching video wait One thing didn notice first every experiment Web Service switch Eventually define training testing procedure web service within experiment switching back forth two visualizations simply highlight blur input output blocks re almost one task publishing web service need name Web Service Input Web Service Output blocks named mine record result names used JSON fields web service response finally run experiment one last time click Publish Web Service Azure Machine Learning automatically creates ad-hoc API key default endpoint published web service really ll need bottom web service public documentation page ll find sample code C Python R simple Python script coded based default one Azure ML - Web Service PredictionPython import urllib2 import json web service config webservice 'YOUR_WS_URL' api_key 'YOUR_WS_KEY' dataset config labels '1' 'walking' '2' 'walking upstairs' '3' 'walking downstairs' '4' 'sitting' '5' 'standing' '6' 'laying' n_classes len labels n_columns 562 read new record file open 'record csv' f record_str f readline POST data input_data Inputs record ColumnNames Col n n range 1 n_columns 1 dynamic Values '0' record_str split ' ' leading zero GlobalParameters request config headers 'Content-Type' 'application json' 'Authorization' 'Bearer ' api_key req urllib2 Request webservice json dumps input_data headers try data json loads urllib2 urlopen req read result data 'Results' 'result' 'value' 'Values' 0 deep label result -1 last item classified class stats result - n_classes 1 -1 M probabilities one class print currently class labels label label p enumerate stats print Scored Probabilities Class 1 p except urllib2 HTTPError e print request failed status code str e code print e info print json loads e read 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import urllib2import json web service configwebservice 'YOUR_WS_URL'api_key 'YOUR_WS_KEY' dataset configlabels '1' 'walking' '2' 'walking upstairs' '3' 'walking downstairs' '4' 'sitting' '5' 'standing' '6' 'laying' n_classes len labels n_columns 562 read new record filewith open 'record csv' f record_str f readline POST datainput_data Inputs record ColumnNames Col n n range 1 n_columns 1 dynamic Values '0' record_str split ' ' leading zero GlobalParameters request configheaders 'Content-Type' 'application json' 'Authorization' 'Bearer ' api_key req urllib2 Request webservice json dumps input_data headers try data json loads urllib2 urlopen req read result data 'Results' 'result' 'value' 'Values' 0 deep label result -1 last item classified class stats result - n_classes 1 -1 M probabilities one class print currently class labels label label p enumerate stats print Scored Probabilities Class 1 p except urllib2 HTTPError e print request failed status code str e code print e info print json loads e read notice Azure Machine Learning causes lot data travel great deal across network POST request pretty heavy course since sending 500 input values auto-generated variable name surprised unnecessarily heavy response format JSON table since contains input record ve sent together predicted class classes reliability measure Indeed case contains 18KB data HTTPS call takes seven seconds versus two seconds experienced Amazon Machine Learning UPDATE reader Dmitry clarified comment easily reduce size every response placing Project Columns module right Web Service Output one way projecting scores labels managed cut response size 560B although response time still 7 seconds JSON responseJavaScript Results result type table value ColumnNames Col1 Scored Probabilities Class 1 Scored Probabilities Class 2 Scored Probabilities Class 3 Scored Probabilities Class 4 Scored Probabilities Class 5 Scored Probabilities Class 6 Scored Labels ColumnTypes Numeric Numeric Numeric Numeric Numeric Numeric Numeric Categorical Values 0 0 00638132821768522 0 970375716686249 0 0132338870316744 0 000609736598562449 0 00881590507924557 0 000583554618060589 2 1234567891011121314151617181920212223242526272829303132333435363738394041 Results result type table value ColumnNames Col1 Scored Probabilities Class 1 Scored Probabilities Class 2 Scored Probabilities Class 3 Scored Probabilities Class 4 Scored Probabilities Class 5 Scored Probabilities Class 6 Scored Labels ColumnTypes Numeric Numeric Numeric Numeric Numeric Numeric Numeric Categorical Values 0 0 00638132821768522 0 970375716686249 0 0132338870316744 0 000609736598562449 0 00881590507924557 0 000583554618060589 2 Anyway re Scored Labels column last one contains integer represents predicted class given record Furthermore M previous columns contain Scored Probabilities Class N already mentioned AWS ML values might help take advanced decisions case selected prediction model reliable enough Note exists Azure SDK Python even read-only Azure Machine Learning client used access Web Service see new web service particularly hard query honestly would rather access well structured tested client using raw http requests JSON parsing Monitoring Web ServiceIf model works fine team access endpoint API key web service start serving predictions new data may want monitor load find Azure Machine Learning web services general Azure dashboard inside Web Services section Azure Machine Learning Workspace find detailed visualization requests predictions configured endpoint Furthermore create new endpoints configure Throttle level Diagnostics Trace level course used monitor separately grant remove temporary access Web Service Note delete default endpoint example create dev endpoint Low Throttle level Diagnostics Trace level used development won confused production loads statistics Experiments many modelsAs promised m going discuss create experiment train test one model Basically add parallel blocks diagram use output first blocks multiple times configured scored two different models reuse Evaluate Model block take additional Scored Dataset object input Let see trained evaluated Logistic Regression Neural Network models dataset much moreWhile goal reproduce ML model created AWS last week ve found Azure Machine Learning much Machine Learning Service practice create data science workflow using ready-to-use set modules manipulate analyze data including Data conversion transformation Feature Selection Filter-based Fisher LDA etc OpenCV algorithms Image Reader Cascade Image Classification Python scripts script must contain azureml_main function eventually import zipped libraries R models scripts Statistical Functions correlation trigonometric transformations rounding deviation kurtosis etc Text Analytics Feature Hashing Named Entity Recognition etc learn use amazing tools exploring Azure Machine Learning Gallery even add experiments list Become Cloud expert Annual MembershipNew courses every week 100 May 31st Start 7 day Free Trial Alex CasalboniI Software Engineer great passion music web technologies I'm experienced web development software design particular focus frontend UX sound music engineering background allows deal multimedia signal processing machine learning AI lot interesting tools even powerful merge Cloud Posts - WebsiteFollow http ksmigiel com Krzysztof migiel nice article Azure ML Thanks Dmitry surprised unnecessarily heavy response format JSON table since contains input record ve sent together predicted class classes reliability measure reduce size response message placing Project Columns module Score Model Web Service Output Project Columns specify columns web service output http alexcasalboni com Alex Casalboni Thank Dmitry going give try asap update article would nice make automatic though never need input record prediction response course problem feature columns case really experienced performance issue m happy see way avoid Latest E-booksSign Cloud Academy Start Freeor social sign Learn AWS Cloud Academy Labs Learn Cloud Computing Cloud Computing Fundamentals Become AWS Certified AWS Certifications Guide Amazon Web Services AWS Microsoft Azure Docker Open-Source TechSubscribe newsletterStay updated Cloud Computing news weekly learning material Follow Cloud Cloud Academy Blog Azure Machine Learning simplified predictive analyticsAbout Sitemap Cloud FundamentalsCopyright 2013-2015 Cloud Academy rights reserved Share Facebook Twitter Google LinkedIn"),
]